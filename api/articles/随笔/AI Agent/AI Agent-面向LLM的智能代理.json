{"title":"随笔-AI Agent-面向LLM的智能代理","uid":"0a820d257941cf971cd1cbe029f2be41","slug":"随笔/AI Agent/AI Agent-面向LLM的智能代理","date":"2026-01-31T10:01:03.000Z","updated":"2026-02-04T06:54:34.516Z","comments":true,"path":"api/articles/随笔/AI Agent/AI Agent-面向LLM的智能代理.json","keywords":null,"cover":"img/post/随笔/AI Agent/AI Agent-面向LLM的智能代理/cover.jpeg","content":"<h1 id=\"零-目录\"><a class=\"markdownIt-Anchor\" href=\"#零-目录\"></a> 零. 目录</h1>\n<h2 id=\"1-目录\"><a class=\"markdownIt-Anchor\" href=\"#1-目录\"></a> 1. 目录</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">章节</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent%2FAI%20Agent%E7%9A%84%E8%AE%B0%E5%BF%86%E5%B7%A5%E7%A8%8B\">AI Agent的记忆工程</a></td>\n<td>Agent 的记忆机制和管理策略<br>写入、压缩 机制的规范</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent%2FAI%20Agent%E7%9A%84%E6%A3%80%E7%B4%A2%E5%B7%A5%E7%A8%8B\">AI Agent的检索工程</a></td>\n<td>Agent 如何从管理的数据中检索相关上下文<br>RAG、Skills 技术</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent%2FAI%20Agent%E7%9A%84%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8%E5%B7%A5%E7%A8%8B\">AI Agent的工具调用工程</a></td>\n<td>Agent 如何调用外部工具<br> MCP 协议</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent%2FAI%20Agent%E7%9A%84%E6%8B%9F%E4%BA%BA%E7%B3%BB%E7%BB%9F\">AI Agent的拟人系统</a></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-核心技术\"><a class=\"markdownIt-Anchor\" href=\"#2-核心技术\"></a> 2. 核心技术</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">技术</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">WSCI 框架</td>\n<td>写入 (Write)、选择 (Select)、压缩 (Compress)、隔离 (Isolate)</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">KV 缓存命中</td>\n<td>尽可能地只追加上下文，最大化 KV 缓存命中率</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">ACE 框架</td>\n<td>一种 Generator-Reflector-Curator 的三段式 Agent 架构 <br>主要在于如何更新记忆</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">卸载与压缩、渐进式披露</td>\n<td>将长的上下文信息卸载到外部存储中<br>在需要时逐步加载和披露给模型</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Skills</td>\n<td>使用 metadata 来快速检索相关知识的方法</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MCP 协议</td>\n<td>Model Context Protocol，一种用于 AI Agent 与外部工具交互的标准协议</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"一-介绍\"><a class=\"markdownIt-Anchor\" href=\"#一-介绍\"></a> 一. 介绍</h1>\n<p>Agent 是一种以环境、任务和目标为基础能够自主决策的计算机程序。</p>\n<p>基于 LLM 的 AI Agent 是利用大语言模型对自然语言的处理来让程序更好地理解和执行任务。也可以说是一种指导 LLM 进行自主决策和行动的系统。</p>\n<p>AI Agent 主要包括以下几个核心组件：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">组件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">检索 <br> Retrieval</td>\n<td>负责从知识库中检索相关信息来组成上下文</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">工具调用 <br> Tools Calling</td>\n<td>负责调用外部工具和 API 来扩展模型的功能</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">记忆 <br> Memory</td>\n<td>负责读写和管理长期和短期记忆，以满足用户个性化需求</td>\n</tr>\n</tbody>\n</table>\n<p align=\"center\">\n<img src=\"/img/post/随笔/AI Agent/AI Agent-面向LLM的智能代理/cover.jpeg\" alt=\"LLM agent 框架图\" width=\"400\">\n</p>\n<h2 id=\"1-ai-agent-要注意的事\"><a class=\"markdownIt-Anchor\" href=\"#1-ai-agent-要注意的事\"></a> 1. AI Agent 要注意的事</h2>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>如果 LLM 能够很好地理解自然语言并生成严格遵循规范的输出，那么 Agent 就能够准确地理解任务需求并做出更合适的决策；相反，如果 LLM 的能力欠佳，那么 Agent 的表现也会大大受限。</p></blockquote>\n<p>相对于传统的 Agent，基于 LLM 的 Agent 性能不仅取决于其算法和规则框架，还非常依赖于 LLM 的质量和能力。</p>\n<p>此外，LLM 的输出是基于概率的，是偏向于自然语言的，具有一定的不确定性。因此，AI Agent 需要设计相应的机制来处理这种不确定性，例如通过多次采样、结果验证等方法来提高决策的鲁棒性。</p>\n<p>因此，为了保证适应大部分 LLM 模型，提高系统的鲁棒性，设计 Agent 要考虑以下几点：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">设计要点</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">权限</td>\n<td>不提供给 Agent 过多的权限，避免误操作<br>或采取沙盒技术</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">异常处理</td>\n<td>设计良好的异常处理和回退机制，确保在出现异常时能够及时应对并恢复<br>同时要注意 <strong>异常错误</strong> 是一个很好让 Agent 学习记录的机会</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">反馈机制</td>\n<td>设计反馈机制让 Agent 能够从错误中学习和改进</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">监控和日志</td>\n<td>设计良好的监控和日志系统，实时跟踪 Agent 的行为和决策过程，便于排查问题</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">测试框架</td>\n<td>设计良好的测试 Agent 框架，例如测试时让人类充当 LLM 观察 Agent 的表现<br>这也要求 LLM 模块与 Agent 架构进行解耦 (合理的模块化)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-最简单的-ai-agent\"><a class=\"markdownIt-Anchor\" href=\"#2-最简单的-ai-agent\"></a> 2. 最简单的 AI Agent</h2>\n<p>最简单的 AI Agent 就是我们常见的网页与 LLM 进行对话交互。该模式下，用户输入问题，LLM 根据模型知识库生成回答，用户再根据回答继续提问，以此类推。每一次 LLM 的输入都是基于用户的最新输入和历史对话内容生成的。这种历史对话内容也被称之为 上下文 (Context)。</p>\n<hr />\n<h1 id=\"二-agent-管理的数据类型\"><a class=\"markdownIt-Anchor\" href=\"#二-agent-管理的数据类型\"></a> 二. Agent 管理的数据类型</h1>\n<p>AI Agent 主要管理的数据类型包括：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">数据类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">指令<br>Instruction</td>\n<td>Agent 的行动纲领，确定了目标<br>包括 Prompts、工具定义、思考链</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">知识<br>Knowledge</td>\n<td>决策的实施依据<br>包括检索的文档、用户画像</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">工具反馈<br>Tool Feedback</td>\n<td>与世界交互的观察结果<br>包括 API 返回、数据库内容</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">示例<br>Examples</td>\n<td>历史中回答过的成功或反思案例 <br>也属于指令、知识中的混合部分</td>\n</tr>\n</tbody>\n</table>\n<p>也就是说，Agent要考虑：</p>\n<ul>\n<li>如何解析输入，如何构建输出 (从指令、示例中获取)</li>\n<li>事实依据都有哪些 (从知识、工具、实例中获取)</li>\n</ul>\n<hr />\n<h1 id=\"三-提示词工程\"><a class=\"markdownIt-Anchor\" href=\"#三-提示词工程\"></a> 三. 提示词工程</h1>\n<p>LLM 相当于一种黑盒函数，能够将序列输入 (自然语言) 映射到序列输出。我们可以简单定义为：</p>\n<p class='katex-block'><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"bold\">y</mi></mrow><annotation encoding=\"application/x-tex\">f(\\mathbf{x}) = \\mathbf{y}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathbf\">x</span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.63888em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span></span></span></span></span></p>\n<p>其中，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span></span></span></span></span> 是输入的自然语言序列，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">y</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.63888em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span></span></span></span> 是输出的自然语言序列，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 是 LLM 模型。</p>\n<p>为了提升 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">y</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.63888em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span></span></span></span> 的质量，我们有两种思路：</p>\n<ol>\n<li>提升 LLM 模型 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span> 的能力。</li>\n<li>优化输入 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">x</span></span></span></span></span>。</li>\n</ol>\n<p>其中对于第二种方法，是一种完全不依赖于训练模型、修改模型参数来提升 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">y</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.63888em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">y</span></span></span></span></span> 质量的方法，只需要组织好自然语言即可。这种方法被称之为 提示工程 (Prompt Engineering)。</p>\n<p>例如使用自然语言去引导 LLM 生成 JSON 格式的数据，亦或者让 LLM 去扮演某个角色，并以该角色的身份去回答问题。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">假设你是一个专业的数学老师，正在教导一个学生解决下面的数学问题。</span><br><span class=\"line\">先回答下面的问题的答案，并列出该问题的每个详细步骤。</span><br><span class=\"line\">并利用这些步骤对答案进行验算是否正确，</span><br><span class=\"line\">最终以 latex 代码的格式输出结果。</span><br></pre></td></tr></table></figure>\n<hr />\n<h1 id=\"四-上下文工程\"><a class=\"markdownIt-Anchor\" href=\"#四-上下文工程\"></a> 四. 上下文工程</h1>\n<p>在传统的提示词工程中，往往专注的是设计少量次数的输入输出交互，引导模型作出最佳单次输出决策。这种方法在简单应用中非常有效，但是在更复杂的任务中，单次交互往往无法满足需求。上下文越长，会出现上下文腐烂(Context Rot)，模型表现越不可靠：</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(n^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 的注意力机制，稀散了所有 token 的注意力</li>\n<li>训练数据以短序列为主，缺乏长序列训练</li>\n</ul>\n<p>上下文工程 (Context Engineering) 则是通过设计和动态地管理更大范围的上下文信息，从大量的上下文信息中筛选出关键内容，从而在多轮、持续的交互中提升模型的表现，做出符合长期目标的决策。上下文工程是一种更高级的提示工程。</p>\n<p>上下文工程的关键组成成分包括但不限于：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">组成成分</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">系统提示</td>\n<td>是 Agent 的大脑，用于帮助 Agent 思考<br>提供启发式原则，而非硬编码规则，平衡指导性和灵活性<br>避免过于具体和过于模糊，要结构化书写，不断迭代优化</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">工具使用</td>\n<td>是 Agent 的手脚，用于让 Agent 与外界交互获取感官<br>允许模型在需要时调用外部资源，例如网络搜索、本地文件检索<br>要求高内聚、低耦合，责任单一，避免功能重叠和工具臃肿；最小工具集原则</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">示例</td>\n<td>提供多样化、有代表的行为范本，用于 “行为校准”<br>质量远胜于数量</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">短期记忆</td>\n<td>近期的对话内容和交互历史，帮助模型理解当前的上下文<br>往往是直接存放在内存中的信息</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">长期记忆</td>\n<td>重要的背景信息和用户偏好，帮助模型做出更符合用户需求的决策<br>往往是存放在数据库中的信息，在需要时加载</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">窗口优化</td>\n<td>管理和优化模型的上下文窗口，确保关键内容被优先考虑，包括：<br> 1. 压缩：总结过往对话<br>2. 过滤：去除无关信息<br>3. 优先级排序：将更关键的信息进行特殊标注</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">思维链</td>\n<td>一种分步推理的方法，帮助模型逐步解决复杂问题<br>通过引导模型进行多步思考，提升其解决问题的能力<br> 例如 ReAct 框架、DeepSeek 深度思考</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"1-为什么需要上下文工程\"><a class=\"markdownIt-Anchor\" href=\"#1-为什么需要上下文工程\"></a> 1. 为什么需要上下文工程</h2>\n<p>上下文工程的目标往往是：</p>\n<ul>\n<li>方便用户快速调用和管理大量的提示词和上下文信息。</li>\n<li>提升模型的长期记忆和理解能力。</li>\n<li>节约 Token 资源，避免输入冗余和无关的信息。</li>\n<li>与模型无关，适用于各种 LLM 模型，也可以同时适用于多模型协同工作。</li>\n</ul>\n<h3 id=\"i-管理大量提示词\"><a class=\"markdownIt-Anchor\" href=\"#i-管理大量提示词\"></a> i. 管理大量提示词</h3>\n<p>假设用户现在拥有一个 AI Agent 来帮助管理日常生活中的各种应用，包括教用户写程序的提示词(使用什么样的伪代码、什么样的输出格式、怎么测试代码)、教用户做菜的提示词(每一步的详细步骤、注意事项)、教用户聊天的提示词(对方的性格、兴趣爱好、聊天风格)等。</p>\n<p>每一个提示词都使用了 Prompt Engineering 的方法设计了非常详细的上下文内容。当用户想要用到某一种提示词时，只需要复制粘贴到 LLM 的输入框中即可。但是这些提示词就会慢慢积累变多，单纯依靠复制粘贴就会变得非常麻烦。</p>\n<p>如果一股脑将所有提示词丢给 LLM，会浪费 Token 资源，并且也导致 LLM 很难聚焦在用户当前想要使用的提示词上。</p>\n<p>上下文工程就是来帮助用户管理和使用这些提示词。用户可以将所有提示词存放在一个数据库中，当需要使用某一种提示词时，AI Agent 会根据用户的需求，从数据库中检索出最相关的提示词，并将其作为上下文的一部分，添加到 LLM 的输入中。</p>\n<h3 id=\"ii-节约-token-资源\"><a class=\"markdownIt-Anchor\" href=\"#ii-节约-token-资源\"></a> ii. 节约 Token 资源</h3>\n<p>为什么要节约 Token 资源? 非常简单，Token = 钱。上下文越长，消耗的 Token 越多，花费的成本也就越多。</p>\n<h3 id=\"iii-避免长上下文引发的问题\"><a class=\"markdownIt-Anchor\" href=\"#iii-避免长上下文引发的问题\"></a> iii. 避免长上下文引发的问题</h3>\n<p>上下文越长，模型的表现往往越不可靠。通常会出现以下四种问题：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">问题</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">污染<br> Poisoning</td>\n<td>错误信息渗入上下文，并被后续推理作为事实依据</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">干扰<br> Distraction</td>\n<td>上下文信息过多，模型预训练的知识不堪重负<br>信噪比过低，噪声太多</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">混淆<br> Confusion</td>\n<td>模型错误地关联无关信息<br>注意力被分散</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">冲突<br> Clash</td>\n<td>上下文部分出现不一致，导致决策混乱</td>\n</tr>\n</tbody>\n</table>\n<p>这些问题的根本病因就是上下文腐烂 (Rot)，是因上下文过长触及模型架构瓶颈，导致模型认知能力系统性下降的底层问题。</p>\n<hr />\n<h2 id=\"2-上下文工程中提示词类型\"><a class=\"markdownIt-Anchor\" href=\"#2-上下文工程中提示词类型\"></a> 2. 上下文工程中提示词类型</h2>\n<p>提示词类型主要分为四种：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">用户提示词<br>User Prompt</td>\n<td>由用户直接输入的消息类型，是要解决的问题或指令</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">系统提示词<br>System Prompt</td>\n<td>用于设定 AI Agent 行为和思维方式的消息类型<br>其核心作用就是为 Agent 提供一个高层次的指导原则，帮助其在面对复杂任务时做出合理的决策</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">工具提示词<br>Tool Prompt</td>\n<td>用于返回 “工具执行结果” 的消息，也就是环境对 Agent 行动的反馈</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">助手提示词<br>Assistant Prompt</td>\n<td>由模型生成的消息类型，是对用户提示词的响应<br>是一种模型对话的短期记忆，也会用来与工具提示词的 id 进行关联</td>\n</tr>\n</tbody>\n</table>\n<p>可以通过编写 User Prompt 和 Assistant Prompt 模拟对话，来达到一些 Few-Shot 效果。</p>\n<hr />\n<h2 id=\"3-wsci-框架\"><a class=\"markdownIt-Anchor\" href=\"#3-wsci-框架\"></a> 3. WSCI 框架</h2>\n<p>WSCI 框架是由 LangChain 团队提出的一种用于构建 AI Agent 的通用框架。WSCI 代表了四个关键组件：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">组件</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">写入 <br>Write</td>\n<td>将 Agent 的观察、思考和记忆存入记忆库<br>是信息产生的源头</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">选择 <br>Select</td>\n<td>从所有信息中挑选最相关的内容，是信噪比管理的关键<br>是信息获取的入口</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">压缩 <br>Compress</td>\n<td>“选择” 的辅助手段，如果信息过长时按需触发，通过总结或裁剪来节省 Token</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">隔离 <br>Isolate</td>\n<td>顶层架构决策，为不同的子任务或子 Agent 划分独立上下文，避免信息干扰</td>\n</tr>\n</tbody>\n</table>\n<p>关于写入和压缩中在笔记 记忆工程 中有详细介绍。<br />\n关于选择在 检索工程 中有详细介绍。</p>\n<p>关于隔离，分有：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">多智能体之间隔离</td>\n<td>是分而治之的架构设计，每个子 Agent 都在完全独立的上下文窗口中工作<br>要评估：<br>1. 任务是否高度可并行？<br>2. 子任务是否需要深度搜索？<br>3. 架构升级带来的收益是否大于成本？</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">环境沙箱</td>\n<td>LLM 上下文只负责生成代码，代码执行交给外部隔离的执行环境</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">运行时状态对象</td>\n<td>将 Agent 的内部状态结构化为多个字段<br>通过 Schema 设计，预先定义信息边界和访问权限</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"五-kv-缓存命中\"><a class=\"markdownIt-Anchor\" href=\"#五-kv-缓存命中\"></a> 五. KV 缓存命中</h1>\n<p>在 Agent 中，KV 缓存命中 (Key-Value Cache Hit) 是一种优化技术，用于提高 LLM 在处理重复或相似请求时的效率和响应速度。</p>\n<h2 id=\"1-kv-缓存介绍\"><a class=\"markdownIt-Anchor\" href=\"#1-kv-缓存介绍\"></a> 1. KV 缓存介绍</h2>\n<p>KV 缓存是 LLM 推理引擎中利用前缀匹配 (Prefix Matching) 技术实现的一种缓存机制。它通过存储先前计算的结果 (注意力矩阵权值) 在后续请求中进行重用，从而避免重复计算，提高响应速度。</p>\n<p>举例来说，假设第一次调用时有：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt: [Prefix A] + [Content 1]</span><br></pre></td></tr></table></figure>\n<p>推理引擎会将完整计算 <code>[Prefix A]</code> 并将其以 Key/Value 向量存储缓存。</p>\n<p>第二次调用有</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt: [Prefix A] + [Content 2]</span><br></pre></td></tr></table></figure>\n<p>推理引擎检测到 <code>[Prefix A]</code> 已经存在缓存中，因此可以直接重用之前计算的 Key/Value 向量，只需计算 <code>[Content 2]</code> 部分，从而节省计算资源和时间。</p>\n<p>可能不是每个 LLM 都支持 KV 缓存，但是在设计 Agent 时要以这样的原则去设计。大部分支持 KV 缓存的 LLM 引擎会降低命中 token 的价钱。</p>\n<p>如下图是 DeepSeek 模型的价格表 (2026 年 2 月 3 日截取):</p>\n<p align=\"center\">\n<img src=\"/img/post/随笔/AI Agent/AI Agent-面向LLM的智能代理/deepseek价格.png\" alt=\"DeepSeek 模型价格表\" width=\"600\">\n</p>\n<hr />\n<h2 id=\"2-最大化-kv-缓存命中率的五大原则\"><a class=\"markdownIt-Anchor\" href=\"#2-最大化-kv-缓存命中率的五大原则\"></a> 2. 最大化 KV 缓存命中率的五大原则</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">原则</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">开启与选型</td>\n<td>选择支持 KV 缓存的 LLM 引擎，并确保在配置中启用该功能</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">保证会话保持</td>\n<td>分布式服务中会通过 Session ID 来确保同一会话的请求被路由到同一个推理引擎</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">保持前缀稳定</td>\n<td>任何字符或序列化顺序的改变都会导致缓存从变更点开始失效</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">上下文只追加</td>\n<td>在上下文末尾追加是缓存友好的<br>修改或删除中间部分是缓存不友好的</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">明确标记缓存断点</td>\n<td>某些框架不支持自动增量前缀缓存，需手动插入断点标记<br>要考虑潜在的缓存过期问题，至少确保断点包含系统提示的结尾</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"3-只追加策略\"><a class=\"markdownIt-Anchor\" href=\"#3-只追加策略\"></a> 3. 只追加策略</h2>\n<p>只追加策略不仅仅可以最大化 KV 缓存命中率，还能利用模型的注意力特性。</p>\n<p>LLM 往往对最末尾的上下文给予更多关注，因此将关键信息不断修改在上下文序列的末尾可以确保模型更好地关注这些信息，从而提升生成结果的相关性和准确性。其中复数循环就是一个典型应用场景：</p>\n<p>复数循环是让 Agent 在关键时期主动更新 <code>todo.md</code>：</p>\n<ul>\n<li>初始规划：受到复杂任务后，Agent 分解计划并首次写入 <code>todo.md</code>。</li>\n<li>核心复数：每完成一个子任务，就用包含最新进度的完成列表重写覆盖 <code>todo.md</code></li>\n</ul>\n<p>每个决策循环的开始，Agent 都会基于上下文 <strong>最末尾</strong> (最末尾的注意力最高) 的 <code>todo.md</code> 开进行思考：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 读取计划：&quot;哪些已经完成，哪些还未完成&quot;</span><br><span class=\"line\">2. 确定任务：&quot;下一个任务是什么&quot;</span><br><span class=\"line\">3. 指定行动：&quot;为了解决这个任务，我需要做什么&quot;</span><br></pre></td></tr></table></figure>\n<hr />\n<h1 id=\"六-多-agent-架构\"><a class=\"markdownIt-Anchor\" href=\"#六-多-agent-架构\"></a> 六. 多 Agent 架构</h1>\n<p>按职责为单一任务划分多 Agent 其实是错误的，需要更多的沟通成本。</p>\n<p>例如对于写代码的职责 （包括需求分析、设计、编码、测试等），划分为多个 Agent 反而会增加沟通成本，不同 Agent 之间不懂为什么前者要这样做。</p>\n<p>什么时候不能拆分：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">情况</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">同一工作的连续阶段</td>\n<td>高度依赖同一上下文的连续任务<br>例如需求分析、设计、编码、测试</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">紧密耦合的组件</td>\n<td>需要频繁来回确认细节、同步理解模块</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">需要持续共享状态</td>\n<td>必须不断对齐当前系统理解到哪一步</td>\n</tr>\n</tbody>\n</table>\n<p>什么时候可以拆分：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">情况</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">独立的研究路径</td>\n<td>无共享上下文</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">拥有清晰接口的组件</td>\n<td>只要接口定义明确，不需要了解内部实现细节</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">高度并行的任务</td>\n<td>任务之间没有依赖关系，可以同时进行</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">黑盒验证任务</td>\n<td>只需要运行验证、汇报结果，不需要理解过程</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"参考文献\"><a class=\"markdownIt-Anchor\" href=\"#参考文献\"></a> 参考文献</h1>\n<ul>\n<li><a href=\"https://www.bilibili.com/video/BV1PRvhBSEwx/\">video: 【闪客】你管这破玩意叫智能体？Manus背后的技术</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1iSxVz2Epq\">video 系列: 合集·上下文工程 context engineering</a></li>\n</ul>\n<hr />\n<h1 id=\"参考项目\"><a class=\"markdownIt-Anchor\" href=\"#参考项目\"></a> 参考项目</h1>\n<p>排名不分先后，内容只显示了与该文章相关的技术栈：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">项目名称</th>\n<th>项目包含内容</th>\n<th>项目仓库</th>\n<th>项目文档</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">MCP</td>\n<td>MCP 官方文档</td>\n<td></td>\n<td>- <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">wiki</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MemoryOS</td>\n<td>参考系统中内存管理原理实现的高效记忆管理<br>1. 记忆添加、查询、生成用户画像</td>\n<td>- <a href=\"https://github.com/BAI-LAB/MemoryOS\">github</a></td>\n<td>- <a href=\"https://arxiv.org/abs/2506.06326\">arxiv</a><br>- <a href=\"https://bai-lab.github.io/MemoryOS/docs\">wiki</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Langchain</td>\n<td>用于构建智能体和基于 LLM 的应用程序的框架<br>1. 长期记忆与短期记忆</td>\n<td>- <a href=\"https://github.com/langchain-ai/langchain\">github</a></td>\n<td>- <a href=\"https://docs.langchain.com/oss/python/langchain/overview\">wiki</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MoeChat</td>\n<td>语音交互系统，用语音和 AI 角色自然对话<br>1. 长期记忆查询 (针对时间)</td>\n<td>- <a href=\"https://github.com/AlfreScarlet/MoeChat/tree/main\">github</a></td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MaiBot</td>\n<td>具有拟人功能的 AI Agent<br>1. 模仿人类对话风格<br>2. Agent 行为规划<br>3. 情绪系统和表情包互动能力</td>\n<td>- <a href=\"https://github.com/Mai-with-u/MaiBot\">github</a></td>\n<td>- <a href=\"https://docs.mai-mai.org/\">wiki</a><br>- <a href=\"https://reference.langchain.com/python/\">api</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MoFox</td>\n<td>基于 <a href=\"https://github.com/Mai-with-u/MaiBot\">MaiBot</a> 的第三方分支<br>1. MCP 系统<br>2. 拓展记忆系统</td>\n<td>- <a href=\"https://github.com/MoFox-Studio/MoFox-Core\">github</a></td>\n<td>- <a href=\"https://deepwiki.com/MoFox-Studio/MoFox_Bot\">deep wiki</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">NagaAgent</td>\n<td>功能丰富的智能对话助手系统<br>1. 多 Agent 管理系统<br>2. MCP 系统</td>\n<td>- <a href=\"https://github.com/Xxiii8322766509/NagaAgent\">github</a></td>\n<td>- <a href=\"https://github.com/Xxiii8322766509/NagaAgent/blob/main/README.md\">github</a></td>\n</tr>\n</tbody>\n</table>\n","feature":true,"text":"有关面向大语言模型（LLM）的现代AI Agent的随笔，是 AI Agent 相关笔记的目录。...","permalink":"/post/随笔/AI Agent/AI Agent-面向LLM的智能代理","photos":[],"count_time":{"symbolsCount":"7.2k","symbolsTime":"7 mins."},"categories":[{"name":"随笔-Agent","slug":"随笔-Agent","count":5,"path":"api/categories/随笔-Agent.json"}],"tags":[{"name":"Prompt Engineering","slug":"Prompt-Engineering","count":2,"path":"api/tags/Prompt-Engineering.json"},{"name":"LLM","slug":"LLM","count":5,"path":"api/tags/LLM.json"},{"name":"Agent","slug":"Agent","count":5,"path":"api/tags/Agent.json"},{"name":"Context Engineering","slug":"Context-Engineering","count":4,"path":"api/tags/Context-Engineering.json"},{"name":"WSCI","slug":"WSCI","count":1,"path":"api/tags/WSCI.json"},{"name":"KV Cache","slug":"KV-Cache","count":1,"path":"api/tags/KV-Cache.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E9%9B%B6-%E7%9B%AE%E5%BD%95\"><span class=\"toc-text\"> 零. 目录</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E7%9B%AE%E5%BD%95\"><span class=\"toc-text\"> 1. 目录</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF\"><span class=\"toc-text\"> 2. 核心技术</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80-%E4%BB%8B%E7%BB%8D\"><span class=\"toc-text\"> 一. 介绍</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-ai-agent-%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E4%BA%8B\"><span class=\"toc-text\"> 1. AI Agent 要注意的事</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84-ai-agent\"><span class=\"toc-text\"> 2. 最简单的 AI Agent</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%8C-agent-%E7%AE%A1%E7%90%86%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B\"><span class=\"toc-text\"> 二. Agent 管理的数据类型</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B\"><span class=\"toc-text\"> 三. 提示词工程</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9B%9B-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B\"><span class=\"toc-text\"> 四. 上下文工程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B\"><span class=\"toc-text\"> 1. 为什么需要上下文工程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#i-%E7%AE%A1%E7%90%86%E5%A4%A7%E9%87%8F%E6%8F%90%E7%A4%BA%E8%AF%8D\"><span class=\"toc-text\"> i. 管理大量提示词</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ii-%E8%8A%82%E7%BA%A6-token-%E8%B5%84%E6%BA%90\"><span class=\"toc-text\"> ii. 节约 Token 资源</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#iii-%E9%81%BF%E5%85%8D%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98\"><span class=\"toc-text\"> iii. 避免长上下文引发的问题</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E4%B8%AD%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%B1%BB%E5%9E%8B\"><span class=\"toc-text\"> 2. 上下文工程中提示词类型</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-wsci-%E6%A1%86%E6%9E%B6\"><span class=\"toc-text\"> 3. WSCI 框架</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%94-kv-%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD\"><span class=\"toc-text\"> 五. KV 缓存命中</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-kv-%E7%BC%93%E5%AD%98%E4%BB%8B%E7%BB%8D\"><span class=\"toc-text\"> 1. KV 缓存介绍</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E6%9C%80%E5%A4%A7%E5%8C%96-kv-%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD%E7%8E%87%E7%9A%84%E4%BA%94%E5%A4%A7%E5%8E%9F%E5%88%99\"><span class=\"toc-text\"> 2. 最大化 KV 缓存命中率的五大原则</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-%E5%8F%AA%E8%BF%BD%E5%8A%A0%E7%AD%96%E7%95%A5\"><span class=\"toc-text\"> 3. 只追加策略</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%85%AD-%E5%A4%9A-agent-%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\"> 六. 多 Agent 架构</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\"><span class=\"toc-text\"> 参考文献</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83%E9%A1%B9%E7%9B%AE\"><span class=\"toc-text\"> 参考项目</span></a></li></ol>","author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"笔记导航","uid":"c9fd788a84f48b02fed840451084249f","slug":"笔记/笔记导航","date":"2099-01-01T15:59:59.000Z","updated":"2026-02-02T18:58:40.430Z","comments":true,"path":"api/articles/笔记/笔记导航.json","keywords":null,"cover":"/img/cover.jpg","text":"关于所有笔记导航的导航。...","permalink":"/post/笔记/笔记导航","photos":[],"count_time":{"symbolsCount":141,"symbolsTime":"1 mins."},"categories":[{"name":"导航","slug":"导航","count":8,"path":"api/categories/导航.json"}],"tags":[{"name":"Notes","slug":"Notes","count":1,"path":"api/tags/Notes.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":true},"next_post":{"title":"运筹学和线性规划导航","uid":"57f63cc34b2f916074a1b8447b1cf916","slug":"笔记/线性规划/线性规划导航","date":"2025-12-29T09:14:00.000Z","updated":"2025-12-31T00:35:16.127Z","comments":true,"path":"api/articles/笔记/线性规划/线性规划导航.json","keywords":null,"cover":"img/post/笔记/线性规划/导航/cover.jpg","text":"关于线性规划的笔记。...","permalink":"/post/笔记/线性规划/线性规划导航","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[{"name":"导航","slug":"导航","count":8,"path":"api/categories/导航.json"}],"tags":[{"name":"Linear Programming","slug":"Linear-Programming","count":5,"path":"api/tags/Linear-Programming.json"},{"name":"Operations Research","slug":"Operations-Research","count":5,"path":"api/tags/Operations-Research.json"},{"name":"Optimization","slug":"Optimization","count":3,"path":"api/tags/Optimization.json"},{"name":"Product-Mix Optimization Problems","slug":"Product-Mix-Optimization-Problems","count":1,"path":"api/tags/Product-Mix-Optimization-Problems.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":true}}