{"title":"随笔-AI Agent的记忆工程","uid":"d4bf4e42fb0c5c829e26c963c245cb13","slug":"随笔/AI Agent的记忆工程","date":"2026-02-02T05:41:00.000Z","updated":"2026-02-02T18:52:53.009Z","comments":true,"path":"api/articles/随笔/AI Agent的记忆工程.json","keywords":null,"cover":"img/post/随笔/AI Agent的记忆工程/cover.png","content":"<p><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent-%E9%9D%A2%E5%90%91LLM%E7%9A%84%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86\">&lt;返回 AI Agent 目录</a></p>\n<hr />\n<h1 id=\"一-介绍\"><a class=\"markdownIt-Anchor\" href=\"#一-介绍\"></a> 一. 介绍</h1>\n<p>AI Agent 的记忆工程是上下文工程中的 写入(Write)、压缩(Compaction) 机制。</p>\n<p>大语言模型的推理本身是不具备上下文记忆能力的，无法记住之前的对话内容或任务状态，每次推理都是独立的。如果任务涉及到多轮交互或长周期的上下文信息，就需要引入记忆管理机制来帮助模型维持状态和上下文连续性。</p>\n<p>长任务的问题主要有：</p>\n<ul>\n<li>上下文窗口有限 (token 限制)</li>\n<li>上下文越长导致上下文腐烂</li>\n<li>在长时间跨度中维持连贯的目标</li>\n</ul>\n<p>主要的解决方法有：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th>机制</th>\n<th>作用</th>\n<th>适用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">压缩<br>Compaction</td>\n<td>当上下文接近饱和时，通过 “元调用” 让模型自身对历史进行高保真总结，形成摘要。</td>\n<td>管理工作记忆，在不丢失核心信息的前提下为新信息腾出空间</td>\n<td>高连贯的对话流任务</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">结构化笔记<br>Structured Note</td>\n<td>赋予 Agent 读写外部文件或数据库的工具，使其主动记录关键决策与状态</td>\n<td>构建 “持久化记忆”，实现跨会话、跨周期的知识积累和状态恢复</td>\n<td>有明确里程碑的迭代式任务</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Sub-Agent 架构</td>\n<td>由 “主 Agent” 规划分解，将子任务分派给多个并行的 “子 Agent” 处理</td>\n<td>实现关注点分离，通过分而治之解决单一 Agent 无法处理的规模化复杂性</td>\n<td>可并行探索的复杂研究分析</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"二-短期记忆\"><a class=\"markdownIt-Anchor\" href=\"#二-短期记忆\"></a> 二. 短期记忆</h1>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">特征</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">颗粒度</td>\n<td>详细的过程、步骤、数据</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">生命周期</td>\n<td>任务级、会话内</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">作用</td>\n<td>保证当前任务的上下文连贯性</td>\n</tr>\n</tbody>\n</table>\n<p>短期记忆核心机制是存储与使用的分离。信息完整存储在窗口之外 (内存中)，仅在每次调用时选择性地载入。</p>\n<p>存储有两种大类型：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">类型</th>\n<th>说明</th>\n<th>优势</th>\n<th>劣势</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">显示存储</td>\n<td>让 Agent 调用写入工具、读取工具</td>\n<td>行为透明，可调试性强</td>\n<td>对模型要求高，Token 消耗大</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">隐式存储</td>\n<td>由框架处理，自动提取、执行写入内容<br>模型并不知道该存储的存在</td>\n<td>高效、自动化，更符合思考流<br> 1. 滑动窗口：只加载最近 N 轮思考链<br>2. 状态总结：只加载上一步的观察结果</td>\n<td>依赖框架实现</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"三-长期记忆\"><a class=\"markdownIt-Anchor\" href=\"#三-长期记忆\"></a> 三. 长期记忆</h1>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">特征</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">颗粒度</td>\n<td>提炼后的结论、经验、抽象知识</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">生命周期</td>\n<td>跨任务、跨会话、永久性</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">作用</td>\n<td>实现学习、进化和个性化</td>\n</tr>\n</tbody>\n</table>\n<p>长期记忆的关键在于如何从大量信息中智能地提取和存储有价值的知识。</p>\n<p>长期记忆有两个主要机制：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">机制</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Reflexion<br>反思</td>\n<td>1. 失败检测 (框架)<br>2. 生成元提示来总结错误 (框架 → LLM)<br>例如，生成一条可复用的规则来避免未来错误<br>3. 写入记忆</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Generative Agent<br>生成式代理</td>\n<td>1. 原始日志收集 (时间 + 地点 + 人物 + 动作) (框架)<br>2. 定时启动合成流程的 Prompt 链，例如 总结 + 新洞察<br>3. 写入记忆</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"1-三分法\"><a class=\"markdownIt-Anchor\" href=\"#1-三分法\"></a> 1. 三分法</h2>\n<p>三分法来分类和选择记忆</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">步骤</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">程序性记忆<br>如何做</td>\n<td>通过意图识别，动态选择并加载匹配当前人物的模块化指令集<br>例如 Skills 技术</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">情景记忆<br>发生了什么</td>\n<td>通过向量搜索，从实例库中动态检索与当前问题最相似的 Few-shot 实例</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">语义记忆<br>是什么</td>\n<td>通过 RAG/混合检索方式，选择事实性知识</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"四-压缩\"><a class=\"markdownIt-Anchor\" href=\"#四-压缩\"></a> 四. 压缩</h1>\n<p>压缩主要针对的是即将进入上下文窗口的内容、原始且未经处理的信息流。例如冗长的对话历史、一次 API 返回的海量文本。主要目的是在作为上下文前进行降噪和减负。</p>\n<p>LangChain 中主要有两大类核心技术：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">维度</th>\n<th>上下文总结<br>Summarization</th>\n<th>上下文裁剪<nr>Trimming</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">核心机制</td>\n<td>LLM 提炼</td>\n<td>规则过滤</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">优点</td>\n<td>1. 智能，能保留核心语义<br>2. 擅长处理非结构化文本</td>\n<td>1. 快速且成本极低<br>2. 对部分保留信息保真度 100%</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">缺点</td>\n<td>1. 慢、成本高<br>2. 有损压缩</td>\n<td>1. 笨拙，可能丢失关键信息<br>2. 需要人工精心设计规则</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">场景</td>\n<td>1. 处理巨型工具返回值<br>2. 总结长对话历史<br>3. Agent间工作交接</td>\n<td>1. 管理常规对话历史 (滑动窗口)<br>2. 清除已消化的旧信息<br>3. 轻量级分类模型</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h1>\n<ul>\n<li><a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\">Anthropic</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1Q7xVzBEbf\">video: 长期任务的上下文工程实践:压缩、笔记与多agent架构—Anthropic《AI Agent的高效上下文工程》</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV16qxCzTEww\">video: 上下文短期记忆与长期记忆的写入机制—langchain《context engineering》</a></li>\n</ul>\n<hr />\n<p><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent-%E9%9D%A2%E5%90%91LLM%E7%9A%84%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86\">&lt;返回 AI Agent 目录</a></p>\n","feature":false,"text":"有关大语言模型长周期任务的记忆管理随笔。...","permalink":"/post/随笔/AI Agent的记忆工程","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[{"name":"随笔-Agent","slug":"随笔-Agent","count":4,"path":"api/categories/随笔-Agent.json"}],"tags":[{"name":"LLM","slug":"LLM","count":4,"path":"api/tags/LLM.json"},{"name":"Agent","slug":"Agent","count":4,"path":"api/tags/Agent.json"},{"name":"Memory","slug":"Memory","count":1,"path":"api/tags/Memory.json"},{"name":"Short-Term Memory","slug":"Short-Term-Memory","count":1,"path":"api/tags/Short-Term-Memory.json"},{"name":"Long-Term Memory","slug":"Long-Term-Memory","count":1,"path":"api/tags/Long-Term-Memory.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80-%E4%BB%8B%E7%BB%8D\"><span class=\"toc-text\"> 一. 介绍</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%8C-%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86\"><span class=\"toc-text\"> 二. 短期记忆</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89-%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86\"><span class=\"toc-text\"> 三. 长期记忆</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E4%B8%89%E5%88%86%E6%B3%95\"><span class=\"toc-text\"> 1. 三分法</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9B%9B-%E5%8E%8B%E7%BC%A9\"><span class=\"toc-text\"> 四. 压缩</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83\"><span class=\"toc-text\"> 参考</span></a></li></ol>","author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"随笔-AI Agent的检索工程","uid":"2dbcaf6d8ccd6aa6ce71937af4c45000","slug":"随笔/AI Agent的检索工程","date":"2026-02-02T06:02:27.000Z","updated":"2026-02-03T10:41:21.498Z","comments":true,"path":"api/articles/随笔/AI Agent的检索工程.json","keywords":null,"cover":"img/post/随笔/AI Agent的检索工程/cover.png","text":"有关 AI Agent 的检索工程知识和思考，包含 RAG、Skills 技术。...","permalink":"/post/随笔/AI Agent的检索工程","photos":[],"count_time":{"symbolsCount":"3.3k","symbolsTime":"3 mins."},"categories":[{"name":"随笔-Agent","slug":"随笔-Agent","count":4,"path":"api/categories/随笔-Agent.json"}],"tags":[{"name":"LLM","slug":"LLM","count":4,"path":"api/tags/LLM.json"},{"name":"Agent","slug":"Agent","count":4,"path":"api/tags/Agent.json"},{"name":"Context Engineering","slug":"Context-Engineering","count":3,"path":"api/tags/Context-Engineering.json"},{"name":"RAG","slug":"RAG","count":1,"path":"api/tags/RAG.json"},{"name":"Skills","slug":"Skills","count":1,"path":"api/tags/Skills.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":false},"next_post":{"title":"AADS-高级算法和数据结构期末","uid":"bd67a29e57b431e7a8ac14fb82ce9f83","slug":"笔记/算法和数据结构/期末","date":"2026-01-06T05:49:58.000Z","updated":"2026-01-31T10:00:32.139Z","comments":true,"path":"api/articles/笔记/算法和数据结构/期末.json","keywords":null,"cover":"img/post/笔记/算法和数据结构/期末/cover.jpg","text":"用于记录AADS-高级算法和数据结构期末复习内容。...","permalink":"/post/笔记/算法和数据结构/期末","photos":[],"count_time":{"symbolsCount":"21k","symbolsTime":"19 mins."},"categories":[{"name":"笔记-算法和数据结构","slug":"笔记-算法和数据结构","count":2,"path":"api/categories/笔记-算法和数据结构.json"}],"tags":[{"name":"Algorithms","slug":"Algorithms","count":3,"path":"api/tags/Algorithms.json"},{"name":"Data Structures","slug":"Data-Structures","count":10,"path":"api/tags/Data-Structures.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":false}}