{"title":"随笔-AI Agent的记忆工程","uid":"d4bf4e42fb0c5c829e26c963c245cb13","slug":"随笔/AI Agent的记忆工程","date":"2026-02-02T05:41:00.000Z","updated":"2026-02-03T16:01:57.920Z","comments":true,"path":"api/articles/随笔/AI Agent的记忆工程.json","keywords":null,"cover":"img/post/随笔/AI Agent的记忆工程/cover.png","content":"<p><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent-%E9%9D%A2%E5%90%91LLM%E7%9A%84%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86\">&lt;返回 AI Agent 目录</a></p>\n<hr />\n<h1 id=\"一-介绍\"><a class=\"markdownIt-Anchor\" href=\"#一-介绍\"></a> 一. 介绍</h1>\n<p>AI Agent 的记忆工程是上下文工程中的 写入(Write)、压缩(Compaction) 机制。</p>\n<p>大语言模型的推理本身是不具备上下文记忆能力的，无法记住之前的对话内容或任务状态，每次推理都是独立的。如果任务涉及到多轮交互或长周期的上下文信息，就需要引入记忆管理机制来帮助模型维持状态和上下文连续性。</p>\n<h2 id=\"1-记忆管理的背景\"><a class=\"markdownIt-Anchor\" href=\"#1-记忆管理的背景\"></a> 1. 记忆管理的背景</h2>\n<p>长任务的问题主要有：</p>\n<ul>\n<li>上下文窗口有限 (token 限制)</li>\n<li>上下文越长导致上下文腐烂</li>\n<li>在长时间跨度中维持连贯的目标</li>\n</ul>\n<p>主要的解决方法有：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th>机制</th>\n<th>作用</th>\n<th>适用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">压缩<br>Compaction</td>\n<td>当上下文接近饱和时，通过 “元调用” 让模型自身对历史进行高保真总结，形成摘要。</td>\n<td>管理工作记忆，在不丢失核心信息的前提下为新信息腾出空间</td>\n<td>高连贯的对话流任务</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">结构化笔记<br>Structured Note</td>\n<td>赋予 Agent 读写外部文件或数据库的工具，使其主动记录关键决策与状态</td>\n<td>构建 “持久化记忆”，实现跨会话、跨周期的知识积累和状态恢复</td>\n<td>有明确里程碑的迭代式任务</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Sub-Agent 架构</td>\n<td>由 “主 Agent” 规划分解，将子任务分派给多个并行的 “子 Agent” 处理</td>\n<td>实现关注点分离，通过分而治之解决单一 Agent 无法处理的规模化复杂性</td>\n<td>可并行探索的复杂研究分析</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-将什么信息写入记忆\"><a class=\"markdownIt-Anchor\" href=\"#2-将什么信息写入记忆\"></a> 2. 将什么信息写入记忆</h2>\n<p>Agent 可写入的记忆有以下几类：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">类型</th>\n<th>说明</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">用户的指令</td>\n<td>用户明确要求 Agent 记住的信息<br>用户要求 Agent 做什么</td>\n<td>“用户要求扮演一个猫娘，每次说话后面必须要加上 “喵” ”</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">对用户的印象</td>\n<td>Agent 对用户偏好、习惯的观察和总结</td>\n<td>“用户喜欢简洁明了的回答风格”</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">用户的反馈</td>\n<td>用户对 Agent 行为的评价和建议<br>尤其是负反馈</td>\n<td>“用户表示 &lt;…&gt; 的回答不符合结构预期”</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">LLM 的回答</td>\n<td>LLM 生成的回答内容<br>包含对长上下文的总结或与用户的反馈一起作为示例</td>\n<td>“这是一个关于 xxx 的文章”</td>\n</tr>\n</tbody>\n</table>\n<p>在反馈中，要适当地引入多样性，防止记忆内容过于单一，导致模型输出趋同。</p>\n<hr />\n<h1 id=\"二-短期记忆\"><a class=\"markdownIt-Anchor\" href=\"#二-短期记忆\"></a> 二. 短期记忆</h1>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">特征</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">颗粒度</td>\n<td>详细的过程、步骤、数据</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">生命周期</td>\n<td>任务级、会话内</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">作用</td>\n<td>保证当前任务的上下文连贯性</td>\n</tr>\n</tbody>\n</table>\n<p>短期记忆核心机制是存储与使用的分离。信息完整存储在窗口之外 (内存中)，仅在每次调用时选择性地载入。</p>\n<p>存储有两种大类型：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">类型</th>\n<th>说明</th>\n<th>优势</th>\n<th>劣势</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">显示存储</td>\n<td>让 Agent 调用写入工具、读取工具</td>\n<td>行为透明，可调试性强</td>\n<td>对模型要求高，Token 消耗大</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">隐式存储</td>\n<td>由框架处理，自动提取、执行写入内容<br>模型并不知道该存储的存在</td>\n<td>高效、自动化，更符合思考流<br> 1. 滑动窗口：只加载最近 N 轮思考链<br>2. 状态总结：只加载上一步的观察结果</td>\n<td>依赖框架实现</td>\n</tr>\n</tbody>\n</table>\n<p>在实践中，往往是采用混合存储。在每一次关键发现时让 LLM 主动写入存储；在遗弃旧信息时由框架自动总结和压缩。</p>\n<hr />\n<h1 id=\"三-长期记忆\"><a class=\"markdownIt-Anchor\" href=\"#三-长期记忆\"></a> 三. 长期记忆</h1>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">特征</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">颗粒度</td>\n<td>提炼后的结论、经验、抽象知识</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">生命周期</td>\n<td>跨任务、跨会话、永久性</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">作用</td>\n<td>实现学习、进化和个性化</td>\n</tr>\n</tbody>\n</table>\n<p>长期记忆的关键在于如何从大量信息中智能地提取和存储有价值的知识。</p>\n<p>长期记忆有两个主要机制：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">机制</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Reflexion<br>反思</td>\n<td>1. 失败检测 (框架)<br>2. 生成元提示来总结错误 (框架 → LLM)<br>例如，生成一条可复用的规则来避免未来错误<br>3. 写入记忆</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Generative Agent<br>生成式代理</td>\n<td>1. 原始日志收集 (时间 + 地点 + 人物 + 动作) (框架)<br>2. 定时启动合成流程的 Prompt 链，例如 总结 + 新洞察<br>3. 写入记忆</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"1-三分法\"><a class=\"markdownIt-Anchor\" href=\"#1-三分法\"></a> 1. 三分法</h2>\n<p>三分法来分类和选择记忆</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">步骤</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">程序性记忆<br>如何做</td>\n<td>通过意图识别，动态选择并加载匹配当前人物的模块化指令集<br>例如 Skills 技术</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">情景记忆<br>发生了什么</td>\n<td>通过向量搜索，从实例库中动态检索与当前问题最相似的 Few-shot 实例</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">语义记忆<br>是什么</td>\n<td>通过 RAG/混合检索方式，选择事实性知识</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h1 id=\"四-裁剪-压缩和摘要\"><a class=\"markdownIt-Anchor\" href=\"#四-裁剪-压缩和摘要\"></a> 四. 裁剪、压缩和摘要</h1>\n<p>裁剪、压缩和摘要都是将即将进入上下文窗口的内容、原始且未经处理的信息流进行预处理的手段，达到降噪和减负的目的。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">机制</th>\n<th>说明</th>\n<th>可逆性</th>\n<th>保真度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">裁剪</td>\n<td>基于规则的过滤机制，去除无关信息<br>例如滑动窗口、关键词过滤</td>\n<td>往往是不可逆的</td>\n<td>对保留的信息是 100%<br>未保留的信息 0%</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">压缩</td>\n<td>将内容分包成不同的文件，避免一次性加载过多信息 <br>简单说，就是将内容直接保存在硬盘中，上下文只保留调用该文件的指针</td>\n<td>可逆的</td>\n<td>100%</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">摘要</td>\n<td>利用 LLM 对内容进行提炼，保留核心语义</td>\n<td>不可逆的</td>\n<td>有损压缩</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"1-卸载与压缩\"><a class=\"markdownIt-Anchor\" href=\"#1-卸载与压缩\"></a> 1. 卸载与压缩</h2>\n<p>卸载指的是将一次 工具/API 调用的内容从上下文窗口中移除，保留指针并转而存储在外部存储 (文件、数据库) 中。只有在需要时才重新加载。</p>\n<p>卸载的类型包含：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">卸载数据</td>\n<td>赋予 Agent 一个文件系统来保存和回忆信息<br> 1. 对抗遗忘：使用只追加策略，将计划写入 <code>todo.md</code>，保证不偏离目标<br>2. 跨对话记忆</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">卸载工具</td>\n<td>保持函数调用层极简，将 “动作” 卸载为外部脚本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">渐进式披露</td>\n<td>即 Skills 技术，详细请看 <a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent%E7%9A%84%E6%A3%80%E7%B4%A2%E5%B7%A5%E7%A8%8B\">AI Agent的检索工程</a></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-高质量的摘要\"><a class=\"markdownIt-Anchor\" href=\"#2-高质量的摘要\"></a> 2. 高质量的摘要</h2>\n<p>摘要一定要从原文中摘要，而不是压缩后的内容 (需要解压)。</p>\n<p>摘要是一个有损操作，如果提示词设计不当会导致永久丢失关键信息。因此不应该使用开放式的指令，而是定义一个结构化的表单或模板让 LLM 去填表：</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"bullet\">-</span> 我修改了哪些文件</span><br><span class=\"line\"><span class=\"bullet\">-</span> 用户的目标是什么</span><br><span class=\"line\"><span class=\"bullet\">-</span> 当前任务的进度如何</span><br></pre></td></tr></table></figure>\n<hr />\n<h1 id=\"五-ace-框架\"><a class=\"markdownIt-Anchor\" href=\"#五-ace-框架\"></a> 五. ACE 框架</h1>\n<p>在常见的自然语言反馈中常见以下问题：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">局限</th>\n<th style=\"text-align:center\">症状</th>\n<th style=\"text-align:center\">危害</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">简洁性偏见<br>Brevity Bias</td>\n<td style=\"text-align:center\">LLM 倾向于生成简短、通用的提示</td>\n<td style=\"text-align:center\">牺牲了多样性并忽略了特定领域的细节</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">上下文崩溃<br> Context Collapse</td>\n<td style=\"text-align:center\">Agent 倾向于压缩成极短的摘要</td>\n<td style=\"text-align:center\">累计的知识被突然丢弃</td>\n</tr>\n</tbody>\n</table>\n<p>ACE (Agentic Context Engineering) 框架则是解决上述问题的方案，采用生成、反思和策展的模块化流程进行增量式的更新：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模块</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">生成器<br>Generator</td>\n<td>通过查询和上下文手册为新的问题生成推理轨迹<br>揭示了有效策略和常见错误</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">反思器<br>Reflector</td>\n<td>将推理轨迹作为输入，批判和总结生成洞见，进行多轮迭代优化<br>结构化错题报告：<br>输入：“你是一个专家，找出当前轨迹出现了什么问题” <br>输出：JSON，结构化推理、错误识别(what)、主要原因分析(why)、正确方法、主要洞见 (哪些策略比较重要)、tags</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">策展器<br>Curator</td>\n<td>将反思器提取的经验凝练成紧凑的增量上下文条目，通过非 LLM 逻辑被高效地合并进现有的上下文<br>输入：“是一个知识策展大师，识别出哪些是缺失的、新的洞见”<br>输出：JSON，包括推理和决策。决策包括增加知识元数据和内容</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"1-delta-更新\"><a class=\"markdownIt-Anchor\" href=\"#1-delta-更新\"></a> 1. delta 更新</h2>\n<p>deleta 更新是指将上下文拆分成结构化条目 (使用 bullet)，仅对相关内容进行局部更新。每个 bullet 都包含 <code>[ID], [Helpful Counter], [Harmful Counter], [Content]</code>。</p>\n<p>在 Curator 模块中，只是用 <code>ADD</code> 指令来添加新的条目。使用非 LLM 逻辑只追加新条目或更新就条目的 Counter。</p>\n<h2 id=\"2-grow-and-refine机制\"><a class=\"markdownIt-Anchor\" href=\"#2-grow-and-refine机制\"></a> 2. grow-and-refine机制</h2>\n<p>grow-and-refine机制是支持上下文的持续扩展和冗余控制，通过语义去重和 Counter 维护，保证上下文紧凑且相关性强。使用定期或惰性地对 结构化条目 进行去重和精炼。</p>\n<ul>\n<li>Gorw：默认值追加新策略或更新旧策略的元数据</li>\n<li>Refine：周期性地优化 (使用 embeddings 相似度去重)，修剪掉重复冗余。</li>\n</ul>\n<hr />\n<h1 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h1>\n<ul>\n<li><a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\">Anthropic</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1Q7xVzBEbf\">video: 长期任务的上下文工程实践:压缩、笔记与多agent架构—Anthropic《AI Agent的高效上下文工程》</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV16qxCzTEww\">video: 上下文短期记忆与长期记忆的写入机制—langchain《context engineering》</a></li>\n</ul>\n<hr />\n<p><a href=\"%E9%9A%8F%E7%AC%94%2FAI%20Agent-%E9%9D%A2%E5%90%91LLM%E7%9A%84%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86\">&lt;返回 AI Agent 目录</a></p>\n","feature":false,"text":"有关大语言模型长周期任务的记忆管理随笔。...","permalink":"/post/随笔/AI Agent的记忆工程","photos":[],"count_time":{"symbolsCount":"3.5k","symbolsTime":"3 mins."},"categories":[{"name":"随笔-Agent","slug":"随笔-Agent","count":4,"path":"api/categories/随笔-Agent.json"}],"tags":[{"name":"LLM","slug":"LLM","count":4,"path":"api/tags/LLM.json"},{"name":"Agent","slug":"Agent","count":4,"path":"api/tags/Agent.json"},{"name":"Memory","slug":"Memory","count":1,"path":"api/tags/Memory.json"},{"name":"Short-Term Memory","slug":"Short-Term-Memory","count":1,"path":"api/tags/Short-Term-Memory.json"},{"name":"Long-Term Memory","slug":"Long-Term-Memory","count":1,"path":"api/tags/Long-Term-Memory.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80-%E4%BB%8B%E7%BB%8D\"><span class=\"toc-text\"> 一. 介绍</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86%E7%9A%84%E8%83%8C%E6%99%AF\"><span class=\"toc-text\"> 1. 记忆管理的背景</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E5%B0%86%E4%BB%80%E4%B9%88%E4%BF%A1%E6%81%AF%E5%86%99%E5%85%A5%E8%AE%B0%E5%BF%86\"><span class=\"toc-text\"> 2. 将什么信息写入记忆</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%8C-%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86\"><span class=\"toc-text\"> 二. 短期记忆</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89-%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86\"><span class=\"toc-text\"> 三. 长期记忆</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E4%B8%89%E5%88%86%E6%B3%95\"><span class=\"toc-text\"> 1. 三分法</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9B%9B-%E8%A3%81%E5%89%AA-%E5%8E%8B%E7%BC%A9%E5%92%8C%E6%91%98%E8%A6%81\"><span class=\"toc-text\"> 四. 裁剪、压缩和摘要</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E5%8D%B8%E8%BD%BD%E4%B8%8E%E5%8E%8B%E7%BC%A9\"><span class=\"toc-text\"> 1. 卸载与压缩</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E6%91%98%E8%A6%81\"><span class=\"toc-text\"> 2. 高质量的摘要</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%94-ace-%E6%A1%86%E6%9E%B6\"><span class=\"toc-text\"> 五. ACE 框架</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-delta-%E6%9B%B4%E6%96%B0\"><span class=\"toc-text\"> 1. delta 更新</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-grow-and-refine%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\"> 2. grow-and-refine机制</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83\"><span class=\"toc-text\"> 参考</span></a></li></ol>","author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"随笔-AI Agent的检索工程","uid":"2dbcaf6d8ccd6aa6ce71937af4c45000","slug":"随笔/AI Agent的检索工程","date":"2026-02-02T06:02:27.000Z","updated":"2026-02-03T17:13:45.990Z","comments":true,"path":"api/articles/随笔/AI Agent的检索工程.json","keywords":null,"cover":"img/post/随笔/AI Agent的检索工程/cover.png","text":"有关 AI Agent 的检索工程知识和思考，包含 RAG、Skills 技术。...","permalink":"/post/随笔/AI Agent的检索工程","photos":[],"count_time":{"symbolsCount":"3.3k","symbolsTime":"3 mins."},"categories":[{"name":"随笔-Agent","slug":"随笔-Agent","count":4,"path":"api/categories/随笔-Agent.json"}],"tags":[{"name":"LLM","slug":"LLM","count":4,"path":"api/tags/LLM.json"},{"name":"Agent","slug":"Agent","count":4,"path":"api/tags/Agent.json"},{"name":"Context Engineering","slug":"Context-Engineering","count":3,"path":"api/tags/Context-Engineering.json"},{"name":"RAG","slug":"RAG","count":1,"path":"api/tags/RAG.json"},{"name":"Skills","slug":"Skills","count":1,"path":"api/tags/Skills.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":false},"next_post":{"title":"AADS-高级算法和数据结构期末","uid":"bd67a29e57b431e7a8ac14fb82ce9f83","slug":"笔记/算法和数据结构/期末","date":"2026-01-06T05:49:58.000Z","updated":"2026-01-31T10:00:32.139Z","comments":true,"path":"api/articles/笔记/算法和数据结构/期末.json","keywords":null,"cover":"img/post/笔记/算法和数据结构/期末/cover.jpg","text":"用于记录AADS-高级算法和数据结构期末复习内容。...","permalink":"/post/笔记/算法和数据结构/期末","photos":[],"count_time":{"symbolsCount":"21k","symbolsTime":"19 mins."},"categories":[{"name":"笔记-算法和数据结构","slug":"笔记-算法和数据结构","count":2,"path":"api/categories/笔记-算法和数据结构.json"}],"tags":[{"name":"Algorithms","slug":"Algorithms","count":3,"path":"api/tags/Algorithms.json"},{"name":"Data Structures","slug":"Data-Structures","count":10,"path":"api/tags/Data-Structures.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":false}}