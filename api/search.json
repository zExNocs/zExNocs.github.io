[{"id":"c9fd788a84f48b02fed840451084249f","title":"笔记导航","content":" 一. 基础理论导航\n\n\n\n导航\n\n\n\n\n问题\n\n\n形式与语言与自动机理论\n\n\n符号人工智能SAI\n\n\n线性规划导航\n\n\n计算机安全\n\n\n\n 二. 算法导航\n\n\n\n导航\n\n\n\n\n算法和数据结构\n\n\n优化算法\n\n\n深度学习\n\n\n\n 三. 应用导航\n\n\n\n导航\n\n\n\n\n安卓开发\n\n\n\n","slug":"笔记/笔记导航","date":"2099-01-01T15:59:59.000Z","categories_index":"导航","tags_index":"Notes","author_index":"zExNocs"},{"id":"6393e4ab4d95c5009d47dbf92444dece","title":"线性规划-选择优化问题","content":"&lt;返回线性规划导航\n\n 一. 选择问题介绍\n选择性的问题简单说就是在多个备选方案中判断每个方案是否被选择，从而达到某种优化目标的问题。简单说就是为每一个方案设置一个 二进制 决策变量来表示该方案是否被选择。\n选择优化问题的搜索空间非常大，是 O(2n)O(2^n)O(2n)。\n问题总汇：\n\n\n\n问题\n说明\n约束\n解决要点\n\n\n\n\n最小子集覆盖问题SCP\n选择最少的物品集合覆盖所有特征\n覆盖所有特征\n一维变量表示物品是否被选择\n\n\n任务分配问题AOP\n将任务分配给工人，使总成本最小\n1. 任务被分配2. 工人最多分配量(如果有)\n二维变量表示任务是否分配给某个工人\n\n\n背包问题KP\n在容量限制下选择物品使总价值最大\n容量限制\n一维变量表示物品是否被选择\n\n\n多背包问题MKP\n多个背包下选择物品使总价值最大\n1. 容量限制2. 物品最多选一次\n二维变量表示物品是否被选择进入某个背包\n\n\n广义指派问题GAP\n将任务分配给工人，使总成本最小\n1. 任务被分配2. 工人时间限制\n二维变量表示任务是否分配给某个工人\n\n\n带成本的选择优化问题SPIF\n选择一个活动会产生固定成本和分配成本\n1. 变量依赖2. 满足分配约束\n1. 二元变量表示是否选择活动2. 要给变量选择如何分配活动3. 使用大 M 法处理变量依赖\n\n\n选址问题FLP\n选择合适的地点建设设施以满足顾客需求\n1. 变量依赖2. 满足顾客需求\n1. 二元变量表示是否在某地点建设设施2. 二维变量表示顾客在某地点满足需求量3. 使用大 M 法处理变量依赖\n\n\n装箱问题BPP\n将物品装入最少数量箱子\n1. 容量约束/变量依赖2. 物品选且只选一次\n1. 二维二元变量表示物品是否装入某箱子2. 二元变量表示是否选择这个箱子 3. 假设最大箱子个数为物品个数\n\n\n作业调度问题JSP\n在单个机器上调度多个作业以最小化总完成时间\n1. 同一时间只能有一个作业2. 逾期时间计算\n1. 变量表示任务开始时间2. 二维二进制变量表示两个作业的先后3. 两个变量表示逾期时间和提前时间\n\n\n最小生成树问题MST\n选择边使得图连通且总权值最小\n1. 连通约束2. 无环约束\n1. 二元变量表示边是否被选择2. 连通约束：边个数等于点个数减一3. 使用子环消除约束防止环的出现\n\n\n非对称旅行商问题ATSP\n选择路径使得旅行商经过所有城市且总成本最小\n1. 每个点入一次2. 每个点出一次3. 无环约束\n1. 二元变量表示边是否被选择2. 使用子环消除约束防止环的出现\n\n\n对称旅行商问题TSP\n选择路径使得旅行商经过所有城市且总成本最小\n1. 每个点连接固定两个边2. 无环约束\n1. 二元变量表示边是否被选择2. 使用子环消除约束防止环的出现\n\n\n\n 二. 选择优化问题的子问题解决方法\n选择优化问题往往伴随着一些子问题，这些子问题需要通过特定的方法进行处理才能转化为线性规划模型。\n 1. 大 M 法处理变量依赖\n选择优化问题往往涉及到变量之间的依赖关系，这些依赖关系需要通过特定的方法进行处理，才能将问题转化为线性规划模型。常用的方法是大 M 法 (Big-M Method)。\n假设我们有一个正常变量 XXX 和一个二进制选择决策变量 YYY，存在以下依赖约束：\n{X&lt;Bif  Y=0X&gt;Bif  Y=1\\begin{cases}\nX &lt; B &amp; \\text{if }\\ Y = 0 \\\\\nX &gt; B &amp; \\text{if }\\ Y = 1 \\\\\n\\end{cases}\n{X&lt;BX&gt;B​if  Y=0if  Y=1​\n i. 转化线性规划\n主要思路是使用大 M 法 (Big-M Method) 使得一个约束变成非有效约束。即将两个约束都变形地加入到建模中，让 YYY 取任意一个值时使得另一个约束变成非有效约束：\n\n\n\n情况\n结果\n做法\n\n\n\n\nY=0Y = 0Y=0\n使得 X&gt;BX &gt; BX&gt;B 约束失效\n将约束转变为 X&gt;−∞X &gt; -\\infinX&gt;−∞，这是恒成立的\n\n\nY=1Y = 1Y=1\n使得 X&lt;BX &lt; BX&lt;B 约束失效\n将约束转变为 X&lt;+∞X &lt; +\\infinX&lt;+∞，这是恒成立的\n\n\n\n ii. 具体转化方法\n\n将约束变形的方法：\n\n\n\n\n步骤\n公式\n说明\n\n\n\n\n将严格不等式替换\nX&lt;B⇒X≤B−ϵ1X &lt; B ⇒ X \\leq B - \\epsilon_1X&lt;B⇒X≤B−ϵ1​X&gt;B⇒X≥B+ϵ2X &gt; B ⇒ X \\geq B + \\epsilon_2X&gt;B⇒X≥B+ϵ2​\n1. ϵ\\epsilonϵ 是极小的数字，例如10−610^{-6}10−6；整数可以为 1112. 这是因为 &lt;&lt;&lt; 和 &gt;&gt;&gt; 不适用于线性规划3. 如果本来就是 ≤,≥\\leq, \\geq≤,≥，则 ϵ=0\\epsilon = 0ϵ=0\n\n\n引入 Big-M\nX≤B−ϵ1+M1YX≥B+ϵ2−M2(1−Y)\\begin{aligned}X &amp;\\leq B - \\epsilon_1 + M_1Y\\\\X &amp;\\geq B + \\epsilon_2 - M_2(1 - Y)\\end{aligned}XX​≤B−ϵ1​+M1​Y≥B+ϵ2​−M2​(1−Y)​\n1. M1M_1M1​ 是极大的数，要 M1≥Xmax−B+ϵ1M_1 \\geq X_{max} - B + \\epsilon_1M1​≥Xmax​−B+ϵ1​2. M2M_2M2​ 是极大的数，要 M2≥B+ϵ2−XminM_2 \\geq B + \\epsilon_2 - X_{min}M2​≥B+ϵ2​−Xmin​3. 可以取 M1=M2M_1 = M_2M1​=M2​4. 这是因为当 YYY 取任意一个值时，让其中一个约束变成无效约束，另一个约束为原约束\n\n\n\n\n要点：\n\n\n\n\n激活公式/要求\n新增公式\n说明\n\n\n\n\n&lt;&lt;&lt;\n− ϵ-\\ \\epsilon− ϵ\n要符合原公式\n\n\n&gt;&gt;&gt;\n+ ϵ+\\ \\epsilon+ ϵ\n要符合原公式\n\n\n≤\\leq≤\n+++\n增加足够的量来无效约束\n\n\n≥\\geq≥\n−-−\n减去足够的量来无效约束\n\n\n000\nMYMYMY\nY=1Y = 1Y=1 才会激活无效化\n\n\n111\nM(1−Y)M(1 - Y)M(1−Y)\nY=0Y = 0Y=0 才会激活无效化\n\n\n\n iii. 双向绑定关系\n变量往往是双向绑定的关系，也就是说：\n{X&lt;B  ⟺  Y=0X&gt;B  ⟺  Y=1\\begin{cases}\nX &lt; B \\iff Y = 0 \\\\\nX &gt; B \\iff Y = 1 \\\\\n\\end{cases}\n{X&lt;B⟺Y=0X&gt;B⟺Y=1​\n对于 X&lt;B→Y=0X &lt; B \\to Y = 0X&lt;B→Y=0 此时我们只需要新增一个约束：\nY≤MXY \\leq M X\nY≤MX\n其中 MMM 是一个极大的数字，取决于 XXX 的最小取值：\n\n主要是用于防止 X∈(0,1)X \\in (0, 1)X∈(0,1) 时 Y=0Y = 0Y=0 的情况。\nM=1XminM = \\frac{1}{X_{min}}M=Xmin​1​。\n如果 XXX 不是连续变量，而是整数变量，那么当 X≠0X \\neq 0X=0 时，X≥1X \\geq 1X≥1，那么可以取 M=1M = 1M=1。\n\n实际上，该约束是可以不添加的，因为我们的目标函数往往在 Yi=1Y_i = 1Yi​=1 时会增加成本，让 Yi=1Y_i = 1Yi​=1 的主要目的是为了激活 X&gt;BX &gt; BX&gt;B 的条件。因此对于当 X&lt;BX &lt; BX&lt;B 时，求解器自然会选择 Y=0Y = 0Y=0 来降低成本。\n iv. B=0B = 0B=0 的整数解情况\n考虑下面情况：\n{X=0if  Y=0X&gt;0if  Y=1\\begin{cases}\nX = 0 &amp; \\text{if }\\ Y = 0 \\\\\nX &gt; 0 &amp; \\text{if }\\ Y = 1 \\\\\n\\end{cases}\n{X=0X&gt;0​if  Y=0if  Y=1​\n其中 XXX 是整数变量，则线性化转化如下：\n\n\n\n步骤\n公式\n说明\n\n\n\n\n将严格不等式替换\nX=0⇒X≤0X = 0 ⇒ X \\leq 0X=0⇒X≤0X&gt;0⇒X≥1X &gt; 0 ⇒ X \\geq 1X&gt;0⇒X≥1\nϵ1=0\\epsilon_1 = 0ϵ1​=0ϵ2=1\\epsilon_2 = 1ϵ2​=1\n\n\n引入 Big-M\nX≤M1YX≥Y\\begin{aligned}X &amp;\\leq M_1Y\\\\X &amp;\\geq Y\\end{aligned}XX​≤M1​Y≥Y​\n1. 因为 B=0,ϵ1=0B = 0, \\epsilon_1 = 0B=0,ϵ1​=0，得出第一个式子2. 因为 Xmin=0X_{min} = 0Xmin​=0取 M2=B+ϵ2−Xmin=1M_2 = B + \\epsilon_2 - X_{min} = 1M2​=B+ϵ2​−Xmin​=1此时有 X≥1−(1−Y)X \\geq 1 - (1 - Y)X≥1−(1−Y)，化简得到第二个式子\n\n\n\n此外，也可以只根据 X=0  ⟺  Y=0X = 0 \\iff Y = 0X=0⟺Y=0 方向来建模：\n\n\n\n约束\n线性\n说明\n\n\n\n\nX=0→Y=0X = 0 \\to Y = 0X=0→Y=0\nY≤M2XY \\leq M_2 XY≤M2​XY≤XY \\leq XY≤X\n1. M2≥1M_2 \\geq 1M2​≥1 为一个极大的数字，取决于 XXX 最小取值2. M2M_2M2​ 用于防止 X∈(0,1)X \\in (0, 1)X∈(0,1) 时 Y=0Y = 0Y=0 的情况3. M2=1XminM_2 = \\frac{1}{X_{min}}M2​=Xmin​1​4. 如果 X≠0X \\neq 0X=0 时候 X≥1X \\geq 1X≥1，那么可以取 M2=1M_2 = 1M2​=1\n\n\nY=0→X=0Y = 0 \\to X = 0Y=0→X=0\nX≤M1YX \\leq M_1 YX≤M1​Y\n1. M1&gt;0M_1 &gt; 0M1​&gt;0 为一个极大数字，取决于 XXX 最大取值2. M1M_1M1​ 用于防止 XiXiXi 不能取大于 111 的数3. M1=XmaxM_1 = X_{max}M1​=Xmax​\n\n\n\n\n 2. 图论无环约束\n在一些选择优化问题中，可能会涉及到图论中的无环约束 (Acyclic Constraint)，即要求所选择的边不能形成环路。这种约束通常用于网络设计、路径规划等问题中。\n假设我们图论中点集合为 VVV，边集合为 EEE，XijX_{ij}Xij​ 表示连接两个点的边 (i,j)∈E(i, j) \\in E(i,j)∈E 是否被选择。\n i. 无向图环\n无向图环的定义：\n\n至少在三个节点的闭合路径\n如果一个图存在环，必定存在一个 点的子集：它们之间的边个数等于子集点的个数\n因此我们通常使用任意点的子集 S⊆VS \\subseteq VS⊆V 中这些点的边个数≤∣S∣−1\\leq |S| - 1≤∣S∣−1\n\n在无向图中，一条边有两种变量 XijX_{ij}Xij​ 和 XjiX_{ji}Xji​，因此需要消除重复计数的问题：\n\n\n\n消除重复方法\n说明\n\n\n\n\n环消除\n1. 也就是让 XijX_{ij}Xij​ 和 XjiX_{ji}Xji​ 最多只有一个为 1112. 即会有约束 Xij+Xji≤1X_{ij} + X_{ji} \\leq 1Xij​+Xji​≤13. 虽然理论上环至少要三个节点，但是在线性规划的建模这这两个点同一个边连接也属于环\n\n\n只建立一个\n1. 保证任意 e(i,j)e(i, j)e(i,j) 中 i&lt;ji &lt; ji&lt;j 恒成立2. 或者保证 e(i,j)e(i, j)e(i,j) 是无序的，也就是 e(i,j)e(i, j)e(i,j) 就是 e(j,i)e(j, i)e(j,i)\n\n\n\n使用环消除的方法，只需要 ∣S∣≥2|S| \\geq 2∣S∣≥2 即可，也就是包括任意两个节点的子集。而对于只建立一个的方法，其天然符合环消除的要求，即使 ∣S∣=2|S| = 2∣S∣=2 也使得边数 ≤∣S∣−1\\leq |S| - 1≤∣S∣−1 恒成立。\n总而言之，其建模的公式为：\n∑(i,j)(S×S)∩EXij≤∣S∣−1∀S⊆V,∣S∣≥2\\sum_{(i, j)}^{(S \\times S) \\cap E} X_{ij} \\leq |S| - 1 \\quad \\forall S \\subseteq V, |S| \\geq 2\n(i,j)∑(S×S)∩E​Xij​≤∣S∣−1∀S⊆V,∣S∣≥2\n这种方法也叫子环消除约束 (Subtour Elimination Constraints, SECs)。\n ii. 有向图哈密顿环\n有向图哈密尔顿环指一条经过图中所有顶点恰好一次，并且最终回到起始顶点的闭合路径。\n其有多种建模方法，例如割平面法 (Cutting Plane Method)、MTZ 约束 (Miller-Tucker-Zemlin Constraints) 等。\n当然也可以使用类似于无向图环的方式进行建模。因为在环消除中就无意间将无向图转化为了有向图的形式：\n∑(i,j)(S×S)∩EXij≤∣S∣−1∀S⊆V,∣S∣≥2\\sum_{(i, j)}^{(S \\times S) \\cap E} X_{ij} \\leq |S| - 1 \\quad \\forall S \\subseteq V, |S| \\geq 2\n(i,j)∑(S×S)∩E​Xij​≤∣S∣−1∀S⊆V,∣S∣≥2\n iii. 优化\n因为环约束具有指数增长性，实践时一般是先不添加环约束来生成解，出现环时手动添加禁止这个环的约束，然后以此类推不断地生成解不断地添加环约束。该方法是针对一个问题实例的。\n\n 三. 经典选择优化问题模型\n经典选择优化问题的关键只在定义清楚二进制选择变量和约束条件。\n 1. 最小子集覆盖问题 SCP\n最小子集覆盖问题(Set Covering Problem, SCP)是指给定一个物品集合和特征集合，每个物品包含若干特征，要求选择最少的物品集合，使得所选物品集合包含所有特征的问题。\n i. SCP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n物品集合 NNN\n通常是一系列数字编号 N=[1,∣N∣]N = [1, |N|]N=[1,∣N∣]\n\n\n特征集合 MMM\nM={F1,F2,…,F∣M∣}M = \\{F_1, F_2, \\dots, F_{|M|}\\}M={F1​,F2​,…,F∣M∣​}特征 FiF_iFi​ 实际上也类似于一个编号\n\n\n特征分配 CijC_{ij}Cij​\n表示如果物品 iii 包含特征 FjF_jFj​，则 Cij=1C_{ij} = 1Cij​=1否则 Cij=0C_{ij} = 0Cij​=0\n\n\n目标 SSS\n找到一个最小子集 S⊆NS \\subseteq NS⊆N 能够包含所有特征 MMM\n\n\n\n ii. SCP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 物品集合 NNN2. 特征集合 MMM3. 特征分配 CijC_{ij}Cij​\n\n\n决策变量Decision Variables\nXi∈{0,1},i∈NX_i \\in \\{0, 1\\}, i \\in NXi​∈{0,1},i∈N表示是否选择 iii 作为子集\n\n\n目标函数Objective Function\nmin⁡ Z=∑i=1NXi\\min \\ Z = \\sum_{i = 1}^N X_imin Z=∑i=1N​Xi​\n\n\n约束Constraints\n1. 包含特征 jjj: ∑i=1NCijXi≥1,∀j∈M\\sum_{i = 1}^N C_{ij}X_i \\geq 1, \\forall j \\in M∑i=1N​Cij​Xi​≥1,∀j∈M2. 二元约束：Xi∈{0,1}X_i \\in \\{0, 1\\}Xi​∈{0,1}\n\n\n\n iii. SCP 常见扩展约束\n\n\n\n约束\n建模\n\n\n\n\n选择至少 kkk 个特征 KjK_jKj​\n∑i=1NCijXi≥k\\sum_{i = 1}^N C_{ij}X_i \\geq k∑i=1N​Cij​Xi​≥k\n\n\n特征 KjK_jKj​ 和 特征 KpK_pKp​ 绑定\nCijXi=CipXi∀i∈NC_{ij}X_i = C_{ip}X_i \\quad \\forall i \\in NCij​Xi​=Cip​Xi​∀i∈N\n\n\n物品 iii 和 物品 ppp 绑定\nXi=XpX_i = X_pXi​=Xp​\n\n\n物品 iii 是 物品 ppp 的前置\nXi≥XpX_i \\geq X_pXi​≥Xp​\n\n\n\n\n 2. 分配优化问题 AOP\n分配优化问题 (Assignment Optimization Problem, AOP) 是指将一组任务分配给一组工人，工人可以分配多个任务，但是每个任务只能分配给一个工人，从而使得总的分配成本最小化的问题。\n i. AOP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n任务集合 TTT\n表示要被分配给工人的任务通常表示数字编号\n\n\n工人集合 WWW\n通常表示为数字编号\n\n\n成本 CijC_{ij}Cij​\n表示任务 iii 分配给工人 jjj 的成本\n\n\n目标 ZZZ\n将所有任务分配出去最小成本\n\n\n\n ii. AOP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 任务、工人集合 T,WT, WT,W2. 任务 iii 分配给 jjj 的成本 CijC_{ij}Cij​\n\n\n决策变量Decision Variables\nXij∈{0,1},∀i∈T,∀j∈WX_{ij} \\in \\{0, 1\\}, \\forall i \\in T, \\forall j \\in WXij​∈{0,1},∀i∈T,∀j∈W是否将任务 iii 分配给工人 jjj\n\n\n目标函数Objective Function\nmin⁡ Z=∑i=1T∑j=1WCijXij\\min\\ Z = \\sum_{i = 1}^{T}\\sum_{j = 1}^{W}C_{ij}X_{ij}min Z=∑i=1T​∑j=1W​Cij​Xij​\n\n\n约束Constraints\n1. 二元约束：Xij∈{0,1}∀i∈T,∀j∈WX_{ij} \\in \\{0, 1\\}\\quad \\forall i \\in T, \\forall j \\in WXij​∈{0,1}∀i∈T,∀j∈W2. 任务 iii 被分配：∑j=1WXij=1∀i∈T\\sum_{j = 1}^{W} X_{ij} = 1 \\quad \\forall i \\in T∑j=1W​Xij​=1∀i∈T3. 工人 jjj 被分配：∑iTXij=1\\sum_{i}^{T} X_{ij} = 1∑iT​Xij​=1 (根据题意)4. 工人 jjj 最多分配 kjk_jkj​ 个：∑iTXij≤kj\\sum_{i}^{T} X_{ij} \\leq k_j∑iT​Xij​≤kj​\n\n\n\n iii. AOP 变体\n可能有以下情况：\n\n\n\n变体\n说明\n\n\n\n\n任务和工人一对一分配\n可以使用匈牙利算法(Hungarian Algorithm)解决也就是二分图分配问题\n\n\n工人可以分配多个任务\n如所描述\n\n\n工人限制分配个数\n如所描述\n\n\n限制任务分配的工人\n如所描述\n\n\n\n\n 3. 背包问题 KP\n背包问题(Knapsack Problem, KP)是一个经典的 01 动态规划问题，其问题是指在给定容量限制的情况下，选择一组物品放入背包，使得背包内物品的总价值最大化。\n i. KP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n背包容量 BBB\n最多可以装下的容量数\n\n\n物品集合 NNN\n要装进背包的物品，每个物品只能选择一次通常是数字编号集合 [1,∣N∣][1, |N|][1,∣N∣]\n\n\n物品重量 wiw_iwi​\n物品 i∈Ni \\in Ni∈N 占背包容量数\n\n\n物品价值 viv_ivi​\n物品 i∈Ni \\in Ni∈N 的价值\n\n\n目标 ZZZ\n在不超过容量的情况下，背包可以拿下最大的价值\n\n\n\n ii. KP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 背包容量 BBB2. 物品集合 NNN3. 各个物品的重量和价值 wi,viw_i, v_iwi​,vi​\n\n\n决策变量Decision Variables\nXiX_iXi​ 表示是否选择这个物品\n\n\n目标函数Objective Function\nmax⁡ Z=∑i=1NviXi\\max\\ Z = \\sum_{i = 1}^{N} v_iX_imax Z=∑i=1N​vi​Xi​\n\n\n约束Constraints\n1. 容量约束：∑i=1NwiXi≤B\\sum^{N}_{i = 1}w_iX_i \\leq B∑i=1N​wi​Xi​≤B2. 二元约束： Xi∈{0,1}∀i∈NX_i \\in \\{0, 1\\} \\quad \\forall i \\in NXi​∈{0,1}∀i∈N\n\n\n\n\n 4. 多背包问题 MKP\n多背包问题(The Multiple Knapsack Problem)是普通背包的一个升级版本：\n\n拥有多个背包且有各自的容量\n每个物品进入不同的背包会有不同的价值\n\n i. MKP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n背包集合 MMM\n背包的集合，每个背包有各自的容量通常是数字编号集合 [1,∣M∣][1, |M|][1,∣M∣]\n\n\n背包容量 BjB_jBj​\n背包 j∈Mj \\in Mj∈M 的容量\n\n\n物品集合 NNN\n要装进背包的物品，每个物品只能选择一个背包\n\n\n物品重量 wiw_iwi​\n物品 i∈Ni \\in Ni∈N 占背包容量数\n\n\n物品价值 vijv_{ij}vij​\n物品 i∈Ni \\in Ni∈N 在进入背包 j∈Mj \\in Mj∈M 的价值\n\n\n目标 ZZZ\n选择哪个物品进入哪个背包，并且不超过任何一个背包的容量情况下最大的总价值\n\n\n\n ii. MKP 线性规划模型\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 背包、物品集合 M,NM, NM,N2. 各个背包容量 Bj,j∈MB_j, j\\in MBj​,j∈M3. 各个物品重量 wi,i∈Nw_i, i \\in Nwi​,i∈N4. 物品 iii 进入背包 jjj 的价值 vijv_{ij}vij​\n\n\n决策变量Decision Variables\nXijX_{ij}Xij​ 表示是否选择物品 iii 进入背包 jjj\n\n\n目标函数Objective Function\nmax⁡ Z=∑i=1N∑j=1MvijXij\\max\\ Z = \\sum_{i = 1}^{N}\\sum_{j = 1}^{M} v_{ij}X_{ij}max Z=∑i=1N​∑j=1M​vij​Xij​\n\n\n约束Constraints\n1. 容量约束：∑i=1NwiXij≤Bj∀j∈M\\sum^{N}_{i = 1}w_iX_{ij} \\leq B_j \\quad \\forall j \\in M∑i=1N​wi​Xij​≤Bj​∀j∈M2. 最多选一次约束：∑j=1MXij≤1∀i∈N\\sum_{j = 1}^{M} X_{ij} \\leq 1 \\quad \\forall i \\in N∑j=1M​Xij​≤1∀i∈N3. 二元约束：Xij∈{0,1}∀i∈N,∀j∈MX_{ij} \\in \\{0, 1\\} \\quad \\forall i \\in N, \\forall j \\in MXij​∈{0,1}∀i∈N,∀j∈M\n\n\n\n\n 5. 广义指派问题 GAP\n广义指派问题(Generalized Assignment Problem, GAP)是多背包(MKP)的一个扩展：\n\n每个任务(物品)都要被分配\n每个任务(物品)占工人(背包)的时间(容量)都不一样\n求最低成本而不是最高价值\n\n同时也是分配优化问题(AOP)的一个扩展：\n\n每个工人做各个任务都需要时间\n每个工人有需要的时间上限\n\n i. GAP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n任务集合 TTT\n表示要被分配给工人的任务通常表示数字编号\n\n\n工人集合 WWW\n通常表示为数字编号\n\n\n工人时间 tjt_jtj​\n工人 j∈Wj \\in Wj∈W 被分配任务的最大时间\n\n\n成本 cijc_{ij}cij​\n表示任务 i∈Ti \\in Ti∈T 分配给工人 j∈Wj \\in Wj∈W 的成本\n\n\n花费时间 wijw_{ij}wij​\n工人 j∈Wj \\in Wj∈W 做任务 i∈Ti \\in Ti∈T 所需要的时间\n\n\n目标 ZZZ\n将所有任务分配出去最小成本\n\n\n\n ii. GAP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 任务、工人集合 T,WT, WT,W2. 工人 jjj 总时间 tjt_jtj​3. 任务 iii 分配给工人 jjj 的成本 cijc_{ij}cij​4. 工人 jjj 做任务 iii 所需要时间 wijw_{ij}wij​\n\n\n决策变量Decision Variables\nXijX_{ij}Xij​ 表示是否将任务 iii 分配给工人 jjj\n\n\n目标函数Objective Function\nmin⁡ Z=∑i=1T∑j=1WcijXij\\min\\ Z = \\sum_{i = 1}^{T}\\sum_{j = 1}^{W} c_{ij}X_{ij}min Z=∑i=1T​∑j=1W​cij​Xij​\n\n\n约束Constraints\n1. 容量约束：∑i=1TwijXij≤tj∀j∈W\\sum^{T}_{i = 1}w_{ij}X_{ij} \\leq t_{j} \\quad \\forall j \\in W∑i=1T​wij​Xij​≤tj​∀j∈W2. 选择约束：∑j=1WXij=1∀i∈T\\sum_{j = 1}^{W} X_{ij} = 1 \\quad \\forall i \\in T∑j=1W​Xij​=1∀i∈T3. 二元约束：Xij∈{0,1}∀i∈T,∀j∈WX_{ij} \\in \\{0, 1\\} \\quad \\forall i \\in T, \\forall j \\in WXij​∈{0,1}∀i∈T,∀j∈W\n\n\n\n\n 6. 作业调度问题 JSP\n作业调度问题(Job Scheduling Problem, JSP)是指在一个机器上对多个作业进行调度，同一个时间下只有一个作业在进行，以最小化总完成时间或最大化资源利用率的问题。\n i. JSP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n作业集合 NNN\n需要被完成的作业通常是数字编号集合 [1,∣N∣][1, |N|][1,∣N∣]\n\n\n作业截止日期 did_idi​\n作业 iii 在规定日期前完成没有惩罚\n\n\n作业处理时间 tit_iti​\n作业 iii 完成一个作业需要的时间\n\n\n逾期单位惩罚 pip_ipi​\n作业 iii 如果没有在规定时间完成受到的单位时间惩罚\n\n\n目标 ZZZ\n在一个机器上处理作业的顺序，使得处理所有作业的总延迟惩罚最小化\n\n\n\n ii. JSP 线性规划分析\n对每个作业，定义：\n\n\n\n变量\n说明\n\n\n\n\nXiX_iXi​\n开始时间\n\n\nYijY_{ij}Yij​\n因为 i,ji, ji,j 都有可能先完成，这是一个或条件我们要将&quot;或&quot;条件转化成&quot;和&quot;条件，因此定义该变量1:1:1: 表示作业 iii 必须先于作业 jjj0:0:0: 表示作业 jjj 必须先于作业 iii\n\n\nSiE≥0S_i^E \\geq 0SiE​≥0\n表示任务 iii 提前完成的时间\n\n\nSiL≥0S_i^L \\geq 0SiL​≥0\n表示任务 iii 逾期时间\n\n\n\n有两个潜在的约束：\n\n\n\n潜在约束\n说明\n\n\n\n\n同一时间段只有一个作业\n1. Yij=1→Xj≥Xi+tiY_{ij} = 1 \\to X_j \\geq X_i + t_iYij​=1→Xj​≥Xi​+ti​2. Yij=0→Xi≥Xj+tjY_{ij} = 0 \\to X_i \\geq X_j + t_jYij​=0→Xi​≥Xj​+tj​都是 ≥\\geq≥，则激活符号为负，经过移项转化成完整的线性规划，就是：1. M(1−Yij)+Xj−Xi≥tiM(1 - Y_{ij}) + X_j - X_i \\geq t_iM(1−Yij​)+Xj​−Xi​≥ti​2. MYij+Xi−Xj≥tjMY_{ij} + X_i - X_j \\geq t_jMYij​+Xi​−Xj​≥tj​\n\n\n只计算超出逾期时间 SiS_iSi​\n1. 直接翻译出来就是 Si=max⁡(0,Xi+ti−di)S_i = \\max(0, X_i + t_i - d_i)Si​=max(0,Xi​+ti​−di​)但是很明显线性规划直接表达，因此要引入变量2. 保证 Xi+ti+SiE−SiL=diX_i + t_i + S_i^{E} - S_i^L = d_iXi​+ti​+SiE​−SiL​=di​，有两种情况：a. 提前：SiE=di−(Xi+ti)≥0S_i^{E} = d_i - (X_i + t_i) \\geq 0SiE​=di​−(Xi​+ti​)≥0 且 SiL=0S_i^{L} = 0SiL​=0b. 延迟：SiL=Xi+ti−di≥0S_i^{L} = X_i + t_i - d_i \\geq 0SiL​=Xi​+ti​−di​≥0 且 SiE=0S_i^{E} = 0SiE​=03. 因为加了 SiE,SiL≥0S_i^{E}, S_i^L \\geq 0SiE​,SiL​≥0，优化器会尽可能降低 SiLS_i^LSiL​ 的取值，所以找出的解必定只符合这两种情况之一 a. 提前：SiLS_i^LSiL​ 能取 000 肯定就取 000b. 延迟：如果取 SiE&gt;0S_i^E &gt; 0SiE​&gt;0，会增加 SiLS_i^LSiL​，所以也会取 SiE=0S_i^E = 0SiE​=0\n\n\n\n iii. JSP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1.NNN：作业集合2. did_idi​：作业 iii 截止日期3. tit_iti​：作业 iii 处理时间 4. pip_ipi​：作业 iii 逾期单位成本5. MMM：取一个极大的数\n\n\n决策变量Decision Variables\n1. XiX_iXi​：任务 iii 的开始时间2. YijY_{ij}Yij​：任务 iii 是否在 jjj 完成之前；只取 i&lt;ji &lt; ji&lt;j3. SiES_i^ESiE​：任务 iii 的提前完成时间4. SiLS_i^LSiL​：任务 jjj 的逾期时间\n\n\n目标函数Objective Function\nmin⁡ Z=∑iNpiSiL\\min\\ Z = \\sum_{i}^{N} p_iS_i^Lmin Z=∑iN​pi​SiL​\n\n\n约束Constraints\n1. 单作业1：Xj−Xi−MYij≥ti−M∀i∈N,∀(j&gt;i)∈NX_j - X_i - MY_{ij} \\geq t_i - M \\quad \\forall i \\in N, \\forall (j &gt; i) \\in NXj​−Xi​−MYij​≥ti​−M∀i∈N,∀(j&gt;i)∈N2. 单作业2：Xi−Xj+MYij≥tj∀i∈N,∀(j&gt;i)∈NX_i - X_j + MY_{ij} \\geq t_j \\quad \\forall i \\in N, \\forall (j &gt; i) \\in NXi​−Xj​+MYij​≥tj​∀i∈N,∀(j&gt;i)∈N3. 逾期时间计算：Xi+SiE−SiL=di−ti∀i∈NX_i + S_i^{E} - S_i^L = d_i - t_i \\quad \\forall i \\in NXi​+SiE​−SiL​=di​−ti​∀i∈N4. 二元约束：Yij∈{0,1}∀i∈N,∀(j&gt;i)∈NY_{ij} \\in \\{0, 1\\} \\quad \\forall i \\in N, \\forall (j &gt; i) \\in NYij​∈{0,1}∀i∈N,∀(j&gt;i)∈N5. 非负约束：Xi,SiE,SiL≥0∀i∈NX_i, S_i^E, S_i^L \\geq 0 \\quad \\forall i \\in NXi​,SiE​,SiL​≥0∀i∈N\n\n\n\n 四. 带变量依赖的选择优化问题\n带变量依赖的选择优化问题往往是指一个二进制决策变量控制另一个变量的取值范围的问题。\n涉及到两个变量之间的变量依赖关系，一般使用大 M 法进行处理，具体方法见第二节。\n 1. 带成本的选择优化问题 SPIF\n带成本的选择优化问题 (Selection Problem Involving Fixed-charges, SPIF) 是指选择一个活动会先产生一个固定的选择成本，然后再产生一个分配资源的成本。如果分配资源为 000，那么这个固定成本在目标函数中为 000。\n也就是\n{Xi=0if  Yi=0Xi&gt;0if  Yi=1\\begin{cases}\nX_i = 0 &amp; \\text{if }\\ Y_i = 0 \\\\\nX_i &gt; 0 &amp; \\text{if }\\ Y_i = 1 \\\\\n\\end{cases}\n{Xi​=0Xi​&gt;0​if  Yi​=0if  Yi​=1​\n i. SPIF 构成元素\n\n\n\n元素\n说明\n\n\n\n\n需要分配的资源量 NNN\n需要将这些资源分配给不同的活动\n\n\n活动集合 MMM\n通常是一系列数字编号 M=[1,∣M∣]M = [1, |M|]M=[1,∣M∣]\n\n\n活动固定成本 fif_ifi​\n给活动 iii 分配资源产生的固定成本\n\n\n活动分配成本 cic_ici​\n给活动 iii 分配资源后根据资源产生的成本\n\n\n目标 ZZZ\n将所有资源分配出去的最小成本\n\n\n\n那么假设对活动 iii 分配的资源量是 XiX_iXi​，那么该活动的成本为：\nZi={0if  Xi=0fi+ciXiif  Xi≠0Z_i = \n\\begin{cases}\n0 &amp; \\text{if }\\ X_i = 0 \\\\\nf_i + c_iX_i &amp; \\text{if }\\  X_i \\neq 0\n\\end{cases}\nZi​={0fi​+ci​Xi​​if  Xi​=0if  Xi​=0​\n难点在于这个是非线性模型(non-linear)。所以要将其建模成线性模型。\n ii. SPIF 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 需要分配的资源量 NNN2. 活动集合 MMM3. 活动固定成本 fif_ifi​4. 活动分配成本 cic_ici​\n\n\n决策变量Decision Variables\n1. 给活动 iii 的分配资源量 XiX_iXi​2. 是否添加活动 iii 的固定成本 Yi∈{0,1}Y_i \\in \\{0, 1\\}Yi​∈{0,1}\n\n\n目标函数Objective Function\nmin⁡ Z=∑i=1MfiYi+ciXi\\min\\ Z = \\sum_{i = 1}^{M}f_iY_i + c_iX_imin Z=∑i=1M​fi​Yi​+ci​Xi​\n\n\n约束Constraints\n1. Xi,YiX_i, Y_iXi​,Yi​ 绑定约束：a. Yi≤XiY_i \\leq X_iYi​≤Xi​b. Xi≤NYiX_i \\leq NY_iXi​≤NYi​2. 满足分配约束：∑i=1MXi=N\\sum_{i = 1}^{M} X_i = N∑i=1M​Xi​=N3. 二元约束：Yi∈{0,1}∀i∈MY_i \\in \\{0, 1\\}\\quad \\forall i \\in MYi​∈{0,1}∀i∈M4. 非负约束：Xi≥0∀i∈MX_i \\geq 0 \\quad \\forall i \\in MXi​≥0∀i∈M\n\n\n\n可以舍弃 Yi≤XiY_i \\leq X_iYi​≤Xi​ 约束，因为目标函数会自然优化 YiY_iYi​。\n\n 2. 选址问题 FLP\n选址问题(Facility Location Problems. FLP)是带成本的选择优化问题的升级版，它有多个顾客，每个顾客对同一个活动(设施)都有不同的成本。\n i. FLP 构成元素\n选址问题 (Facility Location Problems. FLP) 的主要元素如下：\n\n\n\n元素\n说明\n\n\n\n\n顾客集合 NNN\n需要被满足需求 did_idi​通常是数字编号集合 [1,∣N∣][1, |N|][1,∣N∣]\n\n\n需求 did_idi​\n顾客 iii 需要满足的需求约束\n\n\n地点集合 MMM\n可以为顾客提供需求的潜在设施地点通常是数字编号集合 [1,∣M∣][1, |M|][1,∣M∣]\n\n\n建设成本 fjf_jfj​\n在地点 jjj 建设设备的成本\n\n\n单位成本 cijc_{ij}cij​\n顾客 iii 在地点 jjj 满足单位需求的花费前提是该地点有建设设备\n\n\n目标 ZZZ\n选择合适的地点来建设该设施，以满足所有顾客需求的情况下最小化成本\n\n\n\nSPIF 问题相当于只有一个顾客和一个需求。\n ii. FLP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 顾客集合 NNN, 地点集合 MMM2. 各个顾客的需求 did_idi​3. 地点建设设施的成本 fif_ifi​4. 顾客 iii 在地点 jjj 满足单位需求的花费 cijc_{ij}cij​\n\n\n决策变量Decision Variables\n1. XijX_{ij}Xij​ 表示 顾客 iii 在地点 jjj 满足的需求量2. Yj∈{0,1}Y_j \\in \\{0, 1\\}Yj​∈{0,1} 是否在地点 jjj 建立设施\n\n\n目标函数Objective Function\nmin⁡ Z=∑j=1MfjYj+∑i=1N∑j=1McijXij\\min\\ Z = \\sum_{j = 1}^{M}f_jY_j + \\sum_{i = 1}^{N}\\sum_{j = 1}^{M}c_{ij}X_{ij}min Z=∑j=1M​fj​Yj​+∑i=1N​∑j=1M​cij​Xij​\n\n\n约束Constraints\n1. 满足需求约束：∑j=1MXij=Di∀i∈N\\sum_{j = 1}^M X_{ij} = D_i \\quad \\forall i \\in N∑j=1M​Xij​=Di​∀i∈N2. Xi,YiX_i, Y_iXi​,Yi​ 绑定：Xi≤LYiX_i \\leq LY_iXi​≤LYi​，其中 LLL 为最大分配值3. 二元约束：Yi∈{0,1}Y_i \\in \\{0, 1\\}Yi​∈{0,1}4. 非负约束：Xij≥0,∀i∈N,∀j∈MX_{ij} \\geq 0, \\forall i \\in N, \\forall j \\in MXij​≥0,∀i∈N,∀j∈M\n\n\n\n\n 3. 装箱问题 BP\n装箱问题(Bin Packing Problem)是将所有带容量的物品装入最少数量箱子的问题。\n i. BP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n箱子容量 BBB\n给定无限个容量为 BBB 的箱子\n\n\n物品集合 NNN\n要装入箱子的物品\n\n\n物品重量 wiw_iwi​\n物品 i∈Ni \\in Ni∈N 占箱子容量的大小wi≤Bw_i \\leq Bwi​≤B\n\n\n目标 ZZZ\n将所有物品装入箱子，且不超过各个箱子容量的情况下所需要最少的数量\n\n\n\n ii. BP 线性规划建模\n我们一般假设有 ∣N∣|N|∣N∣ 个箱子，即最大箱子数。\n因为我们要统计有多少个箱子被选择了，这一点同 SPIF 问题一样，需要一个额外的变量 YjY_jYj​ 来记录箱子 jjj 是否被选择。\n同理，因为是优化 YjY_jYj​ 的个数，所以可以舍去 ∑Xij=0→Yj=0\\sum X_{ij} = 0 \\to Y_j = 0∑Xij​=0→Yj​=0 的约束，让求解器与优化。\n对于 Yj=0→∑Xij=0Y_j = 0 \\to \\sum X_{ij} = 0Yj​=0→∑Xij​=0，有两种方法：\n\n如 SPIF 一样，限制 ∑Xij≤NYj\\sum X_{ij} \\leq NY_j∑Xij​≤NYj​\n和容量约束挂钩，即如果 Yj=0Y_j = 0Yj​=0 那么该箱子的容量为 000，即一个箱子的容量是 YjBY_jBYj​B。\n\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 物品集合 NNN2. 物品重量 wiw_iwi​3. 箱子集合 MMM，一般 ∣M∣=∣N∣|M| = |N|∣M∣=∣N∣4. 箱子容量 BBB\n\n\n决策变量Decision Variables\n1. XijX_{ij}Xij​ 表示是否将物品 iii 放入箱子 jjj2. YjY_jYj​ 表示箱子 jjj 是否有物品\n\n\n目标函数Objective Function\nmin⁡ Z=∑j=1MYj\\min\\ Z = \\sum_{j = 1}^{M} Y_jmin Z=∑j=1M​Yj​\n\n\n约束Constraints\n1. 每个物品只能选一次约束：∑j=1MXij=1∀i∈N\\sum^{M}_{j = 1}X_{ij} = 1\\quad \\forall i \\in N∑j=1M​Xij​=1∀i∈N2. 容量/变量绑定约束：∑i=1NSiXij≤YjB∀j∈M\\sum_{i = 1}^{N}S_iX_{ij} \\leq Y_j B \\quad \\forall j \\in M∑i=1N​Si​Xij​≤Yj​B∀j∈M3. 二元约束：Xij∈{0,1}∀i∈N,∀j∈MX_{ij} \\in \\{0, 1\\} \\quad \\forall i \\in N, \\forall j \\in MXij​∈{0,1}∀i∈N,∀j∈M4. 二元约束：Yj∈{0,1}∀j∈MY_j \\in \\{0, 1\\} \\quad \\forall j \\in MYj​∈{0,1}∀j∈M\n\n\n\n\n 五. 图论中的选择问题\n图论中的选择问题往往涉及到在图中选择节点或边，以满足某些特定的条件或优化目标。这些问题通常可以通过线性规划模型来解决。\n 1. 最小生成树问题 MST\n最小生成树问题(Minimum Spanning Tree Problem, MST) 是个经典的图论问题，其建立在无向图中。\n最小生成树有以下两个条件：\n\n\n\n条件\n实现方式\n\n\n\n\n(N−1)(N - 1)(N−1) 个边\n1. 对于 NNN 个节点的图，最小生成树一定有 N−1N - 1N−1 个边2. 只需要简单变量求和即可\n\n\n连接的点里没有环\n参考第二节无向图无环约束\n\n\n\n i. MST 构成元素\n\n\n\n元素\n说明\n\n\n\n\n点集合 VVV\n需要被全连接的图论点通常是数字编号集合 [1,∣V∣][1, |V|][1,∣V∣]\n\n\n边集合 EEE\n无向边 e(i,j)∈Ee(i, j) \\in Ee(i,j)∈E 表示连接点 i,j∈Vi, j \\in Vi,j∈V 的边\n\n\n边权值 WijW_{ij}Wij​\n表示边 e(i,j)e(i, j)e(i,j) 的连接权值\n\n\n目标 ZZZ\n选择 ∣V∣−1|V| - 1∣V∣−1 个边来全联通所有点 (没有环) 的最小成本值\n\n\n\n ii. MST 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 点、边集合：V,EV, EV,E2. 边权值：W(ij)W_{(ij)}W(ij)​\n\n\n决策变量Decision Variables\nX(ij)X_{(ij)}X(ij)​ 表示是否选择连接点 i,ji, ji,j 的边其中 X(ij)X_{(ij)}X(ij)​ 与 X(ji)X_{(ji)}X(ji)​ 等价\n\n\n目标函数Objective Function\nmin⁡ Z=∑e(i,j)EW(ij)X(ij)\\min \\ Z = \\sum_{e(i, j)}^{E}W_{(ij)}X_{(ij)}min Z=∑e(i,j)E​W(ij)​X(ij)​\n\n\n约束Constraints\n1. 连接所有点：∑e(i,j)EX(ij)=∣V∣−1\\sum_{e(i, j)}^{E} X_{(ij)} = |V| - 1∑e(i,j)E​X(ij)​=∣V∣−12. 无环约束：∑e(i,j)(S×S)∩EX(ij)≤∣S∣−1,∀S⊆V\\sum_{e(i, j)}^{(S \\times S) \\cap E} X_{(ij)} \\leq |S| - 1, \\quad \\forall S \\subseteq V∑e(i,j)(S×S)∩E​X(ij)​≤∣S∣−1,∀S⊆V3. 二元约束：X(ij)∈{0,1}∀e(i,j)∈EX_{(ij)} \\in \\{0, 1\\} \\quad \\forall e(i, j) \\in EX(ij)​∈{0,1}∀e(i,j)∈E\n\n\n\n\n 2. 非对称旅行商问题 ATSP\n旅行商问题是一个经典的 NP-hard 问题。\n非对称旅行商问题 (Asymmetric Traveling Salesman Problem, ATSP)是在有向图的旅行商问题。\n与最小生成树不同是一个树结构不同，旅行商问题是一条 链 连起来的，因此会有两个潜在约束：\n\n\n\n潜在约束\n说明\n\n\n\n\n每个点只拜访一次\n1. 这是因为旅行商问题要求是一个哈密顿回路/路径2. 一般使用流量守恒约束，也就是入点和出点都是 1113.也就是 ∑i=1VXij=∑k=1VXjk=1\\sum_{i = 1}^V X_{ij} = \\sum_{k = 1}^V X_{jk} = 1∑i=1V​Xij​=∑k=1V​Xjk​=1\n\n\n旅行路径没有回环\n1. 保证是一个链，而不是一些独立的环组成的路径2. 对可能形成环或子回路的每个边子集设置约束\n\n\n\n下图是如果没有约束回环可能会出现的情况：\n\n\n\n i. ATSP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n点集合 VVV\n表示旅行商的城市通常是数字编号集合 [1,∣V∣][1, |V|][1,∣V∣]\n\n\n边集合 EEE\n有向边 e(i,j)∈Ee(i, j) \\in Ee(i,j)∈E 表示连接点 i∈Vi \\in Vi∈V 到点 j∈Vj \\in Vj∈V 的边\n\n\n边权值 WijW_{ij}Wij​\n表示经过边 e(i,j)e(i, j)e(i,j) 的成本会出现 Wij≠WjiW_{ij} \\neq W_{ji}Wij​=Wji​ 的情况\n\n\n目标 ZZZ\n求旅行商依次经过所有点的最小成本\n\n\n\n ii. ATSP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 点、边集合：V,EV, EV,E2. 边权值：WijW_{ij}Wij​\n\n\n决策变量Decision Variables\nXijX_{ij}Xij​ 表示是否经过边 e(i,j)e(i, j)e(i,j)\n\n\n目标函数Objective Function\nmin⁡ Z=∑e(i,j)EWijXij\\min \\ Z = \\sum_{e(i, j)}^{E}W_{ij}X_{ij}min Z=∑e(i,j)E​Wij​Xij​\n\n\n约束Constraints\n1. 入点边约束：∑i=1VXij=1∀j∈V\\sum_{i = 1}^V X_{ij} = 1 \\quad \\forall j \\in V∑i=1V​Xij​=1∀j∈V2. 出点边约束：∑k=1VXjk=1∀j∈V\\sum_{k = 1}^V X_{jk} = 1 \\quad \\forall j \\in V∑k=1V​Xjk​=1∀j∈V3. 无环约束：∑e(i,j)(S×S)∩EXij≤∣S∣−1,∀S⊆V\\sum_{e(i, j)}^{(S \\times S) \\cap E} X_{ij} \\leq |S| - 1, \\quad \\forall S \\subseteq V∑e(i,j)(S×S)∩E​Xij​≤∣S∣−1,∀S⊆V4. 二元约束：Xij∈{0,1}∀eij∈EX_{ij} \\in \\{0, 1\\} \\quad \\forall e_{ij} \\in EXij​∈{0,1}∀eij​∈E\n\n\n\n因为环约束具有指数增长性，实践时一般是先不添加环约束来生成解，出现环时手动添加禁止这个环的约束，然后以此类推不断地生成解不断地添加环约束\n\n 3. 对称旅行商问题 TSP\n对称旅行商问题是经典旅行社问题，其建立在无向图中。\n因为是无向图，通常使用以下方法消除重复：\n\n\n\n消除重复方法\n说明\n\n\n\n\n环消除\n1. 在消除环中，会有约束 Xij+Xji≤1X_{ij} + X_{ji} \\leq 1Xij​+Xji​≤12. 虽然理论上环至少要三个节点，但是在线性规划的建模这这两个点同一个边连接也属于环\n\n\n只建立一个\n1. 证任意 e(i,j)e(i, j)e(i,j) 中 i&lt;ji &lt; ji&lt;j 恒成立2. 或者保证 e(i,j)e(i, j)e(i,j) 是无序的，也就是 e(i,j)e(i, j)e(i,j) 就是 e(j,i)e(j, i)e(j,i)\n\n\n\n一般我们使用第二种方法，同非对称旅行商问题，我们通常偏向于逐渐加环约束的方式而不是一次性全加入。\n同非对称旅行商问题，其具有潜在约束：\n\n\n\n潜在约束\n说明\n\n\n\n\n每个点只拜访一次\n1. 拜访一次包含完整的出入2. 即 ∑j=1VXij=2\\sum_{j = 1}^{V} X_{ij} = 2∑j=1V​Xij​=2\n\n\n旅行路径没有回环\n同非对称性旅行商问题一样\n\n\n\n i. TSP 构成元素\n\n\n\n元素\n说明\n\n\n\n\n点集合 VVV\n表示旅行商的城市通常是数字编号集合 [1,∣V∣][1, |V|][1,∣V∣]\n\n\n边集合 EEE\n无向边 e(i,j)∈Ee(i, j) \\in Ee(i,j)∈E 表示连接点 i,j∈Vi, j \\in Vi,j∈V 的边\n\n\n边权值 WijW_{ij}Wij​\n表示经过边 e(i,j)e(i, j)e(i,j) 的成本Wij=WjiW_{ij} = W_{ji}Wij​=Wji​\n\n\n目标 ZZZ\n求旅行商依次经过所有点的最小成本\n\n\n\n ii. TSP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n注释\nEiE_iEi​ 表示经过点 iii 的边集合\n\n\n数据Data\n1. 点、边集合：V,EV, EV,E2. 边权值：W(ij)W_{(ij)}W(ij)​\n\n\n决策变量Decision Variables\nX(ij)X_{(ij)}X(ij)​ 表示是否经过边 e(i,j)e(i, j)e(i,j)其中 X(ij)X_{(ij)}X(ij)​ 与 X(ji)X_{(ji)}X(ji)​ 等价\n\n\n目标函数Objective Function\n1. 根据边：min⁡ Z=∑e(i,j)EW(ij)X(ij)\\min \\ Z = \\sum_{e(i, j)}^{E}W_{(ij)}X_{(ij)}min Z=∑e(i,j)E​W(ij)​X(ij)​2. 根据点：min⁡ Z=12∑iV∑eEiWeXe\\min \\ Z = \\frac{1}{2} \\sum_{i}^{V} \\sum_{e}^{E_i}W_eX_emin Z=21​∑iV​∑eEi​​We​Xe​这是因为一个边会被两个点分别算一次\n\n\n约束Constraints\n1. 点出入约束：∑eEiXe=2∀i∈V\\sum_{e}^{E_i} X_e = 2 \\quad \\forall i \\in V∑eEi​​Xe​=2∀i∈V2. 无环约束：∑e(i,j)(S×S)∩EX(ij)≤∣S∣−1,∀S⊆V\\sum_{e(i, j)}^{(S \\times S) \\cap E} X_{(ij)} \\leq |S| - 1, \\quad \\forall S \\subseteq V∑e(i,j)(S×S)∩E​X(ij)​≤∣S∣−1,∀S⊆V4. 二元约束：Xe∈{0,1}∀e∈EX_e \\in \\{0, 1\\} \\quad \\forall e \\in EXe​∈{0,1}∀e∈E\n\n\n\n同非对称性旅行商问题，因为环约束具有指数增长性，实践时一般是先不添加环约束来生成解，出现环时手动添加禁止这个环的约束，然后以此类推不断地生成解不断地添加环约束。\n\n&lt;返回线性规划导航\n","slug":"笔记/线性规划/选择优化问题","date":"2025-12-30T16:17:00.000Z","categories_index":"笔记-线性规划","tags_index":"Linear Programming,Operations Research,Optimization,Selection Problem,Set Covering Problem,Knapsack Problem,Traveling Salesman Problem,Job Scheduling Problem,Minimum Spanning Tree,Box Packing Problem,Big-M Method,Acyclic Constraint","author_index":"zExNocs"},{"id":"8051c06e186b73cebde981ccfe33cff6","title":"线性规划-网络流","content":"&lt;返回线性规划导航\n\n 一. 网络流优化\n在这里主要介绍三种网络流问题：运输问题(Transportation Problem, TP)、最小消耗流问题(Minimum Cost Flow Problem, MCFP) 和 最大流问题 (Maximum Flow Problem, MFP)\n\n\n\n问题\n说明\n约束\n解决要点\n\n\n\n\n运输问题Transportation ProblemTP\n从多个源点向多个终点运输资源的最低成本\n1. 源点全流出2. 终点全满足\n计算好各个点的净流量\n\n\n最小成本流问题Minimum Cost Flow ProblemMCFP\n允许中转点下，多个源点向多个终点运输资源的最低成本\n1. 源点全流出2. 终点全满足3. 中转点净流量为 0004. 边容量限制\n计算好各个点的净流量\n\n\n最大流问题Maximum Flow ProblemMFP\n在有容量限制的网络中，最大化从单源点到单终点的流量\n1. 中转点净流量为 0002. 边容量限制\n计算好各个点的净流量\n\n\n\n 二. 运输问题 (TP)\n运输问题(Transportation Problem, TP)是定义在一个有向图网络中：\n\n\n\n成分\n说明\n数量\n流量\n\n\n\n\n源点Supply NodesSources\n提供的可用资源 sis_isi​\n多个\n有限流量\n\n\n终点Demand NodesDestinations\n需求的特定数量资源 did_idi​\n多个\n有限流量\n\n\n有向边\n1. 从源点 iii 到 终点 jjj 发送一个单位资源的成本 cijc_{ij}cij​2. 只有源点到终点的边，即流量直接从供应节点流向需求节点\n多个\n无限流量\n\n\n目标\n满足所有需求的前提下最低总运输成本 ZZZ\n\n\n\n\n\n\n\n\n 1. TP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 各个源点的供应量 sis_isi​2. 各个终点的需求量 did_idi​3. 每个边的成本 ci,jc_{i,j}ci,j​\n\n\n决策变量Decision Variables\nxijx_{ij}xij​ 表示从源点 iii 到终点 jjj 的边分配的资源 (流量)\n\n\n目标函数Objective Function\n最小化总成本：min⁡ Z=∑i=1n∑j=1mcijxij\\min\\ Z = \\sum^n_{i = 1}\\sum^m_{j = 1} c_{ij}x_{ij}min Z=∑i=1n​∑j=1m​cij​xij​nnn 是源点的个数，mmm 是终点的个数\n\n\n约束Constraints\n假设总供应量 等于 总需求量即 ∑i=1nsi=∑j=1mdi\\sum_{i=1}^{n}s_i = \\sum_{j=1}^{m}d_i∑i=1n​si​=∑j=1m​di​1. 各个源点全流出：∑j=1mxij=si,∀i∈[1,n]\\sum_{j = 1}^m x_{ij} = s_i, \\forall i \\in [1, n]∑j=1m​xij​=si​,∀i∈[1,n]2. 各个需求全满足：∑i=1nxij=dj,∀j∈[1,m]\\sum_{i = 1}^n x_{ij} = d_j, \\forall j \\in [1, m]∑i=1n​xij​=dj​,∀j∈[1,m]3. 非负需求：xij≥0,∀i∈[1,n],∀j∈[1,m]x_{ij} \\geq 0, \\forall i \\in [1, n], \\forall j \\in [1, m]xij​≥0,∀i∈[1,n],∀j∈[1,m]如果总供应量 与 总需求量不相等，则将多的一方的限制从 === 改为 ≤\\leq≤1. 总约束个数为 n+m+nmn + m + nmn+m+nm2. 因为供给 === 需求，实际上有效的最多 n+m+nm−1n + m + nm - 1n+m+nm−1 个\n\n\n\n 2. TP 性质\n\n\n\n性质\n说明\n\n\n\n\n整数性Integrality\n如果供应量 sis_isi​ 和需求量 did_idi​ 都为整数，那么最优解对各个边的分配 xijx_{ij}xij​ 一定为整数\n\n\n平衡性Balancing\n1. 供应量通常等于需求量2. 如果不平衡，则可以添加虚拟源或目的地3. 一般来说，都是将多的一方总量 === 改为 ≤\\leq≤ 实现\n\n\n\n 3. TP Excel 建模方法\n\n\n\n方案\n说明\n\n\n\n\n边建模\n1. 将每个边的源点、终点、花费、分配值直线列出来2. 分别统计各个点的流量和等于提供/需求值也就是包含这个点的边的分配值的和3. 单独列出目标函数\n\n\n点建模\n1. 横轴需求点，纵轴源点列出两个表格2. 第一个表格表示边的花费2. 第二个表格表示分配值3. 第二个表格横向扩展出各个源点的出流量4. 第二个表格纵向扩展出各个终点的入流量5. 单独列出目标函数\n\n\n\n\nExcel 建模运输问题样例\n\n\n\n\n\n 三. 最小成本流问题 (MCFP)\n最小成本流问题(Minimum Cost Flow Problem, MCFP)是定义在一个有向图网络中：\n\n\n\n成分\n说明\n数量\n流量\n\n\n\n\n源点Supply NodesSources\n提供的可用资源 sis_isi​\n多个\n有限流量\n\n\n终点Demand NodesDestinations\n需求的特定数量资源 did_idi​\n多个\n有限流量\n\n\n中转点Transshipment Nodes\n中转流，但净流量为 000\n多个\n000\n\n\n有向边\n1. 从点 iii 到 点 jjj 发送一个单位资源的成本 cijc_{ij}cij​2. 有限的流量 uiju_{ij}uij​2. 各个点之间都可能有有向边连接\n多个\n有限流量\n\n\n目标\n满足所有需求的前提下最低总运输成本 ZZZ\n\n\n\n\n\n\n\n\n 1. MCFP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 源点、终点、中转点集合 S,D,TS, D, TS,D,T2. 各个源点的供应量 sis_isi​3. 各个终点的需求量 did_idi​4. 每个边的成本 ci,jc_{i,j}ci,j​5. 每个边的容量 ui,ju_{i,j}ui,j​\n\n\n决策变量Decision Variables\nxijx_{ij}xij​ 表示从点 iii 到点 jjj 的边分配的资源 (流量)\n\n\n目标函数Objective Function\n最小化总成本：min⁡ Z=∑i=1n∑j=1ncijxij\\min\\ Z = \\sum^n_{i = 1}\\sum^n_{j = 1} c_{ij}x_{ij}min Z=∑i=1n​∑j=1n​cij​xij​nnn 是点的个数\n\n\n约束Constraints\n点 iii 的净流量： bi=∑j=1nxij−∑j=1nxjib_i = \\sum_{j = 1}^nx_{ij} - \\sum_{j = 1}^nx_{ji}bi​=∑j=1n​xij​−∑j=1n​xji​假设源点的总流量等于终点的总需求量1. 源点 iii 约束：bi=si,∀i∈Sb_i = s_i, \\forall i \\in Sbi​=si​,∀i∈S2. 终点 iii 约束：bi=−di,∀i∈Db_i = -d_i, \\forall i \\in Dbi​=−di​,∀i∈D3. 中转点约束：bi=0,∀i∈Tb_i = 0, \\forall i \\in Tbi​=0,∀i∈T4. 边流量约束：xij≤uij,∀i∈[1,n],∀j∈[1,n]x_{ij} \\leq u_{ij}, \\forall i \\in [1, n], \\forall j \\in [1, n]xij​≤uij​,∀i∈[1,n],∀j∈[1,n]5. 非负约束：xij≥0,∀i∈[1,n],∀j∈[1,n]x_{ij} \\geq 0, \\forall i \\in [1, n], \\forall j \\in [1, n]xij​≥0,∀i∈[1,n],∀j∈[1,n]\n\n\n\n 2. MCFP 性质\n\n\n\n性质\n说明\n\n\n\n\n整数性Integrality\n如果净流量 si,di,ui,js_i, d_i, u_{i,j}si​,di​,ui,j​ 是整数，那么最优解对各个边的分配 xijx_{ij}xij​ 一定为整数\n\n\n平衡性Balancing\n1. 供应量通常等于需求量2. 如果不平衡，则可以添加虚拟源或目的地3. 一般来说，都是将多的一方总量 === 改为 ≤\\leq≤ 实现\n\n\n\n 3. MCFP Excel 建模方法\n\n\n\n方案\n说明\n\n\n\n\n边建模\n1. 将每个边的起点、终点、花费、分配值、最大流量值直线列出来2. 约束分配值最大流量3. 分别统计各个点的净流量，根据其类型分别等于各自的值4. 单独列出目标函数\n\n\n点建模\n1. 各个点横轴、纵轴排列组合列出右、中、下三个表格2. 中间表格表示边的花销3. 右表格表示边的最大容量4. 下表格表示边的分配量，并横轴延申出各个点的出流，纵轴延伸出各个点的入流5. 下右列出各个点的净流量，并分别约束6. 单独列出目标函数\n\n\n\n\nExcel 建模 MCFP\n\n\n\n\n\n 4. 特殊情况\n最短路问题(Shortest Path) 是 MCFP 的一个特殊情况：只有一个源点和终点，且源点的出流量 === 终点的需求量 =1= 1=1，每个边的流量都是二元流。\n可以使用 Dijkstra 算法、Floyd 算法，也可以使用线性规划来高效地建模和求解。\n对于无向边，相当于两条方向相反的边。\n 四. 最大流问题 (MFP)\n最小成本流问题(Minimum Cost Flow Problem, MCFP)是定义在一个有向图网络中：\n\n\n\n成分\n说明\n数量\n流量\n\n\n\n\n源点Source Nodes\n提供无限的流量\n111\n无限流量\n\n\n终点Sink NodesDestination\n接收流量的点目标就是接收尽可能多的流量\n111\n目标流量\n\n\n中转点Transshipment Nodes\n中转流，但净流量为 000\n多个\n000\n\n\n有向边\n1. 点 iii 到点 jjj 非负的最大容量 cijc_{ij}cij​2. 该边分配的流量不能超过 cijc_{ij}cij​\n多个\n有限流量\n\n\n目标\n最大化终点接收的流量值\n\n\n\n\n\n最大流问题可以通过不断地建立增广路来解决。\n但本文主要是通过线性规划方式解决。\n\n\n\n 1. MFP 线性规划建模\n\n\n\n成分\n说明\n\n\n\n\n数据Data\n1. 源点 sss 终点 ddd2. 中转点集合 TTT3. 每个边的容量 ui,ju_{i,j}ui,j​\n\n\n决策变量Decision Variables\nxijx_{ij}xij​ 表示从点 iii 到点 jjj 的流量\n\n\n目标函数Objective Function\n最大化到终点的流量：max⁡ Z={∑i=1nxid(1)−bd(2)∑i=1nxsi(3)bs(4)\\max\\ Z = \\begin{cases}\\sum_{i = 1}^{n}x_{id}&amp;(1)\\\\-b_d&amp;(2)\\\\\\sum_{i = 1}^n x_{si}&amp;(3)\\\\b_s&amp;(4)\\end{cases}max Z=⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​∑i=1n​xid​−bd​∑i=1n​xsi​bs​​(1)(2)(3)(4)​\n\n\n约束Constraints\n点 iii 的净流量： bi=∑j=1nxij−∑j=1nxjib_i = \\sum_{j = 1}^nx_{ij} - \\sum_{j = 1}^nx_{ji}bi​=∑j=1n​xij​−∑j=1n​xji​1. 中转点约束：bi=0,∀i∈Tb_i = 0, \\forall i \\in Tbi​=0,∀i∈T2. 边流量约束：xij≤uij,∀i∈[1,n],∀j∈[1,n]x_{ij} \\leq u_{ij}, \\forall i \\in [1, n], \\forall j \\in [1, n]xij​≤uij​,∀i∈[1,n],∀j∈[1,n]3. 非负约束：xij≥0,∀i∈[1,n],∀j∈[1,n]x_{ij} \\geq 0, \\forall i \\in [1, n], \\forall j \\in [1, n]xij​≥0,∀i∈[1,n],∀j∈[1,n]\n\n\n\n 2. MFP Excel 建模\n\n单独列出所有的出点作为纵轴，横轴列出这个点连接的入点，作为这条边的代表。\n每个边都有两个参数，分别是其分配流量 xijx_{ij}xij​ 和容量 uiju_{ij}uij​，并添加 xij≤uijx_{ij} \\leq u_{ij}xij​≤uij​ 的约束。\n列出每个中转点的入流和出流，添加净流量为 000 的约束\n将源点的出流量或者终点的入流量作为目标函数\n\n\nExcel 建模 MFP\n\n\n\n\n\n\n&lt;返回线性规划导航\n","slug":"笔记/线性规划/网络流","date":"2025-12-30T04:52:00.000Z","categories_index":"笔记-线性规划","tags_index":"Linear Programming,Operations Research,Network Flow,Transportation Problem,Maximum Flow Problem","author_index":"zExNocs"},{"id":"e26b3b541bbfe6b639c5422d89a16083","title":"线性规划-后优化分析","content":"&lt;返回线性规划导航\n\n 一. 后优化分析\n后优化分析(Post-optimality Analysis)/假设分析(What-if Analysi) 是在找到最优解之后进行的，是考察当模型参数改变时解的变化情况，而无需从头开始重新求解问题。\n通过后优化分析，可以通过调整参数来探索其他模型和解。只要应用得当，可以只需要少量的迭代次数就能找到新的解。\n对于模型：\n\n\n\n模型\n解释\n\n\n\n\nLP\n主要适用于线性规划模型\n\n\nIP/BIP\n对于整数规划通常基于其线性规划松弛问题\n\n\nMIP\n可以基于线性规划松弛问题进行，也可以在将整数变量固定为最优值后，基于线性规划子问题进行\n\n\n\n通常分为：\n\n\n\n分析类型\n说明\n展示方向\n\n\n\n\n答案报告Answer Report\n1. 总结求解器找到的最优解2. 帮助决策者识别哪些资源已充分利用，以及哪些资源有剩余产能\n1. 目标函数的最优值2. 最优解下决策变量的值和类型(是否是连续的)3. 每个约束的状态是否有效以及松弛量(Slack)\n\n\n敏感性分析Sensitivity Analysis\n解释最优解，并评估参数变化对解和目标值的影响\n决策变量：1. 决策变量在目标函数中的系数2. 非基决策变量强行增加的后果3. 决策变量系数最大增加/减少多少会改变解结构约束：1. 各个约束的右手值 (资源数量)2. 资源数量增加导致目标函数的变化3. 资源数量最多增加/减少多少会变成解的结构\n\n\n限制报告Limits Report\n解释决策变量变成其符合约束的最小值或最大值，而其他变量保持不变时，目标函数值如何变化\n1. 每个决策变量的范围，包括下限和上限2. 每个变量到最小最大值目标函数的值\n\n\n\n要生成完整的后优化分析结果，所有决策变量都必须出现在每个约束中，最好采用标准形式。\n 二. Excel 答案报告\n在最大化问题的答案报告形式：\n\n目标函数：\n\n\n\n\n\nCell\nName\nOriginal Value\nFinal Value\n\n\n\n\n解释\n目标函数位置\n目标函数标签\n求解前的值\n目标函数最优值\n\n\n样例\n$I$8\nTotal Profit\n0\n1928.571429\n\n\n\n\n\n决策变量：\n\n\n\n\n\nCell\nName\nOriginal Value\nFinal Value\nInteger\n\n\n\n\n解释\n变量位置\n变量标签\n求解前的值\n最优值变量取值\n变量类型\n\n\n样例\n$H$5\nBacon Produced\n0\n60\nContin连续的\n\n\n\n\n\n约束：\n\n\n\n\n\nCell\nName\nCell Value\nFormula\nStatus\nSlack\n\n\n\n\n解释\n约束计算的位置\n计算的标签\n计算结果\n公式包括操作符、对比的值/位置\n是否有效\n松弛量\n\n\n样例 1\n$D$8\nUsed Flour\n428.7514\n$D$8 ≤\\leq≤ $D$10\nNot Binding无效的\n371.4286\n\n\n样例 2\n$E$8\nUsed Pork\n1500\n$E$8 ≤\\leq≤ $E$10\nBinding有效的\n0\n\n\n\n\n 三. Excel 敏感性报告\n在最大化 Excel 敏感性报告中：\n\n决策变量：\n\n\n\n\n\nCell\nName\nFinalValue\nReducedCost\nObjectiveCoefficient\nAllowableIncrease\nAllowableDecrease\n\n\n\n\n解释\n变量位置\n变量标签\n最优值变量取值\n减化成本\n目标函数中的系数利润/unit\n允许系数增量增加多少不会改变最优解结构\n允许系数减量减少多少不会改变最优解结构\n\n\n样例\n$H$5\nBacon Produced\n60\n0\n25\n1E+30\n17.8571\n\n\n\n注意：\n\nAllowable Increase 和 Allowable Decrease 是增加/减少的量，而不是最高/最低至多少。\n最优解结构指的是决策变量哪些是基变量，哪些不是；或者说是选取了哪个顶点\n\n其中在最大化问题的减化成本(Ruduced Cost)表示 (主要针对变量的 系数)：\n\n\n\n变量取值\n减化成本取值\nRuduced Cost 说明\n\n\n\n\n正\n000\n说明变量是基变量\n\n\n正\n正\n1. 变量卡在上边界2. 说明有优化空间，需要放多该资源\n\n\n000\n000\n1. 说明有多个解2. 保持约束的情况下一定范围内提高该变量对最终答案无影响\n\n\n000\n正\n1. 变量因为约束不能取值2. 如果该变量进入基会提升解的最优性3. 说明有优化空间，需要放多该资源\n\n\n000\n负\n1. 强制投入生产所造成的利润损失系数2. 或者说其利润 系数 必须提高多少才能使其进入最优解\n\n\n最小化问题\n\n其减化成本取值的解释刚好取反\n\n\n\n\n\n约束：\n\n\n\n\n\nCell\nName\nFinalValue\nShadowPrice\nConstraintR.H. Side\nAllowableIncrease\nAllowableDecrease\n\n\n\n\n解释\n约束计算的位置\n标签名\n计算结果\n影子价格\n约束右侧\n允许右侧增量增加多少不会改变最优结构\n允许右侧减量减少多少不会改变最优结构\n\n\n样例 1\n$D$8\nUsed Flour\n428.7514286\n0\n800\n1E+30\n371.4285714\n\n\n样例 2\n$E$8\nUsed Pork\n1500\n0.71428514\n1500\n1500\n1300\n\n\n\n注意：\n\nAllowable Increase 和 Allowable Decrease 是增加/减少的量，而不是最高/最低至多少。\n对于非有效的约束，往往其中一个是无穷；另一个表示增加/降低多少会变成具有约束力的 (往往等于 Slack)\n对于有效的约束，表示增加/降低多少变成不具有约束力的\n\n其中影子价格(Shadow Price) (针对的是 约束右手值)：\n\n\n\n约束性质\nShadow Price\nShadow Price 说明\n\n\n\n\nNot Binding\n必定为 000\n该约束不具有约束力\n\n\nBinding\n正\n表示右侧每增加一个单位，目标值会增加多少\n\n\nBinding\n负\n表示右侧每增加一个单位，目标值会减少多少\n\n\n\n此外：\n\n仅在该约束允许的增减范围内有效\n影子价格衡量的是增加一个单位稀缺资源的“价值”\nΔZ=(Shadow Price)×(ΔRHS)\\Delta Z = (\\text{Shadow Price}) × (\\Delta\\text{RHS})ΔZ=(Shadow Price)×(ΔRHS)\n\n\n\n\n英文\n表示\n\n\n\n\nincrease by 50\n增加了 50\n\n\nincrease up to 50\n最多增加到 50最终水平上限 50\n\n\nincrease by up to 50\n最多增加了 50范围最高 50\n\n\n\n\n 四. Excel 限制报告\n\n\n\n\nCell\nName\nValue\nLowerLimit\nObjectiveResult\nUpperLimit\nObjectiveResult\n\n\n\n\n解释\n单元格位置\n变量标签\n当前值\n最小取值\n结果\n最大取值\n结果\n\n\n举例\n$H$5\nBacon Produced\n60\n0\n428.5714286\n60\n1928.571429\n\n\n\n 五. 图方法\n\n\n\n属性\n如何分析\n\n\n\n\nSlack\n最优解的点，到约束超平面的垂直距离\n\n\n多个解\n过最优解点的目标函数超平面与可行域的交点个数与某个约束平行时候会产生多个点\n\n\n有效约束\n点位于哪些约束的超平面上\n\n\n资源增加的利润RHS\n取决于约束线的斜率\n\n\n\n\n&lt;返回线性规划导航\n","slug":"笔记/线性规划/后优化分析","date":"2025-12-30T04:33:00.000Z","categories_index":"笔记-线性规划","tags_index":"Linear Programming,Operations Research,Post-Optimality Analysis","author_index":"zExNocs"},{"id":"1ee97d196e793da1b513fe798274cce8","title":"线性规划-求解原理","content":"&lt;返回线性规划导航\n\n 一. 求解方案类型\n基于模型推导问题的解决方案：\n\n\n\n方法\n说明\n\n\n\n\n作图法Graphical Method\n1. 主要针对二维(具有两个决策变量)的线性规划模型2. 使用二维图来可视化决策变量、目标函数、约束条件、可行域、不可行与、搜索空间、可行解和最优解3. 通过探索可行域的顶点来找到解决方案\n\n\n单纯形法及其变体Simplex Method and Its Variants\n主要对于具有 3 个或更多决策变量的大型 LP 模型\n\n\n精确求解器/启发式方法Exact Solver or Heuristic Approach\n非常大的模型可能无法在实际计算时间内求解到精确最优解\n\n\n\n 二. 作图法\n作图法的步骤如下：\n\n\n\n步骤\n说明\n\n\n\n\n111\n绘制 x,yx, yx,y 的二维图\n\n\n222\n画出表示约束条件的直线\n\n\n333\n确定可行域和不可行域\n\n\n444\n确定包含最优解的约束条件 (顶点)\n\n\n555\n从可行域中的一个解开始，画出表示目标函数的直线假设目标函数 Z=ax+byZ = ax + byZ=ax+by即直线为 y=−abx+Zby = -\\frac{a}{b}x + \\frac{Z}{b}y=−ba​x+bZ​\n\n\n666\n通过在可行域内探索目标函数的更优值，确定能够改进目标函数的方向1. 如果 b&gt;0b &gt; 0b&gt;0 则往上平移 ZZZ 大；往下平移 ZZZ 小2. 如果 b&lt;0b &lt; 0b&lt;0 则往上平移 ZZZ 小；往上平移 ZZZ 大3. 如果 b=0b = 0b=0，则要根据 aaa 分析左右平移\n\n\n777\n通过求解代数方程组来验证最优解\n\n\n888\n确定有效约束和非有效约束\n\n\n\n同时也可能出现以下特殊情况：\n\n\n\n情况\n最优解\n\n\n\n\n不存在可行域\n不存在最优解\n\n\n可行域无界\n不存在确定的最优解\n\n\n存在多个最优解\n1. 正常，但一定有一个顶点是最优解2. 这表示 y=−abx+Zby = -\\frac{a}{b}x + \\frac{Z}{b}y=−ba​x+bZ​ 与可行域有多个交点3. 往往是目标函数与可行域的某条边重合4. 一定都在边界上5. 一定存在非角点的最优解6. 优化求解器通常只报告一个最优解，即它遇到的第一个最优解7. 可以通过逐步地添加限制来找到所有的点\n\n\n\n 三. 单纯形法\n单纯形法 (Simplex Method) 是一种用于求解线性规划问题的迭代算法。\n 1. 基本术语\n\n\n\n术语\n说明\n\n\n\n\n基本变量Basic VariablesBV\n基本变量控制解是否保持在可行域的边界内，它们通过求解方程（增广约束）来计算1. 这些变量对应于约束条件中的线性独立列2. 基本变量的数量等于约束条件的数量\n\n\n非基本变量Non-Basic VariablesNBV\n非基本变量在单纯形法中被设置为零，以简化计算1. 这些变量对应于约束条件中的线性依赖列2. 非基本变量的数量等于决策变量的总数减去基本变量的数量\n\n\n\n 2. 单纯形法原理\n它通过在可行域的顶点之间移动来寻找最优解，其原理是基于以下的几何概念：\n\n\n\n概念\n说明\n\n\n\n\n约束边界Constraint Boundaries\n线性规划问题的约束条件定义了一个多面体 (polyhedron)，其边界由约束条件的交点组成\n\n\n角点解Corner Point SolutionsCP\n多面体的顶点 (角点) 是潜在的最优解位置，因为线性目标函数在这些顶点上取得极值\n\n\n角点可行解Corner Point Feasible SolutionsCPF\n某些角点满足所有约束条件，因此被称为可行解\n\n\n基本可行解Basic Feasible SolutionsBFS\n每个角点对应一个基本可行解，这些解满足所有约束条件，并且是线性独立的1. 这些点一定是至少 nnn 个有效约束边的交点2. nnn 为决策变量的数量\n\n\n角点不可行解Corner Point Infeasible Solutions\n某些角点可能不满足所有约束条件，因此被称为不可行解\n\n\n相邻角点Adjacent Corner Points\n两个角点如果只相差一个基本变量，则称为相邻角点1. 单纯形法通过在相邻角点之间移动来寻找更优解2. 这种移动是通过调整基本变量和非基本变量来实现的3. 如果不存在更优的相邻角点，则称该点为最优解\n\n\n\n 3. 单纯形法步骤\n该方法选择原点作为初始基本可行解 (BFS)，并沿着可行域的边界移动到达相邻的 CPF 解。如果找到更优的解，则继续移动，直到无法找到更优解为止，此时当前的 CPF 解即为最优解。\n假设为最大化问题，单纯形法的步骤如下：\n\n\n\n步骤\n说明\n\n\n\n\n1.1.1. 转化形式\n将线性规划问题转换为标准形式1. ∑jNaijXij≤bi\\sum_j^N a_{ij}X_{ij} \\leq b_i∑jN​aij​Xij​≤bi​2. Z=∑jNcjXjZ = \\sum_j^N c_j X_jZ=∑jN​cj​Xj​\n\n\n2.2.2. 引入变量\n1. 除了非负约束，引入松弛变量 YiY_iYi​ ，将不等式约束转换为等式约束2. 引入松弛变量的非负约束\n\n\n3.3.3. 初始解\n初始基础可行解 (BFS)1. 将 XijX_{ij}Xij​ 设置非基变量，值为 0002. 将 YiY_iYi​ 设置为基变量，值为 bib_ibi​\n\n\n4.4.4. 最优测试Optimality Test\n检查当前目标函数的非基变量的系数 σi\\sigma_iσi​1. 如果σi≤0∀i∈N\\sigma_i \\leq 0 \\quad \\forall i \\in Nσi​≤0∀i∈N 恒成立，则当前解为最优解2. 否则选择 最大 σi\\sigma_iσi​ 对应的变量进入基变量集合3. σi\\sigma_iσi​ 在基变量一定为 000\n\n\n5.5.5. 入基变量\n选择 σi\\sigma_iσi​ 最大 的变量作为入基变量 (一定是非基变量)\n\n\n6.6.6. 出基变量\n选择当前入基变量中允许增加的最大值对应的变量作为出基变量1. 计算允许增加的最大值 bi/aijb_i / a_{ij}bi​/aij​2. 选择 最小 的比值对应的变量作为出基变量\n\n\n7.7.7. 更新表格\n1. 使用高斯消元法更新单纯形表格，使得基向量的列成为单位矩阵2. 同时更新其在目标函数中的系数，也就是 σi\\sigma_iσi​\n\n\n8.8.8. 重复\n重复步骤 444 至 666，直到找到最优解为止\n\n\n\n最终得到的 σi\\sigma_iσi​ 就是变量 XiX_iXi​ 的影子价格 (Shadow Price)，表示增加一个单位的资源对目标函数的影响。\n更新 σi\\sigma_iσi​ 有两种方法：\n\n迭代法：通过高斯消元法一步一步更新 σi\\sigma_iσi​ 的值\n计算法：通过计算新的 σi\\sigma_iσi​，使用以下矩阵运算：\n\nσ=CB⋅B−1⋅A−C\\sigma = \\mathbf{C_B \\cdot B^{-1} \\cdot A - C}\nσ=CB​⋅B−1⋅A−C\n其中：\n\nCB\\mathbf{C_B}CB​：基变量的系数向量\nB−1\\mathbf{B^{-1}}B−1：基矩阵的逆矩阵\nA\\mathbf{A}A：系数矩阵\nC\\mathbf{C}C：目标函数的系数向量\n\n 4. 特殊情况\n\n\n\n情况\n说明\n\n\n\n\n无界解\n如果在步骤 666 中，所有 aij≤0a_{ij} \\leq 0aij​≤0，则问题无界解\n\n\n多重最优解\n目标函数方程中至少有一个非基变量的系数为零：1. 即并非所有非基变量都出现在目标函数中2. 或者说存在非基变量的 σi=0\\sigma_i = 0σi​=03. 也就是常说的取值为 000 的同时影子价格为 0004. 可以通过重复单纯形法来获得更多最优解：每次迭代都选择一个系数为零的非基变量作为进入目标函数的基变量\n\n\n\n 四. 分支定界法\n分支定界法 (Branch and Bound Method) 是一种用于求解整数线性规划问题的算法。其基本思想是通过求解其线性松弛问题来获得上界或下界，并通过分支和定界来逐步缩小搜索空间，最终找到最优整数解。\n对于具有一般整数或二进制决策变量的问题，寻找最优解需要探索以搜索树形式表示的搜索空间。搜索树代表了问题的所有可行解和不可行解。树的大小随一般整数和二进制决策变量的数量呈指数级增长。探索整个搜索树相当于执行穷举枚举过程，这在计算上可能并不实际。基于树搜索的求解技术尝试仅探索搜索空间中具有希望的区域。但是可以使用求其问题的线性松弛问题来进行减枝。\n 1. 分支定界法步骤\n分支定界法的基本原理是分而治之，其基本步骤为：\n\n\n\n步骤\n说明\n\n\n\n\n分支Branch\n1. 将整个可行解集划分成越来越小的子集，从而将问题分解2. 分支变量的顺序会影响找到最优解的速度\n\n\n边界Bound\n1. 计算子树或解集的边界（最佳可能解），通过放宽整数约束来确定现有（潜在最优）解的位置2. 计算边界时，需要对子问题进行松弛\n\n\n征服Conquer\n1. 当一个子问题被征服（解决）后，无需再对该子树进行分支2. 征服相当于消除搜索树的部分节点，从而提高搜索效率3. 征服子问题（节点）有三种方式:a. 通过求解松弛后的子问题，找到目前为止质量最佳的整数解b. 当前子问题的计算界限并不优于当前当前解c. 当前子问题不可行\n\n\n\n其具体步骤如下：\n\n\n\n步骤\n说明\n\n\n\n\n1.1.1. 初始化\n初始全局最优解 Z=−∞Z = - \\inftyZ=−∞\n\n\n2.2.2. 解松弛问题\n1. 求解当前节点的线性松弛问题，得到松弛解 X∗X^*X∗ 和目标函数值 Z∗Z^*Z∗2. 如果当前问题不可解，则将当前节点视为已征服\n\n\n3.3.3. 检查可行性\n1. 如果 X∗X^*X∗ 是整数解则将当前节点视为已征服；如果 Z∗&gt;ZZ^* &gt; ZZ∗&gt;Z，则更新全局最优解2. 如果 Z∗≤ZZ^* \\leq ZZ∗≤Z，则剪枝该子节点，视为已征服3. 如果 Z∗&gt;ZZ^* &gt; ZZ∗&gt;Z，则进行步骤 444 分支\n\n\n4.4.4. 分支\n1. 选择一个非整数变量 XjX_jXj​ 进行分支2. 创建两个子节点：a. 左子节点：添加约束 Xj≤⌊Xj∗⌋X_j \\leq \\lfloor X_j^* \\rfloorXj​≤⌊Xj∗​⌋b. 右子节点：添加约束 Xj≥⌈Xj∗⌉X_j \\geq \\lceil X_j^* \\rceilXj​≥⌈Xj∗​⌉\n\n\n5.5.5. 定界\n对每个子节点，重复步骤 222 至 444，直到所有节点均被征服\n\n\n\n关于剪枝：\n\n每个节点的松弛问题提供了该节点子树的上边界\n每个可行的整数解提供了一个整个树的下边界 (当前的全局最优解)\n如果一个节点的上边界小于或等于当前的下边界，则该节点及其子树可以被剪枝\n\n\n&lt;返回线性规划导航\n","slug":"笔记/线性规划/求解原理","date":"2025-12-29T12:14:00.000Z","categories_index":"笔记-线性规划","tags_index":"Linear Programming,Operations Research,Simplex Method,Branch and Bound,Optimization","author_index":"zExNocs"},{"id":"57f63cc34b2f916074a1b8447b1cf916","title":"运筹学和线性规划导航","content":" 一. 导航\n 1. 导航汇总\n\n\n\n导航\n说明\n\n\n\n\n求解原理\n包括作图法、单纯形法和分支定界法\n\n\n后优化分析\n主要针对 Excel 求解器的报告进行解读\n\n\n网络流\n包括运输问题、最小成本流问题和最大流问题的线性规划建模方法及其Excel建模方法\n\n\n选择优化问题\n包括如何使用大 M 法处理变量依赖、如何线性建模无环图，以及经典选择问题模型，包括最小子集覆盖、任务分配、选址、背包问题、装配问题、旅行商问题等。\n\n\n\n 2. 优化问题导航\n 二. 运筹学\n运筹学(Operations Research, OR) 是一门运用分析和数学方法辅助决策的广泛学科。其包括 优化(optimization), 模拟(simulation), 排队论(queuing theory) 和博弈论(game theory)。\n 三. 优化\n优化(Optimization)问题是运筹学的一个子集。运筹学提供问题背景，优化提供数学方法。\n 1. 优化问题的应用\n\n\n\n应用\n说明\n\n\n\n\n生产计划Production Planning\n确定最佳产品组合和资源分配\n\n\n供应链管理Supply Chain Management\n最大限度地降低运输和物流成本\n\n\n调度Scheduling\n提高效率的人员、机器或项目调度\n\n\n网络设计Network Design\n优化通信和运输网络\n\n\n能源系统Energy Systems\n负荷平衡、电网优化和可再生能源并网\n\n\n设计优化Design Optimization\n在保持性能的前提下降低成本或重量\n\n\n医疗保健Healthcare\n医院资源分配和治疗计划\n\n\n交通运输Transportation\n路线规划、交通流量管理\n\n\n金融Finance\n投资组合优化和风险管理\n\n\n\n 2. 优化模型的分类\n优化模型是：\n\n一组决策变量 (我们要找到的东西)\n一堆约束条件 (哪些组合是合规的)\n通常会有一个评价好坏的准则 (目标函数)\n\n分为三大类：\n\n\n\n成分\n说明\n其他\n举例\n\n\n\n\n数学规划Mathematical Programming\n1. 显式的目标函数2. 约束条件\n根据结果分为：1. 连续的 (Continuous)2. 整型 (Integer)3. 混合 (Mixed)根据建模分为：1. 线性的 (Linear)2. 非线性的(Non-linear)\n1. 饮食搭配问题2.航空公司机组排班问题3. 带二次风险–收益权衡的投资组合优化\n\n\n约束满足Constraint Satisfaction\n1. 只关注满足所有约束条件2. 不去明确地优化某个目标函数\n限制满足问题 CSP\n1. 数独求解2. 课程/考试排课\n\n\n非分析计算Non-analytical\n1. 解析的数学模型过于复杂2. 使用搜索、启发式或仿真来求解模型\n优化算法导航\n1. 遗传算法2. 爬山算法\n\n\n\n i. 数学规划\n数学规划根据建模方式分为：\n\n\n\n模型\n说明\n例子\n\n\n\n\n线性模型Linear Models\n目标函数和约束都是线性的\n1. min⁡(3x1+2x2)\\min(3x_1 + 2x_2)min(3x1​+2x2​)2. x1+x2≤10x_1 + x_2 \\leq 10x1​+x2​≤103. x1,x2≥0x_1, x_2 \\geq 0x1​,x2​≥0\n\n\n非线性模型Nonlinear Models\n目标函数和约束涉及到非线性的函数例如平方、乘积、指数\n1. min⁡(x12+x22)\\min(x_1^2+x_2^2)min(x12​+x22​)2. min⁡(x1x2)\\min(x_1x_2)min(x1​x2​)3. min⁡(ex1+ex2)\\min(e^{x_1} + e^{x_2})min(ex1​+ex2​)\n\n\n\n根据结果又分为：\n\n\n\n结果\n说明\n规划类型\n\n\n\n\n连续的Continuous\n可以取某个范围内的任何实数\n线性规划Linear ProgrammingLP\n\n\n整型Integer\n仅限整数值\n整型规划Integer ProgrammingIP\n\n\n二元整型Binary Integer\n1. 是整型的一种2. 只包含两个数字 {0,1}\\{0, 1\\}{0,1}\n二元整型规划Binary Integer ProgrammingBIP\n\n\n混合Mixed\n连续变量和整数变量的组合\n混合整型规划Mixed Integer Programming MIP\n\n\n\n本笔记主要是以数学规划为模型的线性规划。\n 3. 优化模型的元素\n优化模型的四大元素：\n\n\n\n元素\n说明\n例子\n\n\n\n\n数据Data\n问题提供的数值参数\n单位成本、容量、需求\n\n\n决策变量Decision Variables\n待求解模型的未知量\n最终生产量\n\n\n目标函数Objective Function\n评估决策组合的数学表达式最大化、最小化、精确值、范围\n总成本、总利润\n\n\n约束Constraints\n限制可行解集的条件1. 功能性约束(Functional)：表示资源限制或需求2. 非负约束：决策变量取值非负数\n资源限制、不超过容量\n\n\n\n其中对于约束来说，有标准线性形式(Standard Linear Form)，是一侧变量的线性组合和另一侧常数的线性组合，符号通常是小于、大于、等于和其组合：\na1x1+a2x2+a3x3+⋯≤Ca_1x_1 + a_2x_2 + a_3x_3 + \\dots \\leq C\na1​x1​+a2​x2​+a3​x3​+⋯≤C\n 4. 解决优化问题\n\n\n\n步骤\n说明\n\n\n\n\n明确\n明确数据、决策变量、目标函数、以及约束条件\n\n\n定义\n1. 问题应当被清晰准确地定义，具备明确的描述2. 不过在实际中通常需要做一些假设\n\n\n探索\n1. 如果问题定义不明确，则需要进一步探索实际情境2. 在建模和选择优化技术之前先把问题澄清和细化\n\n\n构建\n可以通过以下两种方式之一构建优化模型：1. 用直觉方式建立表格模型（例如 Excel），然后从表格中提取出代数形式的数学规划模型2. 先写出代数数学规划模型，再把模型输入优化求解器来求解\n\n\n\n 四. 线性/整数规划\n\n\n\n\n\n\n\n\n\n线性规划（Linear Programming, LP）问题是一种优化问题，其中 目标函数和所有约束条件 都是线性的。\n 1. 线性规划模型\n一个标准的线性规划模型主要用下面方式进行描述：\n\n\n\n成分\n说明\n公式\n\n\n\n\n连续的决策变量Continuous Linear Variables\n可以取值任意实数\nx1,x2,…x_1, x_2, \\dotsx1​,x2​,…\n\n\n问题参数Problem Parameters\n是已知的、确定的\nci,aij,bjc_i, a_{ij},b_jci​,aij​,bj​\n\n\n线性目标函数Linear Objective Function\n最大化或最小化，用于评估决策组合的数学表达式\nmax⁡ or min⁡Z=c1x1+c2x2+…\\max \\text{ or } \\min Z = c_1x_1 + c_2x_2 + \\dotsmax or minZ=c1​x1​+c2​x2​+…\n\n\n线性约束Linear Constraints\n约束决策变量，包括功能约束和非负约束\n{a11x1+a12x2+⋯≤b1a21x1+a22x2+⋯≤b1…x1,x2,⋯≥0\\begin{cases}a_{11}x_1 + a_{12}x_2 + \\dots \\leq b_1\\\\a_{21}x_1 + a_{22}x_2 + \\dots \\leq b_1\\\\\\dots\\\\x_1, x_2, \\dots \\geq 0\\end{cases}⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​a11​x1​+a12​x2​+⋯≤b1​a21​x1​+a22​x2​+⋯≤b1​…x1​,x2​,⋯≥0​\n\n\n\n为了定义一个线性规划模型，建议步骤：\n\n\n\n步骤\n说明\n\n\n\n\n111\n确定问题参数（数值数据）\n\n\n222\n定义决策变量\n\n\n333\n陈述目标函数（最大化或最小化 Z）\n\n\n444\n构建线性约束\n\n\n555\n定义非负性约束 (LP) 或整数约束 (IP)\n\n\n\n 2. ※ 线性规划性质\n\n\n\n性质\n说明\n违反例子\n\n\n\n\n比例性Proportionality\n1. 每个决策变量对目标函数和约束条件的贡献与其数值大小成正比2. 这意味着决策变量的值翻倍，其对目标和约束的影响也会翻倍\n1. x\\sqrt{x}x​2. if  x&gt;1\\text{if }\\ x &gt; 1if  x&gt;1\n\n\n相加性Additivity\n1. 决策变量对目标函数和约束的总影响是它们各自贡献的总和2. 这意味着不存在交叉项，并且每个决策变量都独立地做出贡献\n1. x1x2x_1x_2x1​x2​2. max⁡(x1,x2)\\max(x_1, x_2)max(x1​,x2​)\n\n\n可分性Divisibility\n1. 决策变量可以取任何 非负值，包括整数和非整数值，只要它们满足约束条件即可2. 这意味着决策变量不限于整数，它们也可以是小数\n1. 只能取整数例如数量2. 取负数\n\n\n确定的Certainty\n1. 所有参数值都被假定为已知且保持不变2. 这意味着目标函数中的系数和约束条件的值在优化过程中不会改变\n\n\n\n\n 3. 线性规划模型术语\n为了求解一个线性规划模型，定义术语：\n\n\n\n术语\n说明\n\n\n\n\n搜索空间Search Space\n1. 所有决策变量组合的超空间2. 包括可行域和不可行域\n\n\n可行域Feasible Region\n所有约束条件均得到满足的区域\n\n\n不可行域Infeasible Region\n至少违反了一个约束条件的区域\n\n\n可行解Feasible Solutions\n可行域内部或边界上的任何点\n\n\n不可解Infeasible Solutions\n可行域之外的点\n\n\n角点可行域Corner-point Feasible (CPF)\n可行域是一个凸多面体，其顶点是候选解\n\n\n最优解Optimal Solution\nCPF 解中最小/最大目标函数的解是同时存在最优/最差解\n\n\n有效约束Binding Constraints\n约束在最优解处以等式形式满足，并且直接限制可行域\n\n\n非有效约束Non-binding Constraints\n在最优解处不是紧的(tight)，不具有约束力，存在松弛(slack) 或 剩余(surplus)\n\n\n松弛量Slack\n表示 ≤\\leq≤ 约束还有多少空间没有被用掉Slack=0\\text{Slack} = 0Slack=0 表示约束刚好用满，是有效的约束Slack&gt;0\\text{Slack} &gt; 0Slack&gt;0 表示约束没有被紧绷，是非有效约束Slack=C−a1x1−a2x2−a3x3−…\\text{Slack} = C - a_1x_1 - a_2x_2 - a_3x_3 - \\dotsSlack=C−a1​x1​−a2​x2​−a3​x3​−…\n\n\n剩余量Surplus\n表示 ≥\\geq≥ 约束超过了多少空间Surplus=0\\text{Surplus} = 0Surplus=0 表示约束没有超过，是有效的约束Surplus&gt;0\\text{Surplus} &gt; 0Surplus&gt;0 表示约束超过了空间，是非有效的约束Surplus=a1x1+a2x2+a3x3+⋯−C\\text{Surplus} = a_1x_1 + a_2x_2 + a_3x_3 + \\dots - CSurplus=a1​x1​+a2​x2​+a3​x3​+⋯−C\n\n\n尺寸Size\n有三种衡量标准：1. 决策变量的个数2. 约束的个数3. 搜索空间的大小 (找到最优解的搜索空间，也就是可行域的所有点个数)\n\n\n\n存在定理：\n\n\n\n\n\n\n\n\n\n如果线性规划有最优解（且可行域非空），那么一定存在一个顶点是最优解。\n不一定只有顶点是最优解，但一定有一个顶点是最优解。\n\n二维线性规划中展示\n考虑下面的线性规划模型：\n\n\n\n元素\n式子\n\n\n\n\n决策变量Decision Variables\nx,yx, yx,y\n\n\n目标函数Objective Function\nmin⁡ Z=2x+3y\\min\\ Z = 2x + 3ymin Z=2x+3y\n\n\n约束Constraints\n{200x+100y≤100010x+30y≥120x,y≥0\\begin{cases}200x+100y &amp;\\leq 1000\\\\10x+30y &amp;\\geq 120\\\\x, y &amp;\\geq 0\\end{cases}⎩⎪⎪⎨⎪⎪⎧​200x+100y10x+30yx,y​≤1000≥120≥0​\n\n\n\n\n\n\n有：\n\n\n\n术语\n表示\n\n\n\n\n搜索空间Search Space\n整个 (x,y)(x, y)(x,y) 坐标系\n\n\n可行域Feasible Region\n白色区域\n\n\n不可行域Infeasible Region\n灰色区域\n\n\n可行解Feasible Solutions\n白色区域的任一点，包括顶点楞点\n\n\n不可解Infeasible Solutions\n可行解以外的其他点\n\n\n角点可行域Corner-point Feasible (CPF)\n点 (3.6,2.8),(0,10),(0,4)(3.6, 2.8), (0, 10), (0, 4)(3.6,2.8),(0,10),(0,4)\n\n\n最优解Optimal Solution\n点 (0,4)(0, 4)(0,4)\n\n\n有效约束Binding Constraints\n1. 10x+30y≥12010x + 30y \\ge 12010x+30y≥1202. x≥0x \\geq 0x≥0\n\n\n非有效约束Non-binding Constraints\n1. 200x+100y≤1000200x + 100y \\leq 1000200x+100y≤10002. y≥0y \\geq 0y≥0\n\n\n松弛量Slack\n200x+100y≤1000:600200x + 100y \\leq 1000: 600200x+100y≤1000:600\n\n\n剩余量Surplus\ny≥0:4y \\geq 0: 4y≥0:4\n\n\n尺寸Size\n1. 决策变量：2222. 约束：2+22 + 22+2 (功能性 + 非负)3. 搜索空间：可行解的所有的点\n\n\n\n\n\n 4. 线性规划特殊情况\n\n\n\n情况\n说明\n例子\n\n\n\n\n无可行解No Feasible Solution\n1. 可行域为空2. 约束条件相互矛盾\n{x+y≤5x+y≥10\\begin{cases}x + y \\leq 5\\\\x + y \\geq 10\\end{cases}{x+y≤5x+y≥10​\n\n\n无界解Unbounded Solution\n1. 可行域无限延伸2. 目标函数可以无限增大或减小3. 此时最优解可能不存在\n{x−y≥2x,y≥0\\begin{cases}x - y \\geq 2\\\\x, y \\geq 0\\end{cases}{x−y≥2x,y≥0​\n\n\n多重最优解Multiple Optimal Solutions\n1. 存在多个不同的可行解具有相同的最优目标函数值2. 目标函数与可行域的某个有效约束平行3. 一定存在非角点最优解4. 最优解一定处于边界上\n{x+y≤4x,y≥0\\begin{cases}x + y \\leq 4\\\\x, y \\geq 0\\end{cases}{x+y≤4x,y≥0​目标函数：max⁡Z=2x+2y\\max Z = 2x + 2ymaxZ=2x+2y\n\n\n\n 5. 整数规划模型\n\n\n\n\n整数规划IP\n线性规划LP\n混合MIP\n\n\n\n\n可行域\n有限且离散的\n无限且连续的\n整合上述\n\n\n最优解\n可能位于可行域内部\n位于可行域的顶点\n可能位于可行域内部\n\n\n求解难度\n更难需要检查大量的整数组合\n容易只需要检查顶点\n困难\n\n\n求解依赖\n整数变量的数量和类型\n约束数量\n整数变量的数量连续变量影响较小\n\n\n\n对于一个包含 nnn 个二进制整数规划(BIP)问题，其有 2n2^n2n 个可能性。\n 6. LP 松弛问题\nLP 松弛 (LP relaxation) 是 IP, BIP, MIP 中允许整数决策变量取小数的松弛版本问题。\nLP松弛问题通常更容易求解，并且可以为原整数规划问题的最优值提供有用的界限。\n\n\n\n性质\n说明\n\n\n\n\n上下边界\nLP 松弛问题的最优解不会比原问题的最优解更差1. 最大优化问题中，LP 松弛问题给出了上边界2. 最小优化问题中，LP 松弛问题给出了下边界\n\n\n直接求解\n有时候 LP 松弛问题的最优解就是整数值，意味着这就是原问题的最优解\n\n\n不可行解Infeasible Solutions\nLP 的最优解的四舍五入可能对于原问题是不可行的\n\n\n次优解Suboptimal Solutions\n即使四舍五入后的解可行，也不能保证它是最优整数解取整后的解在目标值方面可能比真正的最优解差很多\n\n\n\n存在下面术语：\n\n\n\n术语\n说明\n\n\n\n\n紧边界Tight Bound\n∣ZLP−ZIP∣\\mid Z_{LP} - Z_{IP} \\mid∣ZLP​−ZIP​∣其越小表示 LP 松弛问题可以很好地近似整数规划问题\n\n\n最优性差距Optimality Gap\n在 LP 松弛问题出，会同时给出目标函数的最优解和最差解这两个解目标函数值的差就是最优性差距可以为原整数规划问题的最优值提供有用的界限\n\n\n\n在求解的过程中，求解器可能会在差距低于用户指定的容差内 (例如 0.1%0.1\\%0.1%) 停止。\n求解算法：\n\n\n\n算法\n说明\n\n\n\n\n启发式算法Heuristic Algorithms\n为大规模问题提供高效的近似解，但不能保证最优解\n\n\n分支定界法Branch-and-bound Technique\n系统地（但隐式地）枚举可行的整数解，同时剪除搜索空间的大部分\n\n\n\n 五. 产品组合优化\n产品组合优化问题(Product-Mix Optimization Problems)是线性规划或整数规划问题，其目标是在资源和运营约束条件下，确定产品或活动的最佳组合。\n其组成成分：\n\n\n\n组成成分\n说明\n\n\n\n\n决策变量\n变量作为其他变量的分数或比率\n\n\n约束\n1. 约束通常是一个变量表示为其他变量的分数、比率或线性函数2. 这种关系反映了生产或资源分配决策之间的比例关系或依赖关系\n\n\n目标函数\n通常为线性利润最大化或成本最小化\n\n\n\n 六. 多目标优化 (MOO)\n在许多情况下，需要同时优化多个目标，其中一些目标可能相互冲突或不可比较，因此应该使用多目标优化方法(Multi-objective Optimization, MOO)。\n目标冲突的存在使得优化模型更难处理，因此通常将多目标问题建模为单目标问题。有很多方法可以解决多目标优化问题，包括线性方法和非线性方法。\n多目标问题中的目标可能：\n\n\n\n可能\n说明\n\n\n\n\n相互冲突\n改进一个目标会导致另一个目标恶化\n\n\n不可通约\n1. 衡量标准不同2. 同一个解决方案可能由不同的决策者根据不同的标准进行评估\n\n\n\n因此，在多目标问题中，最优解的概念变得不太清晰，因为每个解都代表了目标之间的权衡或妥协。\n 1. kkk 目标向量表示\n其目标函数通常表达为：\nF(x)=(f1(x),f2(x),…,fk(x))x∈X\\begin{aligned}\nF(x) &amp;= (f_1(x), f_2(x), \\dots, f_k(x)) \\\\\nx &amp;\\in X\n\\end{aligned}\nF(x)x​=(f1​(x),f2​(x),…,fk​(x))∈X​\n\n\n\n元素\n说明\n\n\n\n\nF(x)F(x)F(x)\n是 kkk 维目标向量\n\n\nfi(x)f_i(x)fi​(x)\n是第 iii 个目标向量的值\n\n\nXXX\n是所有可行解\n\n\nxxx\n是决策变量或者决策变量集合\n\n\nx∗x^*x∗\n表示一个有效解\n\n\n\n 2. kkk 目标优化目标术语\n\n\n\n术语\n说明\n类型\n\n\n\n\n支配Dominate\n如果解 S2S_2S2​ 支配解 S1S_1S1​，当且仅当：fi(S2)≥fi(S1)∀i∈[1,k](1)fj(S2)&gt;fj(S1)∃j∈[1,k](2)\\begin{aligned}f_i(S_2) &amp;\\geq f_i(S_1) \\quad &amp;\\forall i \\in [1, k] \\quad &amp; (1)\\\\f_j(S_2) &amp;&gt; f_j(S_1) \\quad &amp;\\exist j \\in [1, k] \\quad &amp; (2)\\end{aligned}fi​(S2​)fj​(S2​)​≥fi​(S1​)&gt;fj​(S1​)​∀i∈[1,k]∃j∈[1,k]​(1)(2)​1. 所有 kkk 个目标上至少与 S1S_1S1​ 的一样好2. 至少一个目标上严格优于 S1S_1S1​\n关系\n\n\n有效解Efficient Solution\n不存在支配解的解是有效解 x∗x^*x∗也被称之为：1. 帕累托最优解(Pareto Optimal Solution)2. 非支配最优解(Non-dominated Optimal Solution)\n自变量的具体值\n\n\n有效集Efficient Set\n1. 所有有效解的集合2. 也称为帕累托最优集或非支配最优集\n自变量的集合/定义域\n\n\n有效点Efficient Point\n1. 有效解 SSS 在 kkk 维目标空间中的投影 (projection)2. 也就是该解所表示的 kkk 维目标值向量\n因变量的具体值\n\n\n有效前沿Efficient Frontier\n1. 所有有效解对应的有效点的集合2. 也称为帕累托最优前沿或非支配最优前沿\n因变量集合/值域\n\n\n\n\n术语例子\n考虑下面有 最小化成本 和 最高性能 双优化目标的例子：\n\n\n\n\n\n\n术语\n说明\n\n\n\n\n有效解Efficient Solution\n1. 达成三个点的决策变量值向量2. 就是汽车设计方案，比如说发动机引擎参数等3. 还有更多有效解\n\n\n有效集Efficient Set\n所有有效解的集合\n\n\n有效点Efficient Point\n1. 曲面上每一个点都是有效点，即在有效解在目标空间中的投影2. 三个点分别对应（成本最低，速度最慢）、（成本适中，速度适中）、（成本最高，速度最快）\n\n\n有效前沿Efficient Frontier\n曲面上所有有效点的集合构成有效边界\n\n\n\n\n\n 3. 多目标优化求解策略\n\n\n\n类型\n说明\n时间\n优点\n缺点\n\n\n\n\n先验型Priori\n1. 优化前先设定目标值之前的优先级或权重2. 根据权重在有效集中求一个折中解\n优化前\n快，简单，实现成本低\n可能错过重要的解，不利于探索\n\n\n后验型Posteriori\n1. 先优化生成有效集2. 让决策者在有效集中选择一个折中解\n优化后\n提供多样解，更灵活\n计算量较大，决策负担提高\n\n\n交互型Interactive\n求解过程与决策者偏好调整同时进行：1. 算法生成候选解2. 决策者反馈3. 算法继续搜索\n贯穿始终\n效率高，适合复杂偏好或动态学习\n实现复杂，需要决策者持续参与\n\n\n\n先验型(Priori)有下面三种方法：\n\n\n\n方法\n说明\n类型\n生成多解\n偏好\n保证Pareto\n\n\n\n\n字典序法Lexicographic Ordering\n让多目标有绝对优先级顺序：1. 按重要程度排列目标 O1&gt;O2&gt;…O_1 &gt; O_2 &gt; \\dotsO1​&gt;O2​&gt;…2. 对第一个目标求最优解并记录最佳值 R1R_1R1​3. 加入约束 O1=R1O_1 = R_1O1​=R1​ 4. 以此类推，处理完所有的目标\n先验偏好硬优先\n✖️\n✔️强烈\n✔️\n\n\n加权求和法Weighted Aggregation\n将多个目标函数线性加权合并成一个：F(x)=w1f1(x)+w2f2(x)+…F(x) = w_1f_1(x) + w_2f_2(x) + \\dotsF(x)=w1​f1​(x)+w2​f2​(x)+…\n先验加权\n✔️\n✔️数值偏好\n✔️仅凸前沿\n\n\n目标规划Goal Programming\n设定理想目标值，然后最小化偏离值：1. 为每个目标函数设立目标值 GiG_iGi​2. 引入偏差变量 did_idi​3. 将偏差变量和目标函数写入到约束 di=∣Gi−Oi∣d_i = |G_i - O_i|di​=∣Gi​−Oi​∣ 4. 是大于小于，还是加减 did_idi​ 取决于最大/最小目标5. 目标最小化偏差：min⁡∑di\\min \\sum d_imin∑di​\n先验目标值\n✖️\n✔️目标驱动\n✖️\n\n\n\n","slug":"笔记/线性规划/线性规划导航","date":"2025-12-29T09:14:00.000Z","categories_index":"导航","tags_index":"Linear Programming,Operations Research,Optimization,Product-Mix Optimization Problems","author_index":"zExNocs"},{"id":"1aefbd0eeb8202cac728bce990008f73","title":"深度学习-CNN","content":"&lt;返回深度学习导航\n\n 一. 介绍\n参数爆炸(Parameter explosion)指的是在训练过程中，随着搜索深度的增加，需要优化的权重过多，导致搜索空间变得更加难以驾驭。\n卷积神经网络 (Convolutional Neural Network, CNN) 使用共享权重和局部连接来减少参数数量，从而缓解参数爆炸问题。\n其结构如下：\n\n\n\n结构\n说明\n\n\n\n\n输入\n输入数据，例如图像的像素值\n\n\n卷积层CONV\n计算与输入中局部区域相连的神经元的输出，每个神经元计算其权重与输入体中与其相连的小区域的点积\n\n\n激活函数\n引入非线性，使网络能够学习复杂的模式，一般用 ReLU 函数\n\n\n池化层POOL\n通过下采样减少空间尺寸，降低计算复杂度和防止过拟合\n\n\n全连接层FC\n将前一层的所有神经元与当前层的每个神经元连接，用于整合特征并进行最终分类或回归\n\n\n\n\n 二. 卷积层\n卷积层 (Convolution Layer) 是卷积神经网络的核心组件，负责提取输入数据中的局部特征。它通过卷积操作将输入数据与一组可学习的滤波器(Filter)/卷积核(Kernel)进行卷积，生成特征图（Feature Map）。\n\n\n\n 1. 卷积操作\n一个卷积核 (Kernel) 是一个小的权重矩阵，通常尺寸为 (Cin,Kh,Kw)(C_{in}, K_h, K_w)(Cin​,Kh​,Kw​)，其中 KhK_hKh​ 和 KwK_wKw​ 分别是卷积核的高度和宽度，CinC_{in}Cin​ 是输入数据的通道数。该卷积核在不同输入通道上的值会进行加权叠加得到最终的输出，最终输出一个 (Hout,Wout)(H_{out}, W_{out})(Hout​,Wout​) 的二维特征图 (Feature Map)。\n简单说就是卷积核在输入数据上滑动（卷积），在每个位置计算卷积核与输入数据局部区域的点积求和再加上偏置项，得到的加权和为特征图的对应位置的值。\n一个卷积核学一种特征，多个卷积核学多种特征。同一层的每一个卷积核对应一个输出通道，CoutC_{out}Cout​ 个卷积核叠加起来最终得到一个尺寸为 (Cout,Cin,Kh,Kw)(C_{out}, C_{in}, K_h, K_w)(Cout​,Cin​,Kh​,Kw​) 的张量卷积核。\n如图表示 NNN (batch) 个尺寸为 (Cin,H,W)(C_{in}, H, W)(Cin​,H,W) 的图片，通过 CoutC_{out}Cout​ 个 (Cin,Kh,Kw)(C_{in}, K_h, K_w)(Cin​,Kh​,Kw​) 卷积核 和 CoutC_{out}Cout​ 维度的偏置向量进行卷积操作，得到 CoutC_{out}Cout​ 个尺寸为 (Cout,H′,W′)(C_{out}, H&#x27;, W&#x27;)(Cout​,H′,W′) 的特征图。\n\n\n\n 2. 卷积操作公式\n假设从 (0,0)(0, 0)(0,0) 作为初始坐标开始，如果：\n\n输入数据的尺寸为 X∈RCin×Hin×Win\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H_{in} \\times W_{in}}X∈RCin​×Hin​×Win​\n该层卷积核的尺寸为 K∈RCout×Cin×Kh×Kw\\mathbf{K} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}K∈RCout​×Cin​×Kh​×Kw​\n偏置项为 b∈RCout\\mathbf{b} \\in \\mathbb{R}^{C_{out}}b∈RCout​\n输出特征图尺寸为 Y∈RCout×Hout×Wout\\mathbf{Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}Y∈RCout​×Hout​×Wout​\n\n那么输出坐标 (co,h,w)(c_o, h, w)(co​,h,w) 下卷积操作的数学表达式如下：\nY(co,h,w)=b(co)+∑ci=0Cin−1∑m=0Kh−1∑n=0Kw−1K(co,ci,m,n)⋅X(ci,S⋅h−P+m,S⋅w−P+n)Hout=⌊Hin−Kh+2PS⌋+1Wout=⌊Win−Kw+2PS⌋+1\\begin{aligned}\n\\mathbf{Y}(c_o, h, w) &amp;= \\mathbf{b}(c_o) + \\sum_{c_i=0}^{C_{in}-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\mathbf{K}(c_o, c_i, m, n) \\cdot \\mathbf{X}(c_i, S \\cdot h - P + m, S \\cdot w - P + n) \\\\\nH_{out} &amp; = \\lfloor \\frac{H_{in} - K_h + 2P}{S} \\rfloor + 1 \\\\\nW_{out} &amp; = \\lfloor \\frac{W_{in} - K_w + 2P}{S} \\rfloor + 1 \\\\\n\\end{aligned}\nY(co​,h,w)Hout​Wout​​=b(co​)+ci​=0∑Cin​−1​m=0∑Kh​−1​n=0∑Kw​−1​K(co​,ci​,m,n)⋅X(ci​,S⋅h−P+m,S⋅w−P+n)=⌊SHin​−Kh​+2P​⌋+1=⌊SWin​−Kw​+2P​⌋+1​\n符号解释：\n\n\n\n符号\n说明\n\n\n\n\nY(co,h,w)\\mathbf{Y}(c_o, h, w)Y(co​,h,w)\n输出特征图在第 coc_oco​ 个输出通道的 (h,w)(h, w)(h,w) 位置的值\n\n\nX(ci,m,n)\\mathbf{X}(c_i, m, n)X(ci​,m,n)\n输入数据在第 cic_ici​ 个输入通道的 (m,n)(m, n)(m,n) 位置的值\n\n\nK(co,ci,m,n)\\mathbf{K}(c_o, c_i, m, n)K(co​,ci​,m,n)\n卷积核在第 coc_oco​ 个输出通道，第 cic_ici​ 个输入通道的 (m,n)(m, n)(m,n) 位置的权重值\n\n\nb(co)\\mathbf{b}(c_o)b(co​)\n第 coc_oco​ 输出通道的偏置项一个卷积核只有一个偏置\n\n\nCinC_{in}Cin​\n输入数据的通道数对于彩色图片，通常为 333 (RGB)\n\n\nCoutC_{out}Cout​\n输出特征图的通道数由该层的卷积核的数量决定与 CinC_{in}Cin​ 无关\n\n\nHinH_{in}Hin​\n输入数据的高度\n\n\nWinW_{in}Win​\n输入数据的宽度\n\n\nKhK_hKh​\n卷积核的高度\n\n\nKwK_wKw​\n卷积核的宽度\n\n\nPPP\n填充（Padding）的大小一般来说 P=(Kh−12,Kw−12)P = (\\frac{K_h - 1}{2}, \\frac{K_w - 1}{2})P=(2Kh​−1​,2Kw​−1​)\n\n\nSSS\n步幅（Stride）的大小\n\n\n\n那么该卷积层的权重参数数量计算公式如下：\nNw=Cout×(Cin×Kh×Kw+1)N_w = C_{out} \\times (C_{in} \\times K_h \\times K_w + 1)\nNw​=Cout​×(Cin​×Kh​×Kw​+1)\n其中 +1+1+1 是偏置项。\n\n 3. 卷积层稀疏矩阵\n卷积操作可以表示为一个巨大但稀疏的矩阵 ×\\times× 输入向量。\n假设输入数据 X∈RCin×Hin×Win\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H_{in} \\times W_{in}}X∈RCin​×Hin​×Win​，那么将其平展成一个向量 x∈RCin⋅Hin⋅Win\\mathbf{x} \\in \\mathbb{R}^{C_{in} \\cdot H_{in} \\cdot W_{in}}x∈RCin​⋅Hin​⋅Win​（约定顺序：通道 → 高度 → 宽度）。\n卷积核形状 K∈RCout×Cin×Kh×Kw\\mathbf{K} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}K∈RCout​×Cin​×Kh​×Kw​，输出向量长度 y∈RCout⋅Hout⋅Wout\\mathbf{y} \\in \\mathbb{R}^{C_{out} \\cdot H_{out} \\cdot W_{out}}y∈RCout​⋅Hout​⋅Wout​。\n最终稀疏矩阵结构 W∈R(Cout⋅Hout⋅Wout)×(Cin⋅Hin⋅Win)\\mathbf{W} \\in \\mathbb{R}^{(C_{out} \\cdot H_{out} \\cdot W_{out}) \\times (C_{in} \\cdot H_{in} \\cdot W_{in})}W∈R(Cout​⋅Hout​⋅Wout​)×(Cin​⋅Hin​⋅Win​)，其中每一行对应输出向量的一个元素，每一列对应输入向量的一个元素。\n对于稀疏矩阵在 (i,j)(i, j)(i,j) 位置的元素公式：\nW(i,j)={K(c,p,m,n)if  j=p⋅Hin⋅Win+(S⋅h−P+m)⋅Win+(S⋅w−P+n)0otherwisey=W⋅x+b\\begin{aligned}\n\\mathbf{W}(i, j) &amp;= \\begin{cases} \\mathbf{K}(c, p, m, n) &amp; \\text{if }\\ j = p \\cdot H_{in} \\cdot W_{in} + (S \\cdot h - P + m) \\cdot W_{in} + (S \\cdot w - P + n) \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\\\\n\\mathbf{y} &amp;= \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b} \\\\\n\\end{aligned}\nW(i,j)y​={K(c,p,m,n)0​if  j=p⋅Hin​⋅Win​+(S⋅h−P+m)⋅Win​+(S⋅w−P+n)otherwise​=W⋅x+b​\n其中每一行有 Cin⋅Kh⋅KwC_{in} \\cdot K_h \\cdot K_wCin​⋅Kh​⋅Kw​ 个非零元素，对应卷积核的权重，其余元素为 000。\n\n 4. 卷积层流程\n卷积操作后通常会应用激活函数（如 ReLU）以引入非线性。\n通常流程：\n\n\n\n步骤\n说明\n\n\n\n\n111\n卷积Conv\n\n\n222\n加偏置Bias\n\n\n333\n激活函数Activation Function\n\n\n\n假设卷积操作的符号为 ⋆\\star⋆，卷积核的参数项为 K\\mathbf{K}K，那么卷积层经过激活函数的输出为 A\\mathbf{A}A，那么可以表示为：\nZ=K⋆X+BA=σ(Z)\\begin{aligned}\n\\mathbf{Z} &amp;= \\mathbf{K} \\star \\mathbf{X} + \\mathbf{B} \\\\\n\\mathbf{A} &amp;= \\sigma(\\mathbf{Z}) \\\\\n\\end{aligned}\nZA​=K⋆X+B=σ(Z)​\n\n 5. 感受野\n感受野 (Receptive Field) 指的是 CNN 中某一层的输出特征图上 一个像素点 映射回输入图像上的区域大小。换句话说，感受野表示卷积神经网络中某一个神经元所能“看到”的输入数据的区域大小。它决定了该神经元能够捕捉到的输入特征的范围。\n每增加一层 (Kh,Kw)(K_h, K_w)(Kh​,Kw​) 的卷积核，感受野会增加 (Kh−1,Kw−1)(K_h - 1, K_w - 1)(Kh​−1,Kw​−1)，其中初始感受野为 (1,1)(1, 1)(1,1)。\nReceptive Field=1+∑l=1L(Kl−1)\\text{Receptive Field} = 1 + \\sum_{l=1}^{L} (K_l - 1)\nReceptive Field=1+l=1∑L​(Kl​−1)\n如果 KlK_lKl​ 都相同，则为：\nReceptive Field=1+L⋅(K−1)\\text{Receptive Field} = 1 + L \\cdot (K - 1)\nReceptive Field=1+L⋅(K−1)\n例如，使用 3×33 \\times 33×3 的卷积核堆叠 555 层，感受野为：\n\n\n\n层数\n感受野\n\n\n\n\n000\n111\n\n\n111\n333\n\n\n222\n555\n\n\n333\n777\n\n\n444\n999\n\n\n555\n111111\n\n\n\n此外要注意，对于上一层的感受野和对于输入的感受野是不同的概念。我们通常只关心对于输入的感受野。\n i. 多个小卷积核 vs 大卷积核\n使用多个小卷积核堆叠多层，可以获得更大的感受野，同时减少参数数量和计算复杂度。\n例如使用 3×33 \\times 33×3 的卷积核堆叠 333 层，感受野为 7×77 \\times 77×7，参数数量为 27C227C^227C2，而使用一个 7×77 \\times 77×7 的卷积核，参数数量为 49C249C^249C2。\n此外，多个小卷积核堆叠多层可以引入更多的非线性，使模型更具表达能力。\n也就是说，深度的增加比宽度的增加更有效。\n ii. 通道数为什么越来越大\n一般来说，随着网络的加深卷积层的分辨率会逐渐减小，而通道数会逐渐增大，经典的 stage 为：\n\n\n\nStage\n输出尺寸 (H,W)(H, W)(H,W)\n通道数 CCC\n\n\n\n\n111\n(224,224)(224, 224)(224,224)\n646464\n\n\n222\n(112,112)(112, 112)(112,112)\n128128128\n\n\n333\n(56,56)(56, 56)(56,56)\n256256256\n\n\n444\n(28,28)(28, 28)(28,28)\n512512512\n\n\n555\n(14,14)(14, 14)(14,14)\n102410241024\n\n\n\n这样设计的原因有以下几点：\n\n\n\n原因\n说明\n\n\n\n\n信息压缩\n1. 在高分辨率时，本身通道不需要太多也能表达原图像特征2. 尺寸减少感受野增加，特征越来越抽象，通道数增加有助于捕捉更多抽象特征\n\n\n计算复杂度\n1. 卷积操作的计算复杂度与输入尺寸和通道数成正比2. 通过减少空间尺寸，可以在增加通道数的同时控制计算复杂度3. 这有助于平衡每一层的计算量和参数量，使得每一阶段计算量维持在同一量级\n\n\n\n\n 6. 起始卷积层\n起始卷积层(Beginning Convolution Layer) 也叫 Stem，是卷积神经网络的第一层卷积层，负责从输入图像中提取初步的特征表示与下采样，是整个网络的入口。\n起始卷积层的设计对于后续网络的性能和效率有重要影响。常见的设计包括：\n\n\n\n设计\n说明\n好处\n\n\n\n\n大卷积核\n使用较大的卷积核 (如 7×77 \\times 77×7) 来捕捉更多的局部特征\n1. 有利于捕捉边缘、轮廓、大尺度结构2. 输入只是 RGB 像素，不需要太深和复杂，一个较大的卷积核可以完成初步的抽象\n\n\n步幅卷积\n使用较大的步幅 (如 222) 来实现下采样，减少特征图尺寸\n快速降低分辨率，将计算预留给后面的深层网络\n\n\n池化层\n在卷积层后添加池化层 (如最大池化) 进一步减少空间尺寸\n1. 降低计算复杂度2. 增加感受野3. 提取更具鲁棒性的特征\n\n\n\n这些设计有助于在网络的早期阶段快速提取有用的特征，同时控制计算复杂度和内存使用。\n例如 ImageNet 上常用的起始卷积层配置为：\n12347×7 Conv, 64 channels, stride=2→ BatchNorm→ ReLU→ 3×3 MaxPool, stride=2\n\n 三. 下采样\n对于大图像，要让一个输出看到整张图需要非常多层卷积。而每一层卷积只会线性增加感受野，这样会导致网络过深，训练困难。\n下采样 (Downsampling) 通过减少特征图的空间尺寸来增加感受野。通常是只降低高度和宽度，而不改变通道数，从而增加感受野降低计算复杂度。\n常见的下采样方法包括池化 (Pooling) 和 步幅卷积 (Strided Convolution)。\n 1. 步幅卷积\n步幅卷积通过增加卷积操作的步幅 (Stride) 来减少输出特征图的空间尺寸，从而增加感受野。\n使得最终的输出尺寸变为：\nHout=Hin−Kh+2PS+1Wout=Win−Kw+2PS+1\\begin{aligned}\nH_{out} = \\frac{H_{in} - K_h + 2P}{S} + 1\\\\\nW_{out} = \\frac{W_{in} - K_w + 2P}{S} + 1\n\\end{aligned}\nHout​=SHin​−Kh​+2P​+1Wout​=SWin​−Kw​+2P​+1​\n\n 2. 池化层\n池化层 (Pooling Layer) 是在不改变通道数的前提下，压缩空间尺寸，从而减少计算量和防止过拟合的操作。\n使用池化层并创建下采样或池化特征图的结果是对输入中检测到的特征进行概括。\n池化层：\n\n没有 padding\n没有权重参数\n没有通道变化\n\n最终：\n\n池化层输出的高度和宽度减少\n计算量减少\n感受野增加\n防止过拟合，增加了泛化能力 (平移不变性)\n\n i. 局部平移不变性\n卷积层检测到的输入中特征位置的微小变化会导致池化特征图中的特征位置保持不变。池化带来的这种能力被称为模型的局部平移不变性(local translation invariance)。\n在所有情况下，池化都有助于使表示近似地对输入的小幅平移保持不变。平移不变性意味着，如果我们对输入进行小幅平移，大多数池化输出的值不会改变。即不关心特征的精确位置，而更关注其存在与否。\n ii. 池化类型\n常见的池化方法有最大池化 (Max Pooling) 和平均池化 (Average Pooling)。\n a. 最大池化\n最大池化 (Max Pooling) 通过在输入特征图上滑动一个窗口，取窗口内的最大值作为输出特征图对应位置的值。\n假设池化窗口的尺寸为 (Hp,Wp)(H_p, W_p)(Hp​,Wp​)，步幅为 SSS，那么最大池化的数学表达式如下：\nY(i,j,c)=max⁡0≤m&lt;Hp,0≤n&lt;WpX(S⋅i+m,S⋅j+n,c)Y(i, j, c) = \\max_{0 \\leq m &lt; H_p, 0 \\leq n &lt; W_p} X(S \\cdot i + m, S \\cdot j + n, c)\nY(i,j,c)=0≤m&lt;Hp​,0≤n&lt;Wp​max​X(S⋅i+m,S⋅j+n,c)\n最终尺寸：\nHout=Hin−HpS+1Wout=Win−WpS+1\\begin{aligned}\nH_{out} = \\frac{H_{in} - H_p}{S} + 1\\\\\nW_{out} = \\frac{W_{in} - W_p}{S} + 1\n\\end{aligned}\nHout​=SHin​−Hp​​+1Wout​=SWin​−Wp​​+1​\n b. 平均池化\n类似于最大池化，平均池化 (Average Pooling) 通过在输入特征图上滑动一个窗口，取窗口内的平均值作为输出特征图对应位置的值。\n假设池化窗口的尺寸为 (Hp,Wp)(H_p, W_p)(Hp​,Wp​)，步幅为 SSS，那么平均池化的数学表达式如下：\nY(i,j,c)=1Hp×Wp∑m=0Hp−1∑n=0Wp−1X(S⋅i+m,S⋅j+n,c)Y(i, j, c) = \\frac{1}{H_p \\times W_p} \\sum_{m=0}^{H_p - 1} \\sum_{n=0}^{W_p - 1} X(S \\cdot i + m, S \\cdot j + n, c)\nY(i,j,c)=Hp​×Wp​1​m=0∑Hp​−1​n=0∑Wp​−1​X(S⋅i+m,S⋅j+n,c)\n最终尺寸：\nHout=Hin−HpS+1Wout=Win−WpS+1\\begin{aligned}\nH_{out} = \\frac{H_{in} - H_p}{S} + 1\\\\\nW_{out} = \\frac{W_{in} - W_p}{S} + 1\n\\end{aligned}\nHout​=SHin​−Hp​​+1Wout​=SWin​−Wp​​+1​\n iii. 常见池化配置\n\n\n\n类型\n窗口尺寸\n步幅\n\n\n\n\n最大池化Max Pooling\n2×22 \\times 22×2\n222\n\n\n平均池化Average Pooling\n2×22 \\times 22×2\n222\n\n\n\n\n 3. 池化层 vs 步幅卷积\n池化层和步幅卷积都可以实现下采样，从而增加感受野，但它们的工作原理和效果有所不同。\n\n\n\n对比点\n池化层Pooling Layer\n步幅卷积Strided Convolution\n\n\n\n\n是否可学习\n✖️\n✔️\n\n\n参数数量\n000\n与普通卷积相同\n\n\n是否下采样\n✔️\n✔️\n\n\n是否混合通道\n✖️\n✔️\n\n\n计算复杂度\n较低\n较高\n\n\n现代架构中使用\n较少\n较多\n\n\n\n\n 四. 全连接层\n全连接层(Fully Connected Layer, FC)是将卷积核走向分类决策的关键过度。全连接层将前一层的所有神经元与当前层的每个神经元连接，用于整合特征并进行最终分类或回归。\n i. 平坦化\n因为全连接层的输入一定是一维向量，所以需要将卷积层或池化层的输出进行平坦化 (Flattening)。简单说就是将 (H,W,C)(H, W, C)(H,W,C) 的三维张量转换为 H×W×CH \\times W \\times CH×W×C 的一维向量。\nFlatten 不会提取特征，而只是改变形状。\n ii. 全连接操作\n假设平坦化后的输入向量为 x∈RD\\mathbf{x} \\in \\mathbb{R}^Dx∈RD，全连接层的权重矩阵为 W∈RC×D\\mathbf{W} \\in \\mathbb{R}^{C \\times D}W∈RC×D，偏置向量为 b∈RC\\mathbf{b} \\in \\mathbb{R}^Cb∈RC，那么全连接层的输出为：\nf=Wx+b\\mathbf{f} = \\mathbf{Wx} + \\mathbf{b}\nf=Wx+b\n其中 CCC 为输出类别数（平坦化层的神经元数量），DDD 为平坦化后的输入维度。\n现代 CNN 中通常不会添加 FC 层\n\n 五. 上采样\n语义分割 (Semantic Segmentation) 是计算机视觉中的一个任务，旨在将图像中的 每个像素 都分类到特定的类别中，从而实现对图像内容的精细理解。\n分类网络通常会减小特征图的空间尺寸以加深网络层数，但语义分割要求输出尺寸与输入尺寸相同。因此就有必要使用上采样 (Upsampling) 技术将特征图恢复到原始尺寸。\n 1. 反池化\n反池化 (Unpooling) 是池化操作的逆过程，使用无参数的方法将特征图的空间尺寸放大。常见的反池化方法有：\n\n\n\n类型\n说明\n\n\n\n\n最近邻插值Nearest Neighbor Interpolation\n倍数扩大图像后，将每个像素直接填充到附近扩大的位置\n\n\n床钉法Bed of Nails\n1. 将每个像素值放置在扩大的图像中的特定位置，其他位置填充为 0002. 相对位置一样\n\n\n最大值反池化Max Unpooling\n1. 使用池化时记录的最大值位置，将输出值放到该位置，其他位置填充为 0002. 相对位置不同\n\n\n\n一个池化层对应一个反池化层。\n 2. 转置卷积\n转置卷积 (Transposed Convolution) 也称为反卷积 (Deconvolution)，是一种可学习的上采样方法。它通过学习一组卷积核，将低分辨率的特征图转换为高分辨率的特征图。\n其转置卷积的过程简单说就是将输入特征图中每个像素值都乘以卷积核得到一个小的矩阵，然后将这些小矩阵根据步长进行叠加，最终得到输出特征图。其过程类似于卷积操作的反向传播。\n i. 转置卷积公式\n转置卷积是将卷积过程反转，那么就假设是从 Y\\mathbf{Y}Y 反推 X\\mathbf{X}X。\n\n\n\n符号\n说明\n\n\n\n\nX∈RCin×Hin×Win\\mathbf{X} \\in \\mathbb{R}^{C_{in} \\times H_{in} \\times W_{in}}X∈RCin​×Hin​×Win​\n1. 需要反推的输入特征图2. 转置的输出\n\n\nK∈RCout×Cin×Kh×Kw\\mathbf{K} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}K∈RCout​×Cin​×Kh​×Kw​\n转置卷积核\n\n\nY∈RCout×Hout×Wout\\mathbf{Y} \\in \\mathbb{R}^{C_{out} \\times H_{out} \\times W_{out}}Y∈RCout​×Hout​×Wout​\n1. 输出特征图2. 转置的输入\n\n\nSSS\n步幅\n\n\nPPP\n填充\n\n\n\n那么转置卷积的数学表达式如下：\nX(ci,h,w)=∑co=0Cout−1∑m=0Kh−1∑n=0Kw−1K(co,ci,m,n)⋅Y(co,h+P−mS,w+P−nS)Hin=S⋅(Hout−1)+Kh−2PWin=S⋅(Wout−1)+Kw−2P\\begin{aligned}\n\\mathbf{X}(c_i, h, w) &amp;= \\sum_{c_o=0}^{C_{out}-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\mathbf{K}(c_o, c_i, m, n) \\cdot \\mathbf{Y}(c_o, \\frac{h + P - m}{S}, \\frac{w + P - n}{S}) \\\\\nH_{in} &amp; = S \\cdot (H_{out} - 1) + K_h - 2P \\\\\nW_{in} &amp; = S \\cdot (W_{out} - 1) + K_w - 2P \\\\\n\\end{aligned}\nX(ci​,h,w)Hin​Win​​=co​=0∑Cout​−1​m=0∑Kh​−1​n=0∑Kw​−1​K(co​,ci​,m,n)⋅Y(co​,Sh+P−m​,Sw+P−n​)=S⋅(Hout​−1)+Kh​−2P=S⋅(Wout​−1)+Kw​−2P​\n ii. 转置卷积的稀疏矩阵表示\n假设该卷积核的卷积操作的稀疏矩阵为 M\\mathbf{M}M，那么转置卷积的稀疏矩阵为 MT\\mathbf{M}^TMT，即转置卷积的数学表达式为：\ny=MT⋅x+b\\mathbf{y} = \\mathbf{M}^T \\cdot \\mathbf{x} + \\mathbf{b}\ny=MT⋅x+b\n iii. 转置卷积的反卷积表示\n假设卷积操作为 ⋆\\star⋆。如果转置卷积核表示为 KKK，输入为 XXX，那么有：\n\nKKK 经过水平和垂直翻转后的卷积核表示为 K′K&#x27;K′\nXXX 经过在高度和宽度方向上插入 S−1S-1S−1 个零后的输入表示为 X′X&#x27;X′，SSS 为步幅\n\n其输出 YYY 可以表示为：\nY=K′⋆X′+B\\mathbf{Y} = K&#x27; \\star X&#x27; + \\mathbf{B}\nY=K′⋆X′+B\n 3. U-Net 架构\nU-Net 是一种常用的语义分割网络架构，采用编码器-解码器结构，通过下采样提取特征，再通过上采样恢复空间尺寸。其结构如下：\n\n编码器 (Encoder)：由多个卷积层和池化层组成，逐步减少特征图的空间尺寸，提取高层次特征。\n解码器 (Decoder)：由多个转置卷积层或反池化层组成，逐步恢复特征图的空间尺寸，生成与输入图像相同大小的输出。\nbottleneck：连接编码器和解码器的部分，通常包含卷积层以进一步提取特征。\n跳跃连接 (Skip Connections)：将编码器中的特征图直接连接到解码器的对应层，帮助恢复空间细节信息。\n\nU-Net 的关键特点是引入了跳跃连接 (Skip Connections)，将编码器中的特征图直接连接到解码器的对应层，帮助恢复空间细节信息。\n\n 六. 现代 CNN 架构\n 1. VGGNet\nVGGNet(Visual Geometry Group Network) 通过使用多个小卷积核 (3x3) 堆叠来增加网络深度，从而提高模型的表达能力。\n其统一使用 3×33 \\times 33×3 的卷积核堆叠多层来代替大卷积核 (如 5×55 \\times 55×5 或 7×77 \\times 77×7)，从而减少参数数量和计算复杂度。\nVGGNet 证明了增加网络深度可以显著提升模型性能，但是深度增加后也带来了训练困难的问题，如梯度消失和梯度爆炸。\n\n 2. ResNet\nResNet (Residual Network) 通过引入残差连接 (Residual Connections) 来缓解深层网络的训练困难问题。残差连接允许梯度直接流过网络，避免梯度消失问题。\n i. 基础术语\n\n\n\n术语\n说明\n\n\n\n\n残差块Residual Block\n包含残差连接的神经网络模块F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x1. xxx: 上一层/残差快的输出2. F(x)F(x)F(x): 残差映射3. H(x)H(x)H(x): 该层神经网络的输出残差映射通常中间包含了两个及以上个卷积层\n\n\n跳跃连接Skip Connection\n将输入直接添加到输出的连接默认是恒等映射，也就是直接添加 xxx\n\n\n\n ii. 引入\n从直觉上：更深的网络 → 表达能力更强 → 训练误差更低\n但实验结果却不是这样。实验证明 56 层的网络比 20 层的网络误差更高。但是一般来说，更深的网络至少不应该比浅层网络差，因为浅层网络的解可以通过深层网络来表示，比如说复制浅层网络的权重到深层网络的前几层，并将额外的层作为恒等映射。这种更差的现象被称为退化问题(Degradation Problem)。\n出现这种情况主要原因是因为深层网络难以训练，梯度消失和梯度爆炸使得网络难以收敛。\n解决这种退化问题有以下几种思路：\n\n引入跳跃连接(Skip Connections)，允许梯度直接流过网络，缓解梯度消失问题。\n使用残差学习(Residual Learning)，让网络学习残差映射而不是直接学习期望映射，简化优化过程。\n\n iii. 残差学习\n假设每一层神经网络表示一个映射：\ny=H(x)y = H(x)\ny=H(x)\n\nxxx 表示该层神经网络的输入，也是上一层神经网络的输出\nyyy 表示该层神经网络的输出\nH(x)H(x)H(x) 表示该层神经网络期望学习的映射\n\n假设神经网络实际学习的映射是 F(x)F(x)F(x)，那么 ResNet 不直接学习期望映射 F(x)=H(x)F(x) = H(x)F(x)=H(x)，而是学习残差映射 F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x，即该层神经网络的最终输出为 H(x)=F(x)+xH(x) = F(x) + xH(x)=F(x)+x。\n这是因为：\n\n\n\n原因\n说明\n\n\n\n\n防止退化问题\n1. 如果两层之间没有必要改变特征，那么相比于直接学习恒等映射 F(x)=xF(x) = xF(x)=x，学习残差映射 F(x)=0F(x) = 0F(x)=0 更容易2. 增加深度至少不会让性能变差，也就是说深度不再是负担\n\n\n梯度\n梯度: ∂H∂x=1+∂F∂x\\frac{\\partial H}{\\partial x} = 1 + \\frac{\\partial F}{\\partial x}∂x∂H​=1+∂x∂F​即使 ∂F∂x\\frac{\\partial F}{\\partial x}∂x∂F​ 很小，梯度至少为 111，避免了梯度消失问题\n\n\n\n iv. 残差块结构\n残差块 (Residual Block) 是 ResNet 的基本构建模块，包含两个或多个卷积层和一个跳跃连接。\n例如在 ResNet-18/34 中，其残差块结构为：\n1x → 3×3 Conv → BatchNorm → ReLU → 3×3 Conv → BatchNorm → +x → ReLU → out\n这属于一个基本残差块(Basic Residual Block)。整个 ResNet 就是不断堆叠残差块来构建深层网络，每一个残差块都包含一个跳跃连接，将输入直接添加到输出。\n v. 下采样的 shortcut\n残差块相加的前提是输入输出张量尺寸相同。但是在进行下采样时会导致输入输出尺寸不匹配。常见做法有两种：\n\n\n\n方法\n说明\n\n\n\n\n投影捷径Projection Shortcut\n使用 1×11 \\times 11×1 可学习的卷积核对 输入 进行线性映射，调整通道数和空间尺寸，使其与输出匹配1. H(x)=F(x)+WsxH(x) = F(x) + W_s xH(x)=F(x)+Ws​x2. WsW_sWs​ 是一个可学习的线性变换3. 尺寸为 1×1×C1 \\times 1 \\times C1×1×C，输出通道为 2C2C2C，步长为 222 的卷积层4. 权重参数个数为 2C(C+1)2C(C + 1)2C(C+1)\n\n\n零填充捷径Zero-Padding Shortcut\n将 shortcut 先用简单下采样，如果通道不够就用零填充补齐，使其与输出匹配1. 实现简单，不需要额外参数2. 主分支必须额外学习这些新通道，表达能力更弱\n\n\n\n\n 七. 训练 CNN\n 1. 初始化\n卷积层的初始化：\n\n\n\n问题\n说明\n结果\n\n\n\n\n初始化过小\n会导致前向传播时输出过小，激活函数 (如 ReLU) 输出接近于 000，使得梯度在反向传播时变得非常小，导致梯度消失问题1. 没经过一层，梯度都会缩小一次2. ReLU 会把一半负数变成 000，均值和方差都会进一步下降\n1. 前向传播信息消失2. 反向传播梯度消失3. 深层网络学不动\n\n\n初始化过大\n会导致前向传播时输出过大，激活函数 (如 ReLU) 输出饱和，梯度在反向传播时变得非常大，导致梯度爆炸问题1. 线性层输出被不断放大，使得方差迅速增大2. ReLU 不会限制正值，没有压缩机制\n1. 前向传播：数值爆炸2. 反向传播：梯度爆炸3. 数值不稳定每一层的平均值和方差都呈指数增长，导致激活值完全失控\n\n\n\n总而言之，卷积层的权重初始化尺度不能是一个固定常数，必须和输入维度 DDD 有关。因为一个神经元是 DDD 个输入的加权和，输入维度越大，输出的方差就越大。因此核心思想是让每一层的激活分布在前向传播和反向传播过程中保持稳定的均值和方差。\n i. Kaiming 初始化\nKaiming 初始化 (He Initialization) 适用于 ReLU 激活函数。其初始化公式如下：\nW∼N(0,2Din)W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{D_{in}}}\\right)\nW∼N(0,Din​2​​)\n\n线性层会把方差放大 DinD_{in}Din​ 倍\nReLU 会把方差缩小 0.50.50.5 倍（因为负值被置为 000，均值和方差都会下降）\n综合起来，方差变化为 Din×0.5=Din2D_{in} \\times 0.5 = \\frac{D_{in}}{2}Din​×0.5=2Din​​\n为了让方差保持不变，需要初始化时乘以 2Din\\sqrt{\\frac{2}{D_{in}}}Din​2​​\n\n 2. 迁移学习和微调\n迁移学习 (Transfer Learning) 是指将一个任务上训练好的模型应用到另一个相关任务上。对于 CNN 来说，通常是将预训练的卷积层权重应用到新的任务中，然后只训练最后的全连接层。\n这是因为：\n\nCNN 的低层学的是边缘、角点、纹理，非常通用\n高层学的是更抽象的特征，和具体任务相关性更大\n\n因此底层特征可以复用，高层分类器需要改造。\n只需要：\n\n从预训练模型中加载卷积层的权重\n冻结(freeze)卷积层的参数，不进行反向传播\n替换最后的全连接层，训练新的分类器\n\n如果数据够多，也可以所有层一起加载并使用较小学习率对整个网络进行训练，也叫做微调(Finetune)。\n\n\n\n数据量\\相似度\n相似数据\n不相似数据\n\n\n\n\n少量数据\n迁移学习最后一层\n换模型/收集更多数据\n\n\n大量数据\n微调整个网络\n微调或重新训练\n\n\n\n 3. Backbone CNN\nBackbone CNN 是指在计算机视觉任务中用作特征提取器的预训练卷积神经网络。它通常是一个在大规模数据集（如 ImageNet）上训练好的深度 CNN 模型，用于提取输入图像的高层次特征表示。\n其主要负责从输入图像中提取有用的特征，这些特征可以用于后续的任务，如分类、检测或分割。\n\n 八. R-CNN\nR-CNN (Region-based Convolutional Neural Networks) 是一种用于目标检测的深度学习方法。它结合了区域提议 (Region Proposal) 和卷积神经网络 (CNN) 的优势，实现了高效的目标检测。\n其同时输出目标类别和目标边界框位置，关键思路是：\n\n不在整张图像上滑动窗口检测目标\n先生成一组候选区域 (Region Proposals)，然后对每个候选区域进行分类和边界框回归\n\n其步骤包括:\n\n\n\n步骤\n简要\n说明\n\n\n\n\n111\n选择性搜索生成候选区域 (RoI)\n生成大约 200020002000 个可能包含目标的区域\n\n\n222\n包装图像区域为固定大小\n将每个 RoI 调整为固定尺寸\n\n\n333\n迁移学习提取特征\n使用预训练的 CNN 提取每个 RoI 的特征\n\n\n444\nSVM 分类器分类\n对每个 RoI 的特征进行分类\n\n\n555\n边界框回归\n调整候选框位置以更准确地拟合目标\n\n\n\n 1. 选择性搜索\n选择性搜索 (Selective Search) 是 R-CNN 中用于生成候选区域的一种方法。它通过结合图像的分割和层次聚类技术，生成一组可能包含目标的区域。选择性搜索的步骤包括：\n\n过分割图像，生成初始的超像素区域\n计算区域之间的相似度，基于颜色、纹理、大小和形状等特征\n通过层次聚类合并相似区域，生成不同尺度的候选区域\n\n最终选择性搜索会生成大约 200020002000 个候选区域(Regions of Interest, RoI)，这些区域被送入 CNN 进行进一步处理。\n比全滑窗少很多，但是仍然比较慢，且不是可学习的。\n 2. 包装图像区域\n由于 RoI 的大小和比例各不相同，R-CNN 需要对每个区域进行包装 (Warping) 操作，将其调整为固定大小的输入格式，例如 227×227227 \\times 227227×227 像素，以适应 CNN 的输入要求。包装过程包括：\n\n裁剪：从原始图像中裁剪出候选区域\n调整大小：将裁剪后的区域缩放到固定尺寸\n归一化：对像素值进行归一化处理，以适应 CNN 的输入规范\n\n这样能把任意任意大小的 RoI 丢进同一个 CNN，但会失去部分比例和位置信息。\n 3. 迁移学习\nR-CNN 通常使用预训练的 CNN 模型（如 AlexNet 或 VGGNet）作为特征提取器。对每一个包装后的 RoI 都单独 forward 一次 CNN 提取高层特征，得到一个固定维度的特征向量拿去做检测和分类。\n 4. SVM 分类器\nR-CNN 使用支持向量机 (SVM) 作为分类器，对每个 RoI 的特征向量进行分类。具体步骤包括：\n\n对每个类别训练一个 SVM 分类器，使用正样本（包含目标的 RoI）和负样本（不包含目标的 RoI）进行训练\n对每个 RoI 的特征向量进行分类，得到类别标签和置信度分数。\n\n 5. 边界框回归\n候选框（RoI）的位置通常不准确，所以除了分类，还要学一个回归器，把候选框“修正”到更贴近真实框。它输出不是直接预测最终框，而是预测相对偏移量（更稳定）。\n假设候选框为 P=(xp,yp,wp,hp)P = (x_p, y_p, w_p, h_p)P=(xp​,yp​,wp​,hp​)，真实框为 G=(xg,yg,wg,hg)G = (x_g, y_g, w_g, h_g)G=(xg​,yg​,wg​,hg​)，常用参数化为：\ntx=xg−xpwpty=yg−yphptw=log⁡(wgwp)th=log⁡(hghp)\\begin{aligned}\nt_x &amp; = \\frac{x_g - x_p}{w_p} \\\\\nt_y &amp; = \\frac{y_g - y_p}{h_p} \\\\\nt_w &amp; = \\log\\left(\\frac{w_g}{w_p}\\right) \\\\\nt_h &amp; = \\log\\left(\\frac{h_g}{h_p}\\right) \\\\\n\\end{aligned}\ntx​ty​tw​th​​=wp​xg​−xp​​=hp​yg​−yp​​=log(wp​wg​​)=log(hp​hg​​)​\nR-CNN 使用线性回归器来预测这些偏移量 (tx,ty,tw,th)(t_x, t_y, t_w, t_h)(tx​,ty​,tw​,th​)，然后根据预测的偏移量调整候选框的位置和大小，以获得更准确的边界框 G^=(x^,y^,w^,h^)\\hat{G} = ( \\hat{x}, \\hat{y}, \\hat{w}, \\hat{h} )G^=(x^,y^​,w^,h^)，其中：\nx^=tx×wp+xpy^=ty×hp+ypw^=exp⁡(tw)×wph^=exp⁡(th)×hp\\begin{aligned}\n\\hat{x} &amp; = t_x \\times w_p + x_p \\\\\n\\hat{y} &amp; = t_y \\times h_p + y_p \\\\\n\\hat{w} &amp; = \\exp(t_w) \\times w_p \\\\\n\\hat{h} &amp; = \\exp(t_h) \\times h_p \\\\\n\\end{aligned}\nx^y^​w^h^​=tx​×wp​+xp​=ty​×hp​+yp​=exp(tw​)×wp​=exp(th​)×hp​​\ndx,dydx, dydx,dy 是中心点的偏移，dw,dhdw, dhdw,dh 是宽高的缩放。\n 6. R-CNN 的缺点\nR-CNN 非常地慢，主要有以下几个原因：\n\n每张图要生成 200020002000 个 RoI\n每个 RoI 都要裁剪、调整大小\n每个 RoI 都要单独跑一边 CNN forward\n没有任何复用计算，即使彼此重叠也不复用\n\n因此难以满足实时检测的需求。\n 7. Fast R-CNN\nFast R-CNN 整张图只跑一次 CNN，做一次卷积得到共享特征图，然后截取 RoI 对应的特征区域，使用 RoI Pooling 层将其转换为固定大小的特征向量，再送入全连接层进行分类和边界框回归。整个过程将目标检测从多次独立 CNN 前向传播，转变为一次卷积 + 多个轻量级区域预测，实现了速度和训练方式上的重大提升。\n\n\n\n对比\nR-CNN\nFast R-CNN\n\n\n\n\n特征提取\n每个 RoI 都单独跑一次 CNN\n整张图只跑一次 CNN\n\n\nRoI 处理\n裁剪并调整大小\n裁取特征图区域并 RoI Pooling\n\n\n重复计算\n大量重复计算\n共享计算，效率高\n\n\n分类\nSVM 分类器\n网络内置分类头Linear + Softmax\n\n\n训练\n多阶段训练\n端到端联合训练\n\n\n速度\n慢\n快\n\n\n\n其结构可以拆成两部分：\n\n共享部分：\n\n\n\n\n\n\n\n\n\n\n整图 → ConvNet → conv feature map\n\nRoI 部分：\n\n\n\n\n\n\n\n\n\n\nRoI Pooling → FC / CNN → 两个输出分支\n其整体流程为：\n1234567891011121314151617181920212223输入图像  ↓Backbone CNN  ↓共享特征图 (H × W × C)  ↓RPN（小卷积网络）  ↓得到大量 anchors 的：  - objectness（是不是物体）  - bbox offsets（框怎么调）  ↓筛选 top-N proposals  ↓RoI Pooling  ↓固定大小的特征向量  ↓两个输出分支：  - 分类头 (Classification Head)：输出类别概率分布  - 回归头 (Regression Head)：输出边界框偏移量  ↓联合损失函数训练\n\n i. RPN\nRPN (Region Proposal Network) 是 Fast R-CNN 的一个重要组成部分，用于在共享特征图中生成候选区域 (RoI)。RPN 是一个轻量级的卷积神经网络，在共享的特征图上放置多个尺寸的 anchor，对每个 anchor 预测其是否包含目标以及边界框的偏移量，从而生成候选区域。\n它只负责是不是目标、大概位置，不负责分类和精确位置。\nRPN 本质上是一个非常小的全卷积网络：\n1233×3 conv → 聚合当前点及其邻域的上下文信息 ├─ 1×1 conv → objectness └─ 1×1 conv → bbox regression\n各个卷积核常用输出通道数为 512512512。\n这过程是实现了端到端、可学习、快速的候选区域生成方法。\n a. Anchor 机制\nAnchor 是 RPN 中用于生成候选框的一种机制。它是在特征图的每个位置上 预定义 的一组固定大小和比例的边界框，用于捕捉不同尺度和形状的目标。RPN 会对每个 anchor 进行分类和边界框回归，生成最终的候选区域。\n假设 Anchor 有 kkk 种不同的大小和比例，那么在特征图的每个位置上(像素点)，RPN 会生成 kkk 个候选框。最终会输出 H×W×kH \\times W \\times kH×W×k 个候选框。\n问题本质目标大小、比例变化大且多样，直接预测绝对框很难。Anchor作用将问题简化为选哪个模板接近目标 + 微调模板位置。\n b. RPN 输出\nRPN 对每个 anchor 进行两项预测：\n\n\n\n输出\n说明\n\n\n\n\nobjectness\n每个 anchor 是目标的概率1. foreground: 有物体2. background: 无物体\n\n\nbbox offsets\n每个 anchor 的边界框偏移量1. 预测 (dx,dy,dw,dh)(dx, dy, dw, dh)(dx,dy,dw,dh)\n\n\n\n c. 最终候选区域生成\nRPN 会根据 objectness 分数对所有 anchor 进行排序，选择前 NNN 个高分的 anchor 作为候选区域 (RoI)。然后根据预测的边界框偏移量对这些 anchor 进行调整，得到最终的候选框位置。\n最终这些候选框会被送入 RoI Pooling 层进行进一步处理。\n d. 正负样本\n正负样本定义：\n\n正样本 (Positive Samples)：与真实边界框的 IoU 大于 0.70.70.7 的 anchor\n负样本 (Negative Samples)：与真实边界框的 IoU 小于 0.30.30.3 的 anchor\n忽略样本 (Ignore Samples)：介于 0.30.30.3 和 0.70.70.7 之间的 anchor\n\nIoU 指的是 Intersection over Union，即交并比。\n e. RPN 损失函数\nRPN 使用联合损失函数同时优化目标分类和边界框回归。假设对于每个 anchor，有：\n\n真实类别标签 p∗p^*p∗，111 表示正样本，000 表示负样本\n预测类别概率 ppp\n真实边界框偏移量 t∗=(tx∗,ty∗,tw∗,th∗)\\mathbf{t}^* = (t_x^*, t_y^*, t_w^*, t_h^*)t∗=(tx∗​,ty∗​,tw∗​,th∗​)\n预测边界框偏移量 t=(tx,ty,tw,th)\\mathbf{t} = (t_x, t_y, t_w, t_h)t=(tx​,ty​,tw​,th​)\n\n联合损失函数定义为：\nL(p,p∗,t,t∗)=Lcls(p,p∗)+λ[p∗=1]Lloc(t,t∗)L(p, p^*, \\mathbf{t}, \\mathbf{t}^*) = L_{cls}(p, p^*) + \\lambda [p^* = 1] L_{loc}(\\mathbf{t}, \\mathbf{t}^*)\nL(p,p∗,t,t∗)=Lcls​(p,p∗)+λ[p∗=1]Lloc​(t,t∗)\n\n ii. RoI Pooling\nRoI Pooling 是 Fast R-CNN 中用于将不同大小的 RoI 转换为固定大小特征向量的操作。其过程包括：\n\n将 RoI 映射到共享特征图上，确定对应的区域\n将该区域划分为固定数量的子区域（如 7×77 \\times 77×7）\n对每个子区域进行最大池化，得到固定大小的特征图\n\n\n iii. 分类头和回归头\nFast R-CNN 不再使用 SVM 分类器，而是直接在网络中添加两个输出分支：\n\n分类头 (Classification Head)：使用 Linear(全连接层进行打分) + Softmax (转化成概率) 对每个 RoI 进行分类，输出类别概率分布。支持 end-to-end 训练。\n回归头 (Regression Head)：使用 Linear(全连接层) 对每个 RoI 进行边界框回归，输出边界框的偏移量。\n\n其和 R-CNN 一样进行回归参数化，但是直接再同一个网络中学习，和分类共享特征。\n\n iv. 联合损失函数\nFast R-CNN 使用联合损失函数同时优化分类和边界框回归。假设对于每个 RoI，有：\n\n真实类别标签 uuu\n预测类别概率分布 p=(p0,p1,…,pK)\\mathbf{p} = (p_0, p_1, \\ldots, p_K)p=(p0​,p1​,…,pK​)，其中 KKK 是类别数\n真实边界框偏移量 v=(vx,vy,vw,vh)\\mathbf{v} = (v_x, v_y, v_w, v_h)v=(vx​,vy​,vw​,vh​)\n预测边界框偏移量 v^=(v^x,v^y,v^w,v^h)\\hat{\\mathbf{v}} = (\\hat{v}_x, \\hat{v}_y, \\hat{v}_w, \\hat{v}_h)v^=(v^x​,v^y​,v^w​,v^h​)\n联合损失函数定义为：\n\nL(p,u,v^,v)=Lcls(p,u)+λ[u≥1]Lloc(v^,v)Lcls(p,u)=−log⁡pu\\begin{aligned}\nL(\\mathbf{p}, u, \\hat{\\mathbf{v}}, \\mathbf{v}) &amp;= L_{cls}(\\mathbf{p}, u) + \\lambda [u \\geq 1] L_{loc}(\\hat{\\mathbf{v}}, \\mathbf{v})\\\\\nL_{cls}(\\mathbf{p}, u) &amp; = -\\log p_u\n\\end{aligned}\nL(p,u,v^,v)Lcls​(p,u)​=Lcls​(p,u)+λ[u≥1]Lloc​(v^,v)=−logpu​​\n其中：\n\nλ\\lambdaλ 是平衡分类损失和定位损失的权重超参数\n分类损失 Lcls(p,u)L_{cls}(\\mathbf{p}, u)Lcls​(p,u) 使用交叉熵损失：\nsmooth L1 损失 Lloc(v^,v)L_{loc}(\\hat{\\mathbf{v}}, \\mathbf{v})Lloc​(v^,v)\n\n这是 R-CNN 做不到的，因为 R-CNN 的 SVM 和回归器是分开训练的。\n v. YOLO\nYOLO (You Only Look Once) 是一种端到端的目标检测方法，通过将目标检测任务转化为单一的回归问题，实现了实时的目标检测。YOLO 的核心思想是将输入图像划分为网格，每个网格负责预测该区域内的目标类别和边界框位置。\n相比 Fast R-CNN，YOLO 相当于只有 RPN，没有后续的 RoI Pooling 和分类回归头，直接在特征图上进行预测。\n对于每一个格子，YOLO 会输出：\n\nP(objectness)：该格子包含目标的概率\n边界框坐标 (x,y,w,h)(x, y, w, h)(x,y,w,h)：目标的位置和大小\n类别概率分布 (p1,p2,…,pK)(p_1, p_2, \\ldots, p_K)(p1​,p2​,…,pK​)：目标所属的类别\n\n 8. Mask R-CNN\nMask R-CNN 是一种用于实例分割的深度学习方法，扩展了 Fast R-CNN，通过添加一个分支来预测每个 RoI 的像素级别掩码，实现了目标检测和实例分割的统一。\n其结构在 Fast R-CNN 的基础上增加了一个掩码分支 (Mask Branch)，用于生成每个 RoI 的二进制掩码。\n i. 总 Loss\nMask R-CNN 的总损失函数包括三个部分：\nL=Lcls+Lloc+LmaskL = L_{cls} + L_{loc} + L_{mask}\nL=Lcls​+Lloc​+Lmask​\n\n分类损失 LclsL_{cls}Lcls​：与 Fast R-CNN 相同，使用交叉熵损失进行分类\n边界框回归损失 LlocL_{loc}Lloc​：与 Fast R-CNN 相同，使用 smooth L1 损失进行边界框回归\n掩码损失 LmaskL_{mask}Lmask​：使用二进制交叉熵损失对每个 RoI 的掩码进行训练\n\n ii. RoI Align\nMask R-CNN 引入了 RoI Align 层，解决了 RoI Pooling 中的量化误差问题。RoI Align 通过双线性插值的方法，从共享特征图中精确地提取 RoI 对应的特征区域，避免了池化操作中的空间信息丢失。\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/CNN","date":"2025-12-28T04:45:03.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,CNN,Convolutional Neural Networks","author_index":"zExNocs"},{"id":"05898597ea6e0528433f194d904b671a","title":"深度学习-正则化","content":"&lt;返回深度学习导航\n\n 一. 正则化概述\n正则化（Regularization）一般而言是任何防止过拟合或有助于优化的方法。具体而言，在训练优化目标(损失函数)中添加额外的惩罚项，限制模型的复杂度，以防止过拟合从而提高其泛化能力。\n 二. 常见正则化技术\n 1. 数据增强\n数据增强 (Data augmentation) 通过对训练数据进行各种变换来增加数据的多样性，从而提高模型的泛化能力。\n\n\n\n类型\n说明\n样例\n\n\n\n\n几何变换\n通常作用于图片\n旋转 (Rotate)缩放 (Scale)裁剪(Crop)水平变化(Horizontal Flip)\n\n\n增加噪声\n向数据中添加随机噪声\n高斯噪声 (Gaussian Noise)椒盐噪声 (Salt and Pepper Noise)\n\n\n颜色变换\n调整图像的颜色属性\n亮度 (Brightness)对比度 (Contrast)饱和度 (Saturation)\n\n\n\n\n\n\n 2. 提前停止\n提前停止 (Early Stopping) 是一种在训练过程中监控模型在验证集上的性能，并在性能开始恶化时停止训练的方法。这样可以防止模型过度拟合训练数据。\n\n基础思路:\n\n不要将网络训练到过小的训练误差，使用验证误差来决定何时停止训练。\n\n步骤：\n\n\n\n\n步骤\n说明\n\n\n\n\n111\n训练时，同时输出验证误差\n\n\n222\n每次验证误差改善时，保存一份权重副本\n\n\n333\n当验证误差在若干个epoch内没有改善时，停止训练\n\n\n444\n恢复到保存的最佳权重\n\n\n\n\n超参数：\n\n\n\n\n超参数\n说明\n\n\n\n\n耐心值Patience\n在停止训练前允许验证误差不改善的最大epoch数\n\n\n最小改善值Min Delta\n验证误差必须改善的最小值，才能被认为是“改善”\n\n\n\n\n优缺点：\n\n\n\n\n优点\n缺点\n\n\n\n\n高效：与训练同时进行；仅需存储一份额外的权重副本\n需要监控验证误差，增加计算开销\n\n\n简单：无需更改模型/算法\n可能错过更好的模型，尤其是在验证集噪声较大时\n\n\n有效：防止过拟合，提高泛化能力\n依赖于验证集的选择，可能导致模型偏向特定数据分布\n\n\n\n 3. Dropout\nDropout 是一种在训练过程中随机“丢弃”神经网络中的一部分神经元的方法。通过这种方式，网络不会过度依赖于特定的神经元，从而提高其泛化能力。\n\n基础思路：\n\n使用随机概率 ppp 将神经元的输出设为零，从而防止神经元之间的复杂共适应关系。\n具体来说，在每次训练迭代中，为所有输入单元和隐藏单元随机采样一个不同的二进制掩码，将掩码位与单元相乘从而丢弃一部分单元：\nh=[h1,h2,…,hn]m=[m1,m2,…,mn]mi∼Bernoulli(1−p)h^=h⊙m\\begin{aligned}\n\\mathbf{h} &amp; = [h_1, h_2, \\ldots, h_n] \\\\\n\\mathbf{m} &amp; = [m_1, m_2, \\ldots, m_n] &amp; m_i \\sim \\text{Bernoulli}(1 - p) \\\\\n\\hat{\\mathbf{h}} &amp; = \\mathbf{h} \\odot \\mathbf{m}\n\\end{aligned}\nhmh^​=[h1​,h2​,…,hn​]=[m1​,m2​,…,mn​]=h⊙m​mi​∼Bernoulli(1−p)​\n其中 Bernoulli(p)\\text{Bernoulli}(p)Bernoulli(p) 表示以概率 ppp 采样 111，以概率 1−p1 - p1−p 采样 000 (注意括号内是取 111 的概率)。\n一般来说，输入层的取 000 概率 ppp 是 0.20.20.2，隐藏层的 ppp 是 0.50.50.5。\n\n超参数:\n\n\n\n\n超参数\n说明\n\n\n\n\n丢弃概率Dropout Rate ppp\n每个神经元被丢弃的概率，通常在 0.20.20.2 到 0.50.50.5 之间\n\n\n\n\n优缺点：\n\n\n\n\n优点\n缺点\n\n\n\n\n简单易用：只需在训练时添加Dropout层\n增加训练时间：由于每次迭代都需要随机丢弃神经元\n\n\n有效防止过拟合：提高模型泛化能力\n需要调整丢弃概率等超参数\n\n\n减少神经元共适应：促使网络学习更鲁棒的特征\n在某些任务上可能不如其他正则化方法有效\n\n\n\n 4. 批量归一化\n批量归一化 (Batch Normalization) 通过对每个小批量的数据进行归一化处理，来稳定和加速神经网络的训练过程。\n在机器学习中，我们假设未来的数据将来自与训练数据相同的概率分布。对于一个隐藏单元，训练后前面的层会获得新的权重，导致该隐藏单元的输入分布发生变化，这种现象称为内部协变量偏移 (Internal Covariate Shift)。我们希望减少这种内部协变量偏移，以使后面的层受益。\n\n基础思路：\n\n对每个小批量的数据，计算其均值和方差，然后使用这些统计量对数据进行归一化处理：\nμB=1m∑i=1mxiσB2=1m∑i=1m(xi−μB)2x^i=xi−μBσB2+ϵyi=γx^i+β\\begin{aligned}\n\\mu_B &amp; = \\frac{1}{m} \\sum_{i=1}^{m} x_i \\\\\n\\sigma_B^2 &amp; = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2 \\\\\n\\hat{x}_i &amp; = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\\\\ny_i &amp; = \\gamma \\hat{x}_i + \\beta\n\\end{aligned}\nμB​σB2​x^i​yi​​=m1​i=1∑m​xi​=m1​i=1∑m​(xi​−μB​)2=σB2​+ϵ​xi​−μB​​=γx^i​+β​\n其中:\n\n\n\n符号\n说明\n\n\n\n\nmmm\n小批量的样本数量\n\n\nxix_ixi​\n小批量中的第 iii 个样本\n\n\nμB\\mu_BμB​\n小批量的均值\n\n\nσB2\\sigma_B^2σB2​\n小批量的方差\n\n\nϵ\\epsilonϵ\n一个小的常数，防止除零错误\n\n\nx^i\\hat{x}_ix^i​\n归一化后的样本\n\n\nyiy_iyi​\n批量归一化后的输出\n\n\nγ\\gammaγ\n缩放参数，可学习\n\n\nβ\\betaβ\n平移参数，可学习\n\n\n\n一般来说，批量归一化层通常放置在全连接层或卷积层之后，激活函数之前：\na=σ(BatchNorm(Wx+b))\\mathbf{a} = \\sigma(\\text{BatchNorm}(\\mathbf{W}x + \\mathbf{b}))\na=σ(BatchNorm(Wx+b))\n\n优缺点：\n\n\n\n\n优点\n缺点\n\n\n\n\n加速训练：允许使用更高的学习率\n增加计算开销：每个小批量都需要计算均值和方差\n\n\n稳定训练过程：减少内部协变量偏移\n依赖于小批量大小：小批量过小可能导致估计不准确\n\n\n具有一定的正则化效果：减少过拟合\n在某些情况下可能不如其他正则化方法有效\n\n\n\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/正则化","date":"2025-12-28T00:11:03.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Regularization","author_index":"zExNocs"},{"id":"b47e974ef3ed6b7f31d16c5994a2a8c7","title":"深度学习-激活函数","content":"&lt;返回深度学习导航\n\n 一. 激活函数概述\n激活函数 (Activation Function) 是神经网络中的关键组件，决定了神经元的输出。它引入非线性，使得神经网络能够学习和表示复杂的函数映射关系。\n激活函数的主要作用包括：\n\n引入非线性：使神经网络能够学习复杂的模式和特征。\n控制输出范围：不同的激活函数有不同的输出范围，有助于稳定训练过程。\n影响梯度传播：激活函数的选择会影响梯度的计算和传播，进而影响模型的训练效果。\n\n 二. 梯度下降/反向传播\n对于错误函数 L(w)\\mathcal{L}(\\mathbf{w})L(w)，假设模型函数是 σ(f(x,w))\\sigma(f(\\mathbf{x, w}))σ(f(x,w)), 其中 σ\\sigmaσ 是激活函数，fff 是神经元的加权和，w\\mathbf{w}w 是权重参数，x\\mathbf{x}x 是输入特征。\n对于权重参数 wiw_iwi​，其对于 L\\mathcal{L}L 的偏导数为：\n∂L∂wi=∂L∂σ⋅∂σ∂f⋅∂f∂wi\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} &amp; = \\frac{\\partial \\mathcal{L}}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial f} \\cdot \\frac{\\partial f}{\\partial w_i} \\\\\n\\end{aligned}\n∂wi​∂L​​=∂σ∂L​⋅∂f∂σ​⋅∂wi​∂f​​\n其中：\n\n\n\n公式\n含义\n\n\n\n\n∂L∂σ\\frac{\\partial \\mathcal{L}}{\\partial \\sigma}∂σ∂L​\n1. 损失函数关于激活函数输出的导数2. 由损失函数的类型直接决定\n\n\n∂σ∂f\\frac{\\partial \\sigma}{\\partial f}∂f∂σ​\n1. 激活函数关于其输入的导数2. 由激活函数的选择决定\n\n\n∂f∂wi\\frac{\\partial f}{\\partial w_i}∂wi​∂f​\n1. 神经元加权和关于权重的导数2. 通常为输入特征 xix_ixi​\n\n\n\n 三. 常见激活函数\n\n\n\n激活函数\n公式\n图像\n说明\n导数\n\n\n\n\n阶跃函数Step Function\nσ(x)={1,x≥00,x&lt;0\\sigma(x) = \\begin{cases} 1, &amp; x \\ge 0 \\\\ 0, &amp; x &lt; 0 \\end{cases}σ(x)={1,0,​x≥0x&lt;0​\n\n1. 输出二值2. 难以用于梯度下降\n不可导\n\n\nSigmoidLogistic\nσ(x)=11+e−x\\begin{aligned}\\sigma(x) = \\frac{1}{1 + e^{-x}}\\end{aligned}σ(x)=1+e−x1​​\n\n1. 输出范围 (0,1)(0, 1)(0,1)2. 可用于二元分类概率3. 存在梯度消失问题4. 输出非以 000 为中心\nσ′(x)=σ(x)(1−σ(x))\\sigma&#x27;(x) = \\sigma(x)(1 - \\sigma(x))σ′(x)=σ(x)(1−σ(x))\n\n\n双曲正切Tanh\nσ(x)=ex−e−xex+e−x=21+e−2x−1\\begin{aligned}\\sigma(x) &amp;= \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\\\\&amp;= \\frac{2}{1 + e^{-2x}} - 1 \\end{aligned}σ(x)​=ex+e−xex−e−x​=1+e−2x2​−1​\n\n1. 输出范围 (−1,1)(-1, 1)(−1,1)2. 输出以 000 为中心3. 存在梯度消失问题\nσ′(x)=1−σ2(x)\\sigma&#x27;(x) = 1 - \\sigma^2(x)σ′(x)=1−σ2(x)\n\n\nReLURectified Linear Unit\nσ(x)=max⁡(0,x)\\sigma(x) = \\max(0, x)σ(x)=max(0,x)\n\n1. 计算简单2. 收敛速度快3. 消除了梯度消失问题4. 引入了稀疏性5. 常用于图片识别6. 存在神经元死亡问题\nσ′(x)={1,x&gt;00,x≤0\\sigma&#x27;(x) = \\begin{cases} 1, &amp; x &gt; 0 \\\\ 0, &amp; x \\le 0 \\end{cases}σ′(x)={1,0,​x&gt;0x≤0​\n\n\nLeaky ReLU\nσ(x)={x,x&gt;0αx,x≤0\\sigma(x) = \\begin{cases} x, &amp; x &gt; 0 \\\\ \\alpha x, &amp; x \\le 0 \\end{cases}σ(x)={x,αx,​x&gt;0x≤0​α\\alphaα 很小\n\n解决了神经元死亡问题\nσ′(x)={1,x&gt;0α,x≤0\\sigma&#x27;(x) = \\begin{cases} 1, &amp; x &gt; 0 \\\\ \\alpha, &amp; x \\le 0 \\end{cases}σ′(x)={1,α,​x&gt;0x≤0​\n\n\n\n 四. 常见问题\n 1. 梯度消失问题\n梯度消失问题(Vanishing gradients)指的是在采用特定激活函数的神经网络深层中，随着梯度通过多层传播，梯度值逐渐变小，最终导致前面几层的权重更新非常缓慢，甚至停止更新。这会严重影响模型的训练效果。\n例如使用 Sigmoid 激活函数时，当 x≫0x ≫ 0x≫0 或 x≪0x ≪ 0x≪0 时，导数 σ′(x)\\sigma&#x27;(x)σ′(x) 会接近于 000，导致梯度在反向传播过程中逐渐减小，最终导致梯度消失。\n解决方法是使用更好的激活函数，例如 ReLU 及其变种，这些函数在正区间内具有恒定的梯度，从而缓解了梯度消失问题。\n 2. 神经元死亡问题\n神经元死亡问题主要出现在使用 ReLU 激活函数时。当输入值为负时，ReLU 的输出为 000，其导数也为 000。\n如果某个神经元在训练过程中多次接收到负输入，那么该神经元的权重将无法更新，导致该神经元“死亡”，无法对后续的输入做出响应。\n 3. 稀疏性\n稀疏性指的是在神经网络中，许多神经元的输出为零。\n稀疏性有助于提高模型的泛化能力，减少过拟合，同时也能降低计算复杂度，因为只有一部分神经元在工作。\n这种现象在使用 ReLU 激活函数时尤为明显。\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/激活函数","date":"2025-12-26T19:10:03.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Activation Function","author_index":"zExNocs"},{"id":"88524df85638f6b23deb5b7d601d5559","title":"深度学习-学习率","content":"&lt;返回深度学习导航\n\n 一. 学习率\n学习率(Learning Rate, LR)是每次迭代权值更新的步长。不存在唯一完美的学习率，不同模型都需调整。\n\n\n\n情况\n结果\n\n\n\n\n非常大\nloss 爆炸\n\n\n偏大\n震荡，不易收敛\n\n\n偏小\n收敛慢\n\n\n适中\n快速下降且稳定\n\n\n\n 二. 学习率调度\n\n\n\n时期\n说明\n调整\n\n\n\n\n早期\n希望快速下降 loss\n高学习率\n\n\n后期\n更精细地调整\n低学习率\n\n\n\n因此需要一个随 epoch 逐渐减少的学习率，叫做学习率衰减（Learning Rate Decay）。这个过程叫做学习率调度(Learning Rate Schedule)。\n以下假设\n\n\n\n参数\n说明\n\n\n\n\nttt\n当前 epoch\n\n\nTTT\n总 epoch\n\n\nη0\\eta_0η0​\n初始输入学习率\n\n\nηt\\eta_tηt​\n在 ttt 下的学习率\n\n\n\n 1. 阶梯衰减\n阶梯衰减(Step Decay)是在固定的 epoch 步长 aaa 让学习率降低某一个倍数 β∈(0,1)\\beta \\in (0, 1)β∈(0,1)\nηt=β∣ta∣η0\\eta_t = \\beta^{\\mid\\frac{t}{a}\\mid} \\eta_0\nηt​=β∣at​∣η0​\n例如 β=0.1,a=30\\beta = 0.1, a = 30β=0.1,a=30。\n特点：\n\n简单粗暴\n适合 CNN、ResNet 这种稳定模型\n梯度下降图中会看到明显的台阶状下降\n\n 2. 余弦衰减\n余弦衰减 (Cosine Decay)：\nηt=12η0(1+cos⁡(tπT))\\eta_t = \\frac{1}{2}\\eta_0 (1 + \\cos(\\frac{t\\pi}{T}))\nηt​=21​η0​(1+cos(Ttπ​))\n特点：\n\n平滑衰减，不会突然掉太多\n训练后期学习率很小但不为 000\n常与 warmup 一起使用\n适合一般深度学习模型\n\n 3. 线性衰减\n线性衰减(Linear Decay)：\nηt=η0(1−tT)\\eta_t = \\eta_0(1 - \\frac{t}{T})\nηt​=η0​(1−Tt​)\n\n常用于 NLP 模型 (例如 BERT)\n\n 4. 逆平方根衰减\n逆平方根衰减(Inverse Square Root Decay):\nηt=η0t\\eta_t = \\frac{\\eta_0}{\\sqrt{t}}\nηt​=t​η0​​\n\n初期下降很快\n之后衰减慢\n用于 Transformer\n\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/学习率","date":"2025-12-26T01:00:25.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Learning Rate","author_index":"zExNocs"},{"id":"9d23ccc00b1ad8b23cfee64325a0d6fb","title":"深度学习-优化器","content":"&lt;返回深度学习导航\n\n 一. 深度学习优化介绍\n深度学习的优化指在每个训练步骤中调整模型参数以减少模型误差 L\\mathcal{L}L 的过程。\n优化算法定义了如何执行此过程。所有优化逻辑都封装在优化器对象中。\n阶级优化指如何用导数来优化参数/权值。\n 1. 一阶优化\n一阶优化(First-Order Optimization)方法只使用 一阶导数来更新参数，例如 Gradient Descent、SGD、Momentum、Adam。\n核心思想：\n\n用梯度做线性近似\n朝着使线性近似下降最快的方向走一步\n选择学习率作为步长\n\n更新公式普遍为：\nwt+1=wt−η∇L(wt)\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t)\nwt+1​=wt​−η∇L(wt​)\n其中 η\\etaη 是学习率。\n 2. 二阶优化\n二阶优化(Second-Order Optimization)使用梯度（一次导数） 和 Hessian（海森矩阵，二阶导数） 做二阶近似。\n\n二阶优化公式\n二阶 Taylor 展开：\nJ(θ)≈J(θ0)+(θ−θ0)T∇J(θ0)+12(θ−θ0)TH(θ−θ0)J(\\theta) ≈ J(\\theta_0) + (\\theta - \\theta_0)^T \\nabla J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^T H(\\theta - \\theta_0)\nJ(θ)≈J(θ0​)+(θ−θ0​)T∇J(θ0​)+21​(θ−θ0​)TH(θ−θ0​)\n对零界点求解：\nθ∗=θ0−H−1∇J(θ0)\\theta^* = \\theta_0 - H^{-1}\\nabla J(\\theta_0)\nθ∗=θ0​−H−1∇J(θ0​)\n\n\n为什么二阶方法对深度学习不现实：\n\nHessian 是 N×NN×NN×N 矩阵，元素数量 O(N2)O(N^2)O(N2)，会内存爆炸\n矩阵求逆需要 O(N3)O(N^3)O(N3)，深度学习不可能承受这种计算量\n\n 二. 梯度下降法\n梯度下降法(Gradient Descent Rule)是一种一阶优化，通过偏导找到最快下降错误函数的方向：\n{w←w−η∇L(w)η∈(0,1)wi←wi−η∂L∂wi\\begin{cases}\n\\mathbf{w} ← \\mathbf{w} - \\eta \\nabla \\mathcal{L}(\\mathbf{w}) &amp; \\eta \\in (0, 1) \\\\\nw_i ← w_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_i}\n\\end{cases}\n{w←w−η∇L(w)wi​←wi​−η∂wi​∂L​​η∈(0,1)\n此外，在正则化项 R(w)R(\\mathbf{w})R(w) 存在时，损失函数为：\nL(w)=1K∑k=1KLk((xk,dk),w)+λR(w)∇wL(w)=1K∑k=1K∇wLk((xk,dk),w)+λ∇wR(w)\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}) &amp;= \\frac{1}{K}\\sum_{k = 1}^K L_k((\\mathbf{x}_k, d_k), \\mathbf{w}) + \\lambda R(\\mathbf{w})\\\\\n\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w})&amp;=\\frac{1}{K}\\sum_{k=1}^{K}\\nabla_{\\mathbf{w}}L_{k}((\\mathbf{x}_{k},d_{k}),\\mathbf{w})+\\lambda \\nabla_{\\mathbf{w}}R(\\mathbf{w})\n\\end{aligned}\nL(w)∇w​L(w)​=K1​k=1∑K​Lk​((xk​,dk​),w)+λR(w)=K1​k=1∑K​∇w​Lk​((xk​,dk​),w)+λ∇w​R(w)​\n\nKKK：样本总数\n(xk,dk)(\\mathbf{x}_k, d_k)(xk​,dk​)：第 kkk 个样本的输入和目标输出\nLk((xk,dk),w)L_k((\\mathbf{x}_k, d_k), \\mathbf{w})Lk​((xk​,dk​),w)：第 kkk 个样本在权重 w\\mathbf{w}w 下的损失函数\nR(w)R(\\mathbf{w})R(w)：正则化项\nλ\\lambdaλ：正则化系数\n\n\n 1. 梯度下降的基本原理\n对于单维度的函数来说，其导数：\ndf(x)dx=lim⁡h→0f(x+h)−f(x)h\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\ndxdf(x)​=h→0lim​hf(x+h)−f(x)​\n对于多维度的函数来说，其梯度是个向量：\n∇f=(∂f∂x1,∂f∂x2,… )\\nabla f=(\\frac{\\partial f}{\\partial x_{1}}, \\frac{\\partial f}{\\partial x_{2}}, \\dots)\n∇f=(∂x1​∂f​,∂x2​∂f​,…)\n任意方向的单位向量 v\\mathbf{v}v ，得该方向的斜率：\nslope=∇f⋅v\\text{slope} = \\nabla f \\cdot \\mathbf{v}\nslope=∇f⋅v\n梯度是函数上升最快的方向，那么其下降最快的方向就是 −∇f-\\nabla f−∇f。\n那么此时优化的值应该是\nw=w−η∇f\\mathbf{w} = \\mathbf{w} - \\eta \\nabla f\nw=w−η∇f\n 2. 求梯度 ∇f\\nabla f∇f\n分为数值梯度和解析梯度:\n i. 数值梯度\n数值梯度(Numerical gradient) 是用数值差分近似的方式求坡度 (导数定义)：\n\n\n\n步骤\n说明\n\n\n\n\n111\n选取极小值 hhh，例如 h=0.0001h = 0.0001h=0.0001\n\n\n222\n为每一个权值 wiw_iwi​ 计算 wi+hw_i + hwi​+h 和导数 df(wi)dwi=lim⁡h→0f(wi+h)−f(wi)h\\frac{df(w_i)}{dw_i} = \\lim_{h \\to 0} \\frac{f(w_i + h) - f(w_i)}{h}dwi​df(wi​)​=limh→0​hf(wi​+h)−f(wi​)​\n\n\n\n因为需要多次向前计算，所以计算成本非常高，效率非常慢，并且精度比较低。\n ii. 解析梯度\n解析梯度(analytic gradient) 是利用解析式（偏导公式）求梯度。\n其精度很高且速度很快，基本上所有的现代机器学习都使用该方法。\n\n 三. 梯度下降的实例\n注意以下输入 k=(d,x)k = (d, \\mathbf{x})k=(d,x) 表示第 kkk 个样本的目标输出 ddd 和输入特征 x\\mathbf{x}x。\n\n\n\n类型\n公式\n梯度/偏导\n\n\n\n\n均方差MSE\nL(w)=12∑k=1K(dk−ok)2\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2}\\sum_{k = 1}^K (d_k - o_k)^2L(w)=21​∑k=1K​(dk​−ok​)2\n∂L∂wi=−∑k=1K(dk−ok)⋅∂ok∂wi\\frac{\\partial \\mathcal{L}}{\\partial w_i} = -\\sum_{k = 1}^K(d_k - o_k) \\cdot \\frac{\\partial o_k}{\\partial w_i}∂wi​∂L​=−∑k=1K​(dk​−ok​)⋅∂wi​∂ok​​\n\n\n交叉熵CE\nL(w)=−∑k=1K[dklog⁡(ok)+(1−dk)log⁡(1−ok)]\\mathcal{L}(\\mathbf{w}) = -\\sum_{k = 1}^K \\left[d_k \\log(o_k) + (1 - d_k)\\log(1 - o_k)\\right]L(w)=−∑k=1K​[dk​log(ok​)+(1−dk​)log(1−ok​)]\n∂L∂wi=∑k=1K(ok−dkok(1−ok))⋅∂ok∂wi\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\sum_{k = 1}^K \\left(\\frac{o_k - d_k}{o_k(1 - o_k)}\\right) \\cdot \\frac{\\partial o_k}{\\partial w_i}∂wi​∂L​=∑k=1K​(ok​(1−ok​)ok​−dk​​)⋅∂wi​∂ok​​\n\n\n\n 1. 均方差 MSE 的梯度下降\n均方差 (Mean Squared Error, MSE) 常用于回归问题。\n对于均方误差作为损失函数时：\nL(w)=12∑k=1K(dk−ok)2\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2}\\sum_{k = 1}^K (d_k - o_k)^2\nL(w)=21​k=1∑K​(dk​−ok​)2\n在没有任何激活函数和正则化时，此时对于单个权值 wiw_iwi​ 的偏导数为：\n∂L∂wi=−∑k=1K(dk−ok)xk,i\\frac{\\partial \\mathcal{L}}{\\partial w_i} = -\\sum_{k = 1}^K(d_k - o_k)x_{k, i}\n∂wi​∂L​=−k=1∑K​(dk​−ok​)xk,i​\n因此\n{w←w+η∑k=1K(dk−ok)xkwi←wi+η∑k=1K(dk−ok)xk,i\\begin{cases}\n\\mathbf{w} ← \\mathbf{w} + \\eta \\sum_{k = 1}^K(d_k - o_k)\\mathbf{x}_k\\\\\nw_i ← w_i + \\eta \\sum_{k = 1}^K(d_k - o_k)x_{k, i}\\\\\n\\end{cases}\n{w←w+η∑k=1K​(dk​−ok​)xk​wi​←wi​+η∑k=1K​(dk​−ok​)xk,i​​\n\n偏导 ∂L∂wi\\frac{\\partial \\mathcal{L}}{\\partial w_i}∂wi​∂L​ 公式获取\n∂L∂wi=∂∂wi(12∑k=1K(dk−ok)2)=12∑k=1K(∂∂wi(dk−ok)2)=12∑k=1K(2(dk−ok)∂∂wi(dk−ok))=∑k=1K((dk−ok)∂∂wi(dk−w0−∑i=1nwixk,i))=∑k=1K((dk−ok)(−xk,i)=−∑k=1K(dk−ok)xk,i\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{i}}=&amp; \\frac{\\partial }{\\partial w_{i}} \\left(\\frac{1}{2} \\sum_{k=1}^{K}(d_k-o_k )^{2}\\right)\\\\ =&amp; \\frac{1}{2} \\sum_{k=1}^{K}\\left(\\frac{\\partial  }{\\partial w_{i}}\\left(d_k-o_k\\right)^{2}\\right)\\\\ =&amp; \\frac{1}{2} \\sum_{k=1}^{K}\\left(2(d_k-o_k) \\frac{\\partial }{\\partial w_{i}}\\left(d_k-o_k\\right)\\right)\\\\ =&amp; \\sum_{k=1}^{K}\\left(\\left(d_k-o_k\\right) \\frac{\\partial }{\\partial w_{i}}\\left(d_k-w_{0}-\\sum_{i=1}^{n}w_{i}x_{k, i} \\right)\\right)\\\\ =&amp; \\sum_{k=1}^{K} \\left(\\left(d_k-o_k\\right) \\left(-x_{k, i} \\right)\\right.\\\\ =&amp; -\\sum_{k=1}^{K} \\big(d_k-o_k\\big)x_{k, i} \n\\end{aligned}\n∂wi​∂L​======​∂wi​∂​(21​k=1∑K​(dk​−ok​)2)21​k=1∑K​(∂wi​∂​(dk​−ok​)2)21​k=1∑K​(2(dk​−ok​)∂wi​∂​(dk​−ok​))k=1∑K​((dk​−ok​)∂wi​∂​(dk​−w0​−i=1∑n​wi​xk,i​))k=1∑K​((dk​−ok​)(−xk,i​)−k=1∑K​(dk​−ok​)xk,i​​\n\n\n 2. 交叉熵 CE 的梯度下降\n交叉熵 (Cross Entropy, CE) 常用于分类问题 + one-hot 编码。输出 o(k,i)o_{(k,i)}o(k,i)​ 表示 kkk 为类别 iii 的概率预测值。\n其要求 dk∈{0,1},ok∈(0,1)d_k \\in \\{0, 1\\}, o_k \\in (0, 1)dk​∈{0,1},ok​∈(0,1)。一般使用 Sigmoid 激活函数将输出映射到 (0,1)(0, 1)(0,1)。\n对于交叉熵作为损失函数时：\nL(w)=−∑k=1K[dklog⁡(ok)+(1−dk)log⁡(1−ok)]={log⁡(ok)dk=1log⁡(1−ok)dk=0\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}) &amp;= -\\sum_{k = 1}^K \\left[d_k \\log(o_k) + (1 - d_k)\\log(1 - o_k)\\right]\\\\\n&amp;= \\begin{cases}\n    \\log(o_k) &amp; d_k = 1\\\\\n    \\log(1 - o_k) &amp; d_k = 0\n  \\end{cases}\n\\end{aligned}\nL(w)​=−k=1∑K​[dk​log(ok​)+(1−dk​)log(1−ok​)]={log(ok​)log(1−ok​)​dk​=1dk​=0​​\n当预测概率距离真实值越近时，损失越小。\n在没有任何激活函数和正则化时，此时对于单个权值 wiw_iwi​ 的偏导数为：\n∂L∂wi=∑k=1K(ok−dkok(1−ok))xk,i=∑k=1K(dkok−1−dk1−ok)={1okdk=1−11−okdk=0\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} &amp;= \\sum_{k = 1}^K \\left(\\frac{o_k - d_k}{o_k(1 - o_k)}\\right) x_{k, i} \\\\\n&amp; = \\sum_{k = 1}^K \\left(\\frac{d_k}{o_k} - \\frac{1 - d_k}{1 - o_k}\\right)\\\\\n&amp; = \\begin{cases}\n    \\frac{1}{o_k} &amp; d_k = 1\\\\\n    -\\frac{1}{1 - o_k} &amp; d_k = 0\n  \\end{cases}\n\\end{aligned}\n∂wi​∂L​​=k=1∑K​(ok​(1−ok​)ok​−dk​​)xk,i​=k=1∑K​(ok​dk​​−1−ok​1−dk​​)={ok​1​−1−ok​1​​dk​=1dk​=0​​\n因此\nw←w−η∑k=1K(ok−dkok(1−ok))xk\\mathbf{w} ← \\mathbf{w} - \\eta \\sum_{k = 1}^K \\left(\\frac{o_k - d_k}{o_k(1 - o_k)}\\right) \\mathbf{x}_k\\\\\nw←w−ηk=1∑K​(ok​(1−ok​)ok​−dk​​)xk​\n 四. 优化器类型\n优化逻辑封装在优化器对象中，但是注意优化器只决定了怎么使用梯度来更新参数，而不决定怎么计算梯度。\n梯度的计算取决于损失函数、模型类型(激活函数、正则化等)。\n具体可看：\n\n深度学习-激活函数\n\n 1. 随机梯度下降 (SGD)\nSGD算法是从样本中 随机抽出一组样本，训练后按梯度更新一次，然后再抽取一组，再更新一次（重点：每次迭代使用一组样本，即 batch = 1），其公式为：\n{gt=∇wL(wt)wt+1=wt−ηgt\\begin{cases}\n\\mathbf{g}_t &amp;= \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}_t)\\\\\n\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\eta \\mathbf{g}_t \\\\\n\\end{cases}\n{gt​wt+1​​=∇w​L(wt​)=wt​−ηgt​​\n在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。\n随机梯度下降（SGD）在某些情况下会非常低效：\n i. 震荡\n当损失函数在不同方向上变化速度不同（各向异性）时，SGD表现很差。\n此时 SGD 沿陡峭方向疯狂震荡(Oscillation)，而沿平缓方向非常缓慢前进，从而导致收敛很慢。\n如下图梯度在陡峭 w2w_2w2​ 方向变化非常大，而在 w1w_1w1​ 上前进非常慢。\n\n\n\n ii. 鞍点\n当损失函数在某个点中其梯度为 000，该点称之为鞍点(Saddle Point)，此时梯度下降将会卡住无法移动。\n例如在二维度中：\nf(x,y)=x2−y2f(x, y) = x^2 - y^2\nf(x,y)=x2−y2\n其坡度 ∇f=(2x,2y)\\nabla f = (2x, 2y)∇f=(2x,2y)，在 (0,0)(0, 0)(0,0) 点处其坡度为 (0,0)(0, 0)(0,0)。此时根据坡度法无法进行优化，然而实际中其点并不是最优解点，甚至不是局部最优解。\n鞍点在高纬度中非常常见。\n iii. 局部最优解\n诸如爬山法一样，其方法也会找到局部最优解并卡在局部最优解。\n iv. 梯度噪声\nSGD 不是用全部数据，而是用 minibatch（小批量） 计算梯度。\n因此梯度带有随机噪声（noisy），拥有梯度噪声（Gradient Noise）。SGD 的更新方向一直乱跳，难以朝着最优点直线前进。这是噪声造成的路线抖动。\n 2. 动量优化的SGD\n为了解决上述 SGD 的问题，引入了动量优化(Momentum Optimization)。\n动量是给梯度增加惯性，将每次的梯度作为一个加速度，使用类其公式为：\n{gt=∇wL(wt)vt+1=ρvt+gtwt+1=wt−ηvt+1\\begin{cases}\n\\mathbf{g}_t &amp;= \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}_t)\\\\\n\\mathbf{v}_{t+1} &amp;= \\rho \\mathbf{v}_t + \\mathbf{g}_t \\\\\n\\mathbf{w}_{t + 1} &amp;= \\mathbf{w}_t - \\eta \\mathbf{v}_{t+1}\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​gt​vt+1​wt+1​​=∇w​L(wt​)=ρvt​+gt​=wt​−ηvt+1​​\n此外，与下面公式等价：\n{vt+1=ρvt+ηgtwt+1=wt−vt+1\\begin{cases}\n\\mathbf{v}_{t+1} &amp;= \\rho \\mathbf{v}_t + \\eta \\mathbf{g}_t \\\\\n\\mathbf{w}_{t + 1} &amp;= \\mathbf{w}_t - \\mathbf{v}_{t+1}\n\\end{cases}\n{vt+1​wt+1​​=ρvt​+ηgt​=wt​−vt+1​​\n只不过第一个公式是直接累加的梯度，第二个公式是累加的更新量。\n\n\n\n参数\n说明\n\n\n\n\nttt\n当前迭代数\n\n\nvtv_tvt​\n在 ttt 下的速度\n\n\nρ\\rhoρ\n惯性大小一般为 0.90.90.9 或 0.990.990.99\n\n\nη\\etaη\n学习率\n\n\n\n其代码：\n12345v = 0while True:    g = compute_gradient(w)   # 计算梯度    v = rho * v + g           # 更新速度    w -= learning_rate * v    # 根据速度更新参数\n速度平均了陡峭方向梯度，其好处：\n\n噪声方向会被自动平均掉 → SGD 的乱跳减少\n沿着真正下降方向会不断加速 → 更快收敛\n能冲出局部最小值和鞍点\n能减少震荡\n\n 3. 自适应学习率优化 AdaGrad\nAdaGrad (Adaptive Gradient Algorithm) 是一种自适应学习率优化算法。其关键思想是根据历史梯度调整每个参数的学习率。\n其优化权重的公式为：\n{gt=∇wL(wt)Gt=Gt−1+∥gt∥2wt+1=wt−ηGt+ϵ⊙gt\\begin{cases}\n\\mathbf{g}_t &amp;= \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}_t)\\\\\n\\mathbf{G}_t &amp;= \\mathbf{G}_{t-1} + \\lVert \\mathbf{g}_t \\rVert^2 \\\\\n\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{\\eta}{\\sqrt{\\mathbf{G}_t} + \\epsilon} \\odot \\mathbf{g}_t \\\\\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​gt​Gt​wt+1​​=∇w​L(wt​)=Gt−1​+∥gt​∥2=wt​−Gt​​+ϵη​⊙gt​​\n或者写作对于第 iii 个权重 wt+1,iw_{t+1, i}wt+1,i​：\n{gt,i=∂L(wt)∂wt,iGt,i=∑k=1tgk,i2wt+1,i=wt,i−ηGt,i+ϵgt,i\\begin{cases}\ng_{t, i} &amp;= \\frac{\\partial \\mathcal{L}(\\mathbf{w}_t)}{\\partial w_{t, i}} \\\\\nG_{t, i} &amp;= \\sum^{t}_{k=1}g_{k,i}^2 \\\\\nw_{t+1, i} &amp;= w_{t, i} - \\frac{\\eta}{\\sqrt{G_{t, i}} + \\epsilon} g_{t, i} \\\\\n\\end{cases}\n⎩⎪⎪⎪⎨⎪⎪⎪⎧​gt,i​Gt,i​wt+1,i​​=∂wt,i​∂L(wt​)​=∑k=1t​gk,i2​=wt,i​−Gt,i​​+ϵη​gt,i​​\n其中:\n\n\n\n符号\n说明\n\n\n\n\n⊙\\odot⊙\n逐元素相乘\n\n\n∥gt∥2\\lVert\\mathbf{g}_t\\rVert^2∥gt​∥2\n逐元素平方即 gt⊙gt\\mathbf{g}_t \\odot \\mathbf{g}_tgt​⊙gt​ 或 gtTgt\\mathbf{g}_{t}^T \\mathbf{g}_{t}gtT​gt​\n\n\nϵ\\epsilonϵ\n一个极小值，防止除以零\n\n\nη\\etaη\n初始学习率\n\n\nGt\\mathbf{G}_tGt​\n累积的历史梯度平方和\n\n\n∑i=1tgi2\\sqrt{\\sum^{t}_{i=1}g_i^2}∑i=1t​gi2​​\n将学习率缩放为 η∑i=1tgi2\\frac{\\eta}{\\sqrt{\\sum^{t}_{i=1}g_i^2}}∑i=1t​gi2​​η​1. 过去梯度大→→→分母大→→→学习率更小2. 过去梯度小→→→分母小→→→学习率更大3. 也就是减缓陡峭方向的移动，加快平缓方向的移动\n\n\n\n其代码：\n12345grad_squared = 0while True:  g = compute_gradient(w)                                  # 计算梯度  grad_squared += g * g                                    # 累加梯度平方  w -= learning_rate * g / (np.sqrt(grad_squared) + 1e-7)  # 根据梯度计算参数\nAdaGrad 会无限制累积，使得分母越来越大，从而步长趋向 000 导致无法学习。\n 4. RMSProp\nRMSProp 是 AdaGrad 的优化版。其关键思想是不累计全部历史梯度，而是使用 指数衰减的移动平均(Exponentially Moving Weighted Average, EMWA) 来计算梯度平方的平均值。\n其优化权重的公式为：\n{gt=∇wL(wt)Gt=ρGt−1+(1−ρ)∥gt∥2wt+1=wt−ηGt+ϵ⊙gt\\begin{cases}\n\\mathbf{g}_t &amp;= \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}_t)\\\\\n\\mathbf{G}_t &amp;= \\rho \\mathbf{G}_{t-1} + (1 - \\rho) \\lVert \\mathbf{g}_t \\rVert^2 \\\\\n\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{\\eta}{\\sqrt{\\mathbf{G}_t} + \\epsilon} \\odot \\mathbf{g}_t \\\\\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​gt​Gt​wt+1​​=∇w​L(wt​)=ρGt−1​+(1−ρ)∥gt​∥2=wt​−Gt​​+ϵη​⊙gt​​\n或者写作对于第 iii 个权重 wt+1,iw_{t+1, i}wt+1,i​：\n{gt,i=∂L(wt)∂wt,iE[g2]t,i=ρE[g2]t−1,i+(1−ρ)gt,i2wt+1,i=wt,i−ηE[g2]t,i+ϵgt,i\\begin{cases}\ng_{t, i} &amp;= \\frac{\\partial \\mathcal{L}(\\mathbf{w}_t)}{\\partial w_{t, i}} \\\\\nE[g^2]_{t, i} &amp;= \\rho E[g^2]_{t-1, i} + (1 - \\rho) g_{t, i}^2 \\\\\nw_{t+1, i} &amp;= w_{t, i} - \\frac{\\eta}{\\sqrt{E[g^2]_{t, i}} + \\epsilon} g_{t, i} \\\\\n\\end{cases}\n⎩⎪⎪⎪⎨⎪⎪⎪⎧​gt,i​E[g2]t,i​wt+1,i​​=∂wt,i​∂L(wt​)​=ρE[g2]t−1,i​+(1−ρ)gt,i2​=wt,i​−E[g2]t,i​​+ϵη​gt,i​​\n\n\n\n参数\n说明\n\n\n\n\nη\\etaη\n学习率\n\n\nρ\\rhoρ\n衰减率，表示过去的信息保留多少百分比一般为0.9,0.990.9, 0.990.9,0.99\n\n\nE[g2]tE[g^2]_tE[g2]t​\n梯度平方的指数加权平均（EMWA）1. 只保留近期梯度的加权平均2. 较旧的梯度会逐渐淡化掉\n\n\nϵ\\epsilonϵ\n极小值，防止除以 000\n\n\n\n其代码：\n12345grad_squared = 0while True:  g = compute_gradient(w)                                               # 计算梯度  grad_squared = decay_rate * grad_squared + (1 - decay_rate) * g * g   # 指数加权平均  w -= learning_rate * g / (np.sqrt(grad_squared) + 1e-7)               # 根据梯度计算参数\nRMSProp 实现了：\n\n陡峭方向减速\n平坦方向加速\n避免 AdaGrad 学习率越来越小的缺陷\n能在深度学习中保持长期有效更新\n\n 5. Adam\nAdam（Adaptive Moment Estimation）结合了 Momentum 和 RMSProp 两个优化器的优点：\n\n\n\nMomentum\nRMSProp\n\n\n\n\n平滑梯度，减少震荡\n大梯度方向减速，小梯度方向加速\n\n\n利用过去梯度的平均加速收敛\n避免 AdaGrad 学习率衰减至 0\n\n\n\n i. Adam 非正式 (almost) 版本\n公式：\n{gt=∇wL(wt)vt=β1vt−1+(1−β1)gtEt=β2Et−1+(1−β2)∥gt∥2wt+1=wt−ηϵ+Et⊙vt\\begin{cases}\n\\mathbf{g}_t &amp;= \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}_t) \\\\\n\\mathbf{v}_t &amp;= \\beta_1\\mathbf{v}_{t-1} + (1 - \\beta_1)\\mathbf{g}_t \\\\\n\\mathbf{E}_t &amp;= \\beta_2\\mathbf{E}_{t-1} + (1 - \\beta_2) \\lVert \\mathbf{g}_t \\rVert^2\\\\\n\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{\\eta}{\\epsilon + \\sqrt{\\mathbf{E}_t}} \\odot \\mathbf{v}_t\n\\end{cases}\n⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​gt​vt​Et​wt+1​​=∇w​L(wt​)=β1​vt−1​+(1−β1​)gt​=β2​Et−1​+(1−β2​)∥gt​∥2=wt​−ϵ+Et​​η​⊙vt​​\n\n\n\n参数\n说明\n\n\n\n\nttt\n当前迭代次数\n\n\nη\\etaη\n学习率，一般为 1E−31\\text{E}-31E−3 或 5E−45\\text{E}-45E−4\n\n\nvt\\mathbf{v}_tvt​\n动量项 / 一阶矩估计1. 类似 momentum 的速度向量2. 平滑梯度，使其不那么 noisy3. 方向更加稳定\n\n\nβ1\\beta_1β1​\n一阶矩惯性/衰减度一般 β1=0.9\\beta_1 = 0.9β1​=0.9\n\n\nEt\\mathbf{E}_tEt​\n均方梯度 / 二阶矩估计1. 类似 RMSProp 的梯度平方指数平均2. 衡量梯度最近是否大3. 令大梯度方向学习率变小，小梯度方向学习率变大\n\n\nβ2\\beta_2β2​\n二阶矩衰减度一般 β2=0.999\\beta_2 = 0.999β2​=0.999\n\n\n\n代码：\n1234567first_moment = 0second_moment = 0while True:  g = compute_gradient(w)  first_moment = beta1 * first_moment + (1 - beta1) * g  second_moment = beta2 * second_moment + (1 - beta2) * g * g  w -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)\n该版本中由于初始值为 000，前几步的 vt,Et\\mathbf{v}_t, \\mathbf{E}_tvt​,Et​ 会偏向 000，迭代会偏向 000（biased），导致更新过小。\n ii. Adam 真实公式 (full form)\n真实公式在非正式版本中添加了偏置修正（Bias Correction），修正后不会因初始为 000 而导致过小更新。公式：\n{v^t=vt1−β1tE^t=Et1−β2twt+1=wt−ηϵ+E^t⊙v^t\\begin{cases}\n\\hat{\\mathbf{v}}_t &amp;= \\frac{\\mathbf{v}_t}{1 - \\beta_1^t}\\\\\n\\hat{\\mathbf{E}}_t &amp;= \\frac{\\mathbf{E}_t}{1 - \\beta_2^t}\\\\\n\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{\\eta}{\\epsilon + \\sqrt{\\hat{\\mathbf{E}}_t}} \\odot \\hat{\\mathbf{v}}_t\n\\end{cases}\n⎩⎪⎪⎪⎨⎪⎪⎪⎧​v^t​E^t​wt+1​​=1−β1t​vt​​=1−β2t​Et​​=wt​−ϵ+E^t​​η​⊙v^t​​\n性质：\n\n初期放大 vt,Et\\mathbf{v}_t, \\mathbf{E}_tvt​,Et​，使其不至于过小\n11−βt\\frac{1}{1 - \\beta^t}1−βt1​ 后期会趋近于 111，此时偏置修正的影响会变得很小。\n\n代码：\n123456789first_moment = 0second_moment = 0for t in range(1, num_iterations):  g = compute_gradient(w)  first_moment = beta1 * first_moment + (1 - beta1) * g  second_moment = beta2 * second_moment + (1 - beta2) * g * g  first_unbias = first_moment / (1 - beta1 ** t)  second_unbias = second_moment / (1 - beta2 ** t)  w -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)\n 五. 优化器选择\n\n\n\n优化器\n说明\n\n\n\n\nAdam\n1. 是默认的好选择2. 每个参数都有自适应学习率3. 适合大部分任务4. 对学习率 不敏感：即使学习率保持常数，也能正常收敛\n\n\nSGD + Momentum\n1. 可能优于 Adam2. 收敛理论更扎实，在很多任务中最终表现可能更好(例如图像分类大模型)3. 学习率需要精细搜索4. 学习率调度很重要5. 因此调参要求比 Adam 高\n\n\n二阶优化\n如果能做到 full batch，就可以使用二阶优化：1. 牛顿法2. 共轭梯度3. 自然梯度理论上可以更快到达最优解，但是计算更昂贵\n\n\n\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/优化器","date":"2025-12-26T01:00:25.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Optimizer","author_index":"zExNocs"},{"id":"b81331e08fcec528371eb36f68471d43","title":"深度学习-感知机","content":"&lt;返回深度学习导航\n\n 一. 感知机\n感知机 (Perceptron) 是线性离散分类模型，也是最简单的人工神经网络模型。\n感知机可以被视为在 nnn 维实例特征空间中，存在这样一个超平面决策面：\n\n对位于超平面一侧的实例输出 111，另一侧的实例输出 −1-1−1。\n这个超平面被称为决策面(Decision Surface)。\n\n其公式定义如下：\nR={=w0+w1x1+w2x2+⋯+wnxn=w0+∑i=1nwixi=wT⋅xoutput=o=sign(R)={+1,R&gt;0−1,R≤0\\begin{aligned}\nR = &amp;\n    \\begin{cases} \n    = w_0 + w_1x_1+w_2x_2+\\dots+w_nx_n \\\\\n    = w_0 + \\sum_{i=1}^{n}w_ix_i\\\\\n    = \\mathbf{w}^T \\cdot \\mathbf{x}\n    \\end{cases}\\\\\n\\text{output} &amp; = o = \\text{sign}(R) = \n    \\begin{cases}\n    +1, &amp; R &gt; 0 \\\\\n    -1, &amp; R \\le 0\n    \\end{cases}\n\\end{aligned}\nR=output​⎩⎪⎪⎨⎪⎪⎧​=w0​+w1​x1​+w2​x2​+⋯+wn​xn​=w0​+∑i=1n​wi​xi​=wT⋅x​=o=sign(R)={+1,−1,​R&gt;0R≤0​​\n其中:\n\n\n\n符号\n含义\n\n\n\n\nxix_ixi​\n输入特征\n\n\nwiw_iwi​\n对应 xix_ixi​ 的权重\n\n\nw0w_0w0​\n偏置项 (bias)\n\n\nx\\mathbf{x}x\n输入向量 (x0,x1,x2,…,xn)(x_0, x_1, x_2, \\dots, x_n)(x0​,x1​,x2​,…,xn​)其中 x0=1x_0 = 1x0​=1 是固定值\n\n\nw\\mathbf{w}w\n权重向量 (w0,w1,w2,…,wn)(w_0, w_1, w_2, \\dots, w_n)(w0​,w1​,w2​,…,wn​)\n\n\nRRR\n加权和\n\n\nsign(R)\\text{sign}(R)sign(R)\n激活函数，决定输出类别\n\n\n\n\n\n\n 1. 感知机的特点\n\n感知机的决策面是线性的，只能解决线性可分的问题\n感知机收敛定理(Convergence Theorem)：\n\n只要训练样本是线性可分的：使用足够小的学习率 η\\etaη 感知机一定会在有限的迭代次数内收敛。\n如果是线性不可分的：感知机永远不会收敛。\n\n\n\n 2. 感知机的训练过程\n这个过程是一个简单的梯度下降过程，取误差函数 L(w)=12(d−o)2\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2}(d - o)^2L(w)=21​(d−o)2 的负梯度方向。\n假设我们输入元素为 (x,d)(\\mathbf{x}, d)(x,d) 的训练集，其中 x\\mathbf{x}x 是输入向量，d∈{1,−1}d \\in \\{1, -1\\}d∈{1,−1} 是分类：\n\n\n\n步骤\n说明\n\n\n\n\n111\n将权重 w\\mathbf{w}w 设置为较小的随机值，范围在 (−1,1)(-1, 1)(−1,1)\n\n\n222\n根据输入 x\\mathbf{x}x 计算模型值 RRR 和结果值 ooo\n\n\n333\n更新权重 w←w+η(d−o)x\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (d - o) \\mathbf{x}w←w+η(d−o)x其中 η∈(0,1)\\eta \\in (0, 1)η∈(0,1) 是学习率\n\n\n444\n重复步骤 222 直到所有训练集遍历\n\n\n\n对于步骤 333，对于单个权值 woldw_{old}wold​ 可以表达为：\n{∂L∂wi=−(d−o)xiwnew←wold−η∂L∂wiwnew←wold+η(d−o)xi\\begin{cases}\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = -(d - o)x_i\\\\\nw_{new} \\leftarrow w_{old} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_i}\\\\\nw_{new} \\leftarrow w_{old} + \\eta (d - o) x_i\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​∂wi​∂L​=−(d−o)xi​wnew​←wold​−η∂wi​∂L​wnew​←wold​+η(d−o)xi​​\n如果分类正确，则不需要更新；如果分类错误，则权重沿着正确方向移动：\n\n如果 xix_ixi​ 越大，权重改动越大\n学习率 η\\etaη 控制学习速度\nd−od - od−o 决定方向\n\n 二. 自适应线性神经元 (Adaline)\n自适应线性神经网络(Adaptive Linear Element, Adaline) 是线性回归模型，即输出是一个连续值。\nAdaline 是感知机的一个变种，相比感知机没有激活函数，即\no=w0+∑i=1nwixi=wT⋅xo = w_0 + \\sum_{i=1}^{n}w_ix_i = \\mathbf{w}^T \\cdot \\mathbf{x}\no=w0​+i=1∑n​wi​xi​=wT⋅x\n感知机对于非线性可分的数据无法收敛，但是 Adaline 可以通过寻找一个最合适的相似值克服这个困难。\n 1. Adaline 的损失函数\nAdaline 使用均方误差 (Mean Squared Error, MSE) 作为损失函数。\n假设 KKK 为一个批次(batch)大小，(xk,dk)(\\mathbf{x}_k, d_k)(xk​,dk​) 表示第 kkk 个输入向量、标签值对，则损失函数定义为：\nL(w)=12∑k=1K(dk−ok)2=12∑k=1K(dk−wT⋅xk)2\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}) &amp; = \\frac{1}{2} \\sum_{k=1}^{K} (d_k - o_k)^2 \\\\&amp; = \\frac{1}{2} \\sum_{k=1}^{K} (d_k - \\mathbf{w}^T \\cdot \\mathbf{x}_k)^2\n\\end{aligned}\nL(w)​=21​k=1∑K​(dk​−ok​)2=21​k=1∑K​(dk​−wT⋅xk​)2​\n我们通过梯度下降法来最小化损失函数，更新权重的公式为：\nw←w+η∑k=1K(dk−ok)xk\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{k=1}^{K} (d_k - o_k) \\mathbf{x}_k\nw←w+ηk=1∑K​(dk​−ok​)xk​\n其中 η\\etaη 是学习率。\n这里不再赘述计算过程，详细可以看优化器。\n 2. Adaline 学习算法\n基于梯度下降法则的学习算法：\n\n\n\n步骤\n说明\n\n\n\n\n111\n将权重 w\\mathbf{w}w 设置为较小的随机值，范围在 (−1,1)(-1, 1)(−1,1)\n\n\n222\n对当前批次的数据集元素 (xk,dk)(\\mathbf{x}_k, d_k)(xk​,dk​) 计算结果值 oko_kok​\n\n\n333\n计算 ∇L(w)=−∑k=1K(dk−ok)xk\\nabla \\mathcal{L}(\\mathbf{w}) = - \\sum^K_{k = 1}(d_k - o_k)\\mathbf{x}_k∇L(w)=−∑k=1K​(dk​−ok​)xk​\n\n\n444\n更新权值 w←w−η∇L(w)\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla \\mathcal{L}(\\mathbf{w})w←w−η∇L(w)\n\n\n555\n重复步骤 222 直到中止条件成立\n\n\n\n其中有以下模式：\n\n\n\n模式\n说明\n\n\n\n\n批次模式Batch Mode\n1. 记录所有样本的累计梯度，然后更新权重，即批次数等于样本数2. 学习率可以设置稍大，例如 η=0.2\\eta = 0.2η=0.2\n\n\n在线模式Online Mode\n1. 对于每个样本都更新一次权重，即批次数等于 1112. 学习率要比批次模式低，例如 η=0.01\\eta = 0.01η=0.013. 也可以叫做:a. 随机梯度下降(Stochastic Gradient Descent)b. 增量梯度下降(Incremental Gradient Descent)c. 最小均方 (Least Mean Square, LMS)d. Widrow Hoffe. 三角洲法则 (Delta Rule)\n\n\n\n 3. 训练中止\n中止训练通常有两种方式：\n\n达到预设的 Epoch\nError 小于预设值\n\n 三. 多层感知机 (MLP)\n线性模型只能解决线性可分问题，而多层感知机 (Multi-Layer Perceptron, MLP) 通过引入隐藏层 (Hidden Layer) 和非线性激活函数，能够解决非线性可分问题。\n多层感知机是一种前馈神经网络 (Feedforward Neural Network)，其基本结构包括输入层、一个或多个隐藏层和输出层。此外，多层感知机是一个全连接层次网络 (Fully Connected Network, FC)，即每一层的每个神经元都与下一层的每个神经元相连。\nMLP 通常使用反向传播算法 (Backpropagation) 来训练网络，通过最小化损失函数来调整权重。\n与单层感知机不同，MLP 的权重是一个二维矩阵，而不是一个一维向量。\n 1. 多层感知机结构\n多层感知机是一个有向带权无环图，其表示如下：\n\n\n\n结构\n说明\n\n\n\n\n输入层节点\n每个节点对应一个输入特征，即输入向量的一个元素\n\n\n隐藏层节点\n每个节点是一个神经元，接收前一层的加权输入，经过激活函数后输出到下一层只输出一个值，同一个层的节点组成一个一维向量\n\n\n输出层节点\n每个节点对应一个输出类别或回归值\n\n\n偏置节点bias\n除了输出层通常会有一个偏置节点，输出恒为 111，用于调整激活函数的阈值\n\n\n边\n连接不同层的节点，表示一个权重 wl,i,jw_{l, i,j}wl,i,j​表示第 lll 层节点 iii 到第 l+1l+1l+1 层节点 jjj 的连接权重每一层的权重构成一个二维矩阵\n\n\n\n多层感知机基本上由输入层(输入向量)、隐藏层(多个神经元)和输出层(输出向量)组成。每一层的神经元与下一层的神经元全连接。\n从某一层到不同的下一层神经元都有不同的权重，可能有不同的激活函数。同一层的神经元组成了一个一维向量。\n\n\n\n\n\n\n 2. 多层感知机符号说明\n假设一个多层感知机有 LLL 层(不包括输入层)，在列向量系统中，其符号说明如下：\n\n\n\n符号\n说明\n\n\n\n\nlll\n第 l∈[0,L]l \\in [0, L]l∈[0,L] 层数l=0l = 0l=0 表示输入层\n\n\nnln_lnl​\n第 lll 层的节点数量包括偏置节点(bias)\n\n\nx\\mathbf{x}x\n一个样本的输入其中 x0=1\\mathbf{x}_0 = 1x0​=1 表示偏置节点(bias)\n\n\nd\\mathbf{d}d\n该样本的真实标签值向量\n\n\nW[l]\\mathbf{W}^{[l]}W[l]\n第 l−1l - 1l−1 层到第 lll 层的权重矩阵1. 其中每层的第 000 节点 W0[l]\\mathbf{W}^{[l]}_0W0[l]​ 表示偏置值2. 尺寸为 Rnl−1×nl\\mathbb{R}^{n_{l - 1} \\times n_l}Rnl−1​×nl​3. 每一个纵向表示一个节点的权重\n\n\nσ[l]\\sigma^{[l]}σ[l]\n第 l−1l - 1l−1 层到第 lll 层的激活函数\n\n\nz[l]\\mathbf{z}^{[l]}z[l]\n第 lll 层的加权和向量1. 尺寸为 Rnl×1\\mathbb{R}^{n_{l} \\times 1}Rnl​×1\n\n\na[l]\\mathbf{a}^{[l]}a[l]\n第 lll 层的激活值向量1. 用作下一层的输入2. a[l]=σ[l](z[l])\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})a[l]=σ[l](z[l])3. a0[l]=1\\mathbf{a}^{[l]}_0 = 1a0[l]​=1 表示第 lll 层的偏置节点(bias)输出层不包含偏置节点4. 尺寸为 Rnl×1\\mathbb{R}^{n_{l} \\times 1}Rnl​×1\n\n\nδ[l]\\delta^{[l]}δ[l]\n第 lll 层的误差项1. 点乘其输入得到梯度2. 尺寸为 Rnl×1\\mathbb{R}^{n_l \\times 1}Rnl​×1\n\n\nL\\mathcal{L}L\n损失函数\n\n\nη\\etaη\n学习率\n\n\n\n计算通常使用矩阵运算来高效地进行前向传播和反向传播：\n\n\n\n符号\n说明\n公式\n\n\n\n\nz[l+1]\\mathbf{z}^{[l + 1]}z[l+1]\n第 lll 层到第 l+1l + 1l+1 层的加权和\nz[l+1]={W[l+1]T⋅a[l]l≥1W[l+1]T⋅xl=0\\mathbf{z}^{[l + 1]} = \\begin{cases}\\mathbf{W}^{[l+1]^T} \\cdot \\mathbf{a}^{[l]} &amp; l \\ge 1\\\\\\mathbf{W}^{[l+1]^T} \\cdot \\mathbf{x} &amp; l = 0\\end{cases}z[l+1]={W[l+1]T⋅a[l]W[l+1]T⋅x​l≥1l=0​\n\n\na[l]\\mathbf{a}^{[l]}a[l]\n第 lll 层的激活值\na[l]=σ[l](z[l])\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})a[l]=σ[l](z[l])\n\n\nδ[l]\\delta^{[l]}δ[l]\n第 lll 层的误差项，用于反向传播\nδ[l]=∇z[t]L=∇a[l]L⊙σ[l]′(z[l])\\begin{aligned}\\delta^{[l]} &amp;= \\nabla_{z^{[t]}}\\mathcal{L}\\\\ &amp;= \\nabla_{\\mathbf{a}^{[l]}} \\mathcal{L} \\odot\\sigma^{[l]&#x27;}(\\mathbf{z}^{[l]})\\end{aligned}δ[l]​=∇z[t]​L=∇a[l]​L⊙σ[l]′(z[l])​\n\n\n∇a[l]L\\nabla_{\\mathbf{a}^{[l]}}\\mathcal{L}∇a[l]​L\n损失函数关于第 lll 层激活值的梯度1. 用于计算误差项2. 尺寸为 Rnl×1\\mathbb{R}^{ n_l \\times 1}Rnl​×1\n∇a[l]L={W[l+1]⋅δ[l+1]l&lt;L∇a[L]Ll=L\\nabla_{\\mathbf{a}^{[l]}}\\mathcal{L} = \\begin{cases}\\mathbf{W}^{[l+1]} \\cdot \\delta^{[l+1]} &amp; l &lt; L\\\\\\nabla_{\\mathbf{a}^{[L]}} \\mathcal{L} &amp; l = L\\end{cases}∇a[l]​L={W[l+1]⋅δ[l+1]∇a[L]​L​l&lt;Ll=L​\n\n\n∇W[l]L\\nabla_{\\mathbf{W}^{[l]}}\\mathcal{L}∇W[l]​L\n损失函数关于第 lll 层权重的梯度1. 用于更新权重2. 尺寸为 Rnl−1×nl\\mathbb{R}^{n_{l - 1} \\times n_l}Rnl−1​×nl​\n∇W[l]L={a[l−1]⋅δ[l]Tl&gt;1x⋅δ[1]Tl=1\\nabla_{\\mathbf{W}^{[l]}} \\mathcal{L} = \\begin{cases}\\mathbf{a}^{[l-1]} \\cdot \\delta^{[l]^T} &amp; l &gt; 1 \\\\  \\mathbf{x} \\cdot \\delta^{[1]^T} &amp; l = 1\\end{cases}∇W[l]​L={a[l−1]⋅δ[l]Tx⋅δ[1]T​l&gt;1l=1​\n\n\n\n 2. 多层感知机更新过程\n多层感知机的训练过程包括前向传播和反向传播两个阶段。\n前向传播计算最终输出和Loss，反向传播计算梯度并更新权重。\n假设有一个包含输入层、一个隐藏层和一个输出层的简单 MLP：\n\nl=0l=0l=0：输入层\nl=1l=1l=1：隐藏层\nl=2l=2l=2：输出层\n\n i. 前向传播\n前向传播的计算过程如下：\nz1=W1T⋅xa1=σ1(z1)z2=W2T⋅a1a2=σ2(z2)\\begin{aligned}\n\\mathbf{z}_1 &amp; = \\mathbf{W}^T_1 \\cdot \\mathbf{x} \\\\\n\\mathbf{a}_1 &amp; = \\sigma_1(\\mathbf{z}_1) \\\\\n\\mathbf{z}_2 &amp; = \\mathbf{W}^T_2 \\cdot \\mathbf{a}_1 \\\\\n\\mathbf{a}_2 &amp; = \\sigma_2(\\mathbf{z}_2)\n\\end{aligned}\nz1​a1​z2​a2​​=W1T​⋅x=σ1​(z1​)=W2T​⋅a1​=σ2​(z2​)​\n最终计算出损失函数 L(a2,d)\\mathcal{L}(\\mathbf{a}_2, \\mathbf{d})L(a2​,d)。\n ii. 反向传播\n反向传播的计算过程如下：\nδ2=∇a2L⊙σ2′(z2)(1)δ1=(W2⋅δ2)⊙σ1′(z1)(2)∇W2L=a1⋅δ2T(3)∇W1L=x⋅δ1T(4)\\begin{aligned}\n\\delta_2 &amp;= \\nabla_{\\mathbf{a}_2} \\mathcal{L} \\odot \\sigma_2&#x27;(\\mathbf{z}_2)&amp;(1) \\\\\n\\delta_1 &amp;= (\\mathbf{W}_2 \\cdot \\delta_2) \\odot \\sigma_1&#x27;(\\mathbf{z}_1) &amp;(2) \\\\\n\\nabla_{\\mathbf{W}_2} \\mathcal{L} &amp; = \\mathbf{a}_1 \\cdot \\delta_2^T &amp;(3) \\\\\n\\nabla_{\\mathbf{W}_1} \\mathcal{L} &amp; = \\mathbf{x} \\cdot \\delta_1^T &amp;(4)\n\\end{aligned}\nδ2​δ1​∇W2​​L∇W1​​L​=∇a2​​L⊙σ2′​(z2​)=(W2​⋅δ2​)⊙σ1′​(z1​)=a1​⋅δ2T​=x⋅δ1T​​(1)(2)(3)(4)​\n由(1), (3) 可得到输出层的梯度，这是因为：\n∇W2L=∂L∂a2⋅∂a2∂z2⋅∂z2∂W2=a1⋅δ2T\\nabla_{\\mathbf{W}_2} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_2} \\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial \\mathbf{z}_2} \\cdot \\frac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{W}_2} = \\mathbf{a}_1 \\cdot \\delta_2^T\n∇W2​​L=∂a2​∂L​⋅∂z2​∂a2​​⋅∂W2​∂z2​​=a1​⋅δ2T​\n由(2), (4) 可得到隐藏层的梯度，这是因为：\n∂L∂a1=∂L∂a2⋅∂a2∂z2⋅∂z2∂a1=W2⋅δ2∇W1L=∂L∂a1⋅∂a1∂z1⋅∂z1∂W1=x⋅δ1T\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_1} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_2} \\cdot \\frac{\\partial \\mathbf{a}_2}{\\partial \\mathbf{z}_2} \\cdot \\frac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{a}_1} = \\mathbf{W}_2 \\cdot \\delta_2 \\\\\n\\nabla_{\\mathbf{W}_1} \\mathcal{L} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_1} \\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1} \\cdot \\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{W}_1} = \\mathbf{x}\\cdot \\delta_1^T  \n\\end{aligned}\n∂a1​∂L​∇W1​​L​=∂a2​∂L​⋅∂z2​∂a2​​⋅∂a1​∂z2​​=W2​⋅δ2​=∂a1​∂L​⋅∂z1​∂a1​​⋅∂W1​∂z1​​=x⋅δ1T​​\n iii. 多最终输出的反向传播\n如果有多个输出节点，其中每个输出节点 jjj 都有自己的误差项 δ2j\\delta_{2j}δ2j​ 和梯度 ∇W2jL\\nabla_{\\mathbf{W}_{2j}} \\mathcal{L}∇W2j​​L，则可以将它们组合成矩阵形式进行计算。\n正向传播往往使用为每一个输出节点计算损失，然后将所有输出节点的损失进行平均或求和，得到总的损失函数 L\\mathcal{L}L。\n反向传播时，根据链式法则会使用简单求和的方式，将每个输出节点的 误差项 δ2j\\delta_{2j}δ2j​ 进行累加，得到总的误差项 δ2\\delta_2δ2​。\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/感知机","date":"2025-12-25T23:50:03.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Perceptron","author_index":"zExNocs"},{"id":"201cfb965ea501066396d98e06b42b76","title":"深度学习-二分类模型","content":"&lt;返回深度学习导航\n\n 一. 介绍\n二分类指的是只有 True 和 False 或 111 和 −1-1−1 的分类。二分类模型是将输入 xxx 映射为简单的 111 或 −1-1−1。\n其中 True 或 111 通常表示正类（Positive Class），而 False 或 −1-1−1 则表示负类（Negative Class）。\n 1. 输出类型\n输出可以是直接的离散标签 111 或 −1-1−1，也可以是一个概率值，表示输入属于某个类别的可能性。\n\n\n\n\n\n\n\n\n\n对于大部分指标，我们往往只关心排序而不是具体数值。\n i. 离散输出\n离散输出的模型通常包含：\n\nK-Nearest Neighbor\nDecision Tree\n\n ii. 线性输出\n线性输出是通过一个线性函数计算得到的实数值，通常需要通过一个阈值来决定最终的分类结果。\n其流程一般如下：\n\n\n\n\n\n\n\n\n\nThreshold → Classifier → Metrics\n阈值 → 分类器 → 指标\n\n\n\n输出类型\n说明\n判别\n例子\n\n\n\n\nMargin\n任意实数\n一般是大于阈值则输出正，否则输出负\nSVM\n\n\n概率Probability\n输出一个范围为 [0,1][0, 1][0,1] 的真实数\n一般越接近 111 表示越为正类\nLR神经网络 NN\n\n\n\n 二. 二分类指标\n\n\n\n结果类型\n成功与否\n预测\n实际\n说明\n\n\n\n\n真阳性True PositivesTP\\text{TP}TP\n✔️\n真\n真\n\n\n\n真阴性True NegativesTN\\text{TN}TN\n✔️\n假\n假\n\n\n\n假阳性False PositivesFP\\text{FP}FP\n✖️\n真\n假\n第一类错误Type I error\n\n\n假阴性False NegativesFN\\text{FN}FN\n✖️\n假\n真\n第二类错误Type II error\n\n\n\n\n\n\n指标类型\n说明\n\n\n\n\n分类错误率/准确率Classification error rate/Accuray\n1. 被错误分类的实例占所有实例的比例2. Accuray=TP+TNTP+TN+FP+FN\\text{Accuray} = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}Accuray=TP+TN+FP+FNTP+TN​3. 训练集的分类错误率一般非常乐观\n\n\n精确率Precision\n1. Precision=TPTP+FP\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}Precision=TP+FPTP​2. 预测的正样本中，有多少实际是对的\n\n\n正召回率/敏感度Positive Recall/Sensitivity\n1. Recall=TPTP+FN\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}Recall=TP+FNTP​2. 所有实际正样本中，有多少是预测对的\n\n\n负召回率/特异性Negative Recall/Specificity\n1. Recall=TNTN+FP\\text{Recall} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}Recall=TN+FPTN​2. 所有实际负样本中，有多少是预测对的\n\n\nF-measure\n1. 将召回率和精确率结合成一个单一指标2. fβ=(1+β2)PR(β2P)+Rf_{\\beta} = (1 + \\beta^2)\\frac{PR}{(\\beta^2 P) + R}fβ​=(1+β2)(β2P)+RPR​3. PPP 是精确率，RRR 是召回率4. β\\betaβ 是非负实数值，一般 β=1\\beta = 1β=15. 即 f1=2PRP+Rf_1 = \\frac{2PR}{P + R}f1​=P+R2PR​\n\n\nROC 曲线RoC curve\n1. Receiver Operating Characteristic 用于评价二分类2. 横轴：FPR=FPFP+TN\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}FPR=FP+TNFP​，实际为假的猜错的比例3. 纵轴：Recall/TPR=TPTP+FN\\text{Recall/TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}Recall/TPR=TP+FNTP​4. 可以通过改变分类器的决策阈值来实现，即判断为正的概率阈值5. 曲线下面积（AUROC）常被用来衡量模型好坏，表示取随机一个正样本与负样本，模型有多少概率把正样本排在前面AUROC = 1：完美分类器AUROC = 0.5：随机乱猜AUROC &lt; 0.5：比随机更差6. 不受类别不平衡的影响；只关注分类器的排序能力，而非具体阈值；易于比较不同模型整体性能\n\n\n混淆矩阵Confusion matrix\n1. 用于表示模型的预测类别和真实类别之间的对应关系，可用于多分类2. 如果把真实 AAA 错预测为 BBB，则在矩阵 (A,B)(A, B)(A,B) 处 +1+1+1\n\n\n\n\n&lt;返回深度学习导航\n","slug":"笔记/深度学习/二分类模型","date":"2025-12-25T23:24:03.000Z","categories_index":"笔记-深度学习","tags_index":"Machine Learning,Deep Learning,Binary Classification","author_index":"zExNocs"},{"id":"3a4df68478d92cfa87fd38d380f924de","title":"深度学习-导航","content":" 一. 文章导航\n 1. 基础概念\n\n\n\n笔记\n说明\n\n\n\n\n基本概念和术语\n本文章，阅读其他文章前务必看完该章节\n\n\n优化器\n1. 模型优化训练的原理和常见优化器介绍，例如 SGD、Adam 等2. 优化器用于使用偏导数更新神经网络的权重参数，以最小化损失函数3. 包含优化器介绍、损失函数介绍\n\n\n学习率\n学习率的作用和调整方法\n\n\n激活函数\n激活函数的种类和作用，包含如何求带激活函数的梯度\n\n\n正则化\n正则化技术介绍，防止过拟合\n\n\n\n\n\n\n\n 2. 神经网络\n\n\n\n笔记\n说明\n\n\n\n\n二分类模型\n二分类模型的介绍与指标说明\n\n\n感知机\n感知机和Adaline模型的介绍，最简单的线性分类器和线性回归模型\n\n\nCNN\n卷积神经网络的结构和应用介绍\n\n\nRNN\n循环神经网络的结构和应用介绍\n\n\nLSTM\n长短期记忆网络的结构和应用介绍\n\n\nGNN\n图神经网络的结构和应用介绍\n\n\n\n 二. 深度学习介绍\n传统机器学习依赖于人为设计特征，模型来学习这些特征与目标变量之间的映射关系。例如 SVM、LR、决策树等。\n与标准机器学习不同，表征学习(Representation learning)旨在自动学习特征和规则。\n深度学习算法从原始输入 xxx（例如声音、像素、字符或单词）中学习多个层次的表征（此处指隐藏层）和一个输出层。神经网络(Neural Networks, NN)是目前最成功的深度学习方法，也称为可微分编程 (Differentiable Programming)。\n 1. 为什么选择深度学习\n\n直接学习底层特征表示，而不是依赖于手工设计的特征\n可扩展机器学习\n更复杂的模型学习更多有效特征(表征)\n\n 2. 深度学习兴起的原因\n\n大数据时代导致海量数据的产生\n强大的硬件并行计算能力（GPU、TPU 等）\n强大的软件生态系统和框架（TensorFlow、PyTorch 等）\n\n 三. 术语\n有关深度学习的基本术语说明。\n 1. 基本术语\n\n\n\n术语\n说明\n\n\n\n\n权重Weights\n从训练集自动学习的到的参数\n\n\n特征Features\n1. 输入数据的属性2. 例如：图像的像素值、文本的词向量等\n\n\n超参数Hyperparameters\n1. 训练前人为指定的参数2. 例如：网络层数、激活函数、学习率、Epoch等\n\n\n损失函数Loss\n模型就是通过最小化损失函数来学习的1. 衡量模型预测与真实标签之间差异的函数2. 没有绝对比较，只有相对比较3. 常见的有均方误差（MSE）、交叉熵损失等\n\n\n激活函数Activation Function\n1. 非线性函数，决定神经元的输出2. 常见的有ReLU、Sigmoid、Tanh等\n\n\n优化器Optimizer\n1. 用于更新神经网络权重的算法2. 常见的有SGD、Adam、RMSprop等\n\n\n正则化Regularization\n1. 防止过拟合的技术，增加对模型复杂度的惩罚2. 常见的有L1、L2正则化和Dropout等\n\n\n批归一化Batch Normalization\n1. 一种加速训练和提高稳定性的技术2. 通过标准化每个小批量的数据来减少内部协变量偏移\n\n\n残差连接Residual Connection\n1. 一种跳跃连接技术2. 允许梯度直接传递，缓解深层网络的梯度消失问题\n\n\n注意力机制Attention Mechanism\n1. 允许模型在处理输入时关注不同部分的技术2. 广泛应用于自然语言处理和计算机视觉中\n\n\n\n 2. 训练术语\n\n\n\n术语\n说明\n\n\n\n\n前向传播Forward Propagation\n1. 输入数据通过网络层层传递，计算输出的过程2. 计算损失值\n\n\n反向传播Backpropagation\n1. 计算损失函数相对于每个权重的梯度2. 用于更新权重以最小化损失\n\n\n迭代步Iteration\n一次向前 + 向后传播的单位数Iterations per Epoch=Training SizeBatch Size\\text{Iterations per Epoch} = \\frac{\\text{Training Size}}{\\text{Batch Size}}Iterations per Epoch=Batch SizeTraining Size​\n\n\n轮Epoch\n用整个训练数据集完整训练一次模型的单元数\n\n\n批次Batch\n迭代一次使用的样本数量\n\n\n收敛Converge\n算法运行到某个时间点后不再发生实质性变化，输出稳定下来或者说找到一个权重向量，能够正确分类所有训练样本\n\n\n学习率Learning Rate\n学习率是每个迭代次数权重更新的步长1. 越大，越容易发散、震荡2. 越小，容易收敛但可能停在坏点或训练很慢\n\n\n\n 3. 模型结构术语\n\n\n\n术语\n说明\n\n\n\n\n层Layer\n1. 神经网络的基本组成部分2. 包含多个神经元，负责特定的计算任务\n\n\n神经元Neuron\n1. 神经网络中的基本计算单元2. 接收输入，进行加权求和并通过激活函数产生输出\n\n\nDropout\n1. 一种正则化技术2. 在训练过程中随机丢弃一部分神经元，防止过拟合\n\n\n\n 4. 模型成品术语\n\n\n\n术语\n说明\n\n\n\n\n过拟合Overfitting\n1. 模型在训练数据上表现很好，但在测试数据上表现差2. 说明模型学到了训练数据中的噪声\n\n\n欠拟合Underfitting\n1. 模型在训练数据和测试数据上都表现不好2. 说明模型过于简单，无法捕捉数据的复杂模式\n\n\n泛化能力Generalization\n1. 模型在未见过的数据上表现良好的能力2. 衡量模型是否能有效应用于实际问题\n\n\n\n 四. 杂项\n 1. 定义/训练机器学习流程\n机器学习需要任务 Task TTT，性能评估 Performance measure PPP 和 训练样例 Training experience EEE。\n\n\n\n步骤\n说明\n\n\n\n\n收集训练样例Collect Training Experience\n例如调查、计算机生成\n\n\n表示样例Representing Experience\n即对样例进行编码，样例 K=(x,d)K = (\\mathbf{x}, \\mathbf{d})K=(x,d)1. x\\mathbf{x}x 为特征向量(Feature Vector) 由多维的向量表示2. d\\mathbf{d}d 为对应向量(Corresponding Vector)，用来记录特征向量 x\\mathbf{x}x 的结果/分类4. 因此模型是一个 x→d\\mathbf{x} \\to \\mathbf{d}x→d 的映射/函数\n\n\n选择函数\n选择函数 F(w,x)F(\\mathbf{w}, \\mathbf{x})F(w,x) 来对 x\\mathbf{x}x 分类成 d\\mathbf{d}d1. w\\mathbf{w}w 表示可以调整的参数/权重(Weights)2. w\\mathbf{w}w 就是学习算法进行学习的东西\n\n\n选择错误指标\n选择一个错误指标 L(d,F(w,x))\\mathcal{L}(\\mathbf{d}, F(\\mathbf{w}, \\mathbf{x}))L(d,F(w,x)) 来衡量预测结果与真实结果的差异1. 例如均方误差(MSE)、交叉熵损失等\n\n\n选择其他\n1. 选择激活函数2. 选择如何初始化权重3. 选择优化器\n\n\n学习权重\n选择一个学习算法来调整权重 w\\mathbf{w}w 并预测\n\n\n调整权重\n计算并利用错误指标 L\\mathcal{L}L 来进行调整权重1. 简单的错误计算 L=∣d−F(w,x)∣\\mathcal{L} = |\\mathbf{d} - F(\\mathbf{w}, \\mathbf{x})|L=∣d−F(w,x)∣\n\n\n使用/测试系统\n1. 一旦学习完成，所有的权重就是确定的了2. 给出任意未知 x\\mathbf{x}x 该系统都可以计算出一个答案 F(w,x)F(\\mathbf{w}, \\mathbf{x})F(w,x)\n\n\n\n例如在 8×88 \\times 88×8 画布识别 [0,9][0, 9][0,9] 数字系统中，XXX 表示一个 64-d 的二进制向量用来描述画布的像素情况，而 DDD 表示 10-d 的二进制向量用来表示是哪个数字。如果为数字 2, 则只有第三维向量为 1，其他为 0 (第一维从 0 开始)。\n 2. 特征空间 （Feature Space）\n特征空间是一个由特征组成的数学空间，数据集中每一个样本都可以表示为该空间的一个点或向量。该过程是 从对象(Objects) 到 特征向量(Feature Vectors, XXX) 到 特征空间的点。\n对于 nnn 个特征的特征空间是一个 nnn 维的超空间。\n 3. 学习系统种类\n学习系统一般分为 分类(Classification) 和 回归(Regression)\n\n\n\n\n分类\n回归\n\n\n\n\n预测\n标签Label\n回应Response\n\n\n输出\n离散的值Discrete\n连续的值Continuous\n\n\n例子\n预测一个图片是猫还是狗\n预测明天的股票\n\n\n\n分类学习一般有 监督学习(Supervised Learning) 和 非监督学习 (Unsupervised Learning)。\n\n\n\n\n监督学习Supervised Learning\n非监督学习Unsupervised Learning\n弱监督学习Weakly supervised learning\n\n\n\n\n数据\n输入-输出对包含标签的\n未包含标签的\n半包含标签的\n\n\n学习\n将输入映射到输出的函数\n数据集中元素之间的关系无需帮助即可分类\n\n\n\n应用\n图像识别\n数据聚类Data Clustering\n\n\n\n\n 4. 过拟合与拟合不足\n\n\n\n情况\n发生情况\n解决方法\n\n\n\n\n过拟合Overfitting\n1. 学习时间过长2. 训练样本无法代表所有可能的情况 (通常)3. 参数调整到与真实目标函数没有因果关系的无信息特征(Uninformative Features)上\n1. 增加训练集的大小2. 交叉验证(Cross Validation)\n\n\n拟合不足Underfitting\n1. 模型过于简单，无法捕捉数据的真实模式2. 训练时间不足，尚未完成充分学习3. 正则化过强，限制了模型对数据的拟合能力\n1. 使用更复杂的模型（增加模型容量，如更多层或更多参数）2. 延长训练时间3. 减小正则化强度（降低 L1/L2、减少 dropout）4. 调整模型超参数（如学习率、树深度等）\n\n\n\n 5. 交叉验证\n交叉验证(Cross Validation) 是一种减少过拟合的方法。\n交叉验证将收集到的数据分为训练集、验证集、测试集。\n\n\n\n数据集\n参与训练\n作用\n用途\n重复\n\n\n\n\n训练集Training Set\n✔️\n拟合权重参数 www\n让模型学会如何做任务\n✔️\n\n\n验证集Validation Set\n✖️\n评估模型在未见过的数据上的表现\n1. 超参数调节2. 模型比较和选择3. 防止过拟合选择最好的 Epoch\n✔️\n\n\n测试集Test Set\n✖️\n完全独立的数据在任何训练过程中不会出现\n1. 模型最终性能报告和评估2. 对模型泛化性做出最终客观评价\n✖️\n\n\n\n其步骤如下：\n\n\n\n步骤\n说明\n\n\n\n\n1.1.1. 划分\n将数据集划分为 SSS 个组，包括 (S−2)(S-2)(S−2) 个训练集、一个验证集和一个测试集\n\n\n2.2.2. 重复\n然后对所有 S−1S - 1S−1 个可能的选择重复此过程 (测试集固定，选取不同的验证集)\n\n\n3.3.3. 平均\n将 S−1S - 1S−1 次运行的性能得分取平均值\n\n\n\n 6. 非线性特征变换\n深度学习的核心思想是通过多层非线性变换来学习数据的复杂表示，从而捕捉数据中的复杂模式和关系。\n简单说，就是通过非线性变化将数据不断地往上层映射到高维空间，使得在高维空间中数据变得线性可分，从而可以使用简单的线性模型进行分类或回归。\n i. 单层线性模型\n假设当前模型是线性的：\nf=wx\\mathbf{f} = \\mathbf{wx}\nf=wx\n\n\n\n参数\n说明\n\n\n\n\nx∈RD\\mathbf{x} \\in \\mathbb{R}^Dx∈RD\n输入向量\n\n\nw∈RC×D\\mathbf{w} \\in \\mathbb{R}^{C \\times D}w∈RC×D\n参数矩阵\n\n\nf\\mathbf{f}f\n对输入的线性变化\n\n\n\n线性模型本质上就是找到一个超平面将数据分开。线性模型的限制是表示能力弱，不能拟合复杂关系，例如线性不可分的关系。\n通过非线性特征变换，让数据变得线性可分\n ii. 两层神经网络\n两层神经网络(2-layer network)：\nh=w1xw1∈RH×Dh′=ReLU(h)=max⁡(0,h)f=w2h′w2∈RC×H\\begin{aligned}\n\\mathbf{h} &amp;= \\mathbf{w}_1\\mathbf{x} &amp; \\mathbf{w}_1 \\in \\mathbb{R}^{H \\times D}\\\\\n\\mathbf{h}&#x27; &amp;= \\text{ReLU}(\\mathbf{h}) =\\max(0, \\mathbf{h})\\\\\n\\mathbf{f} &amp;= \\mathbf{w}_2\\mathbf{h}&#x27; &amp; \\mathbf{w}_2 \\in \\mathbb{R}^{C \\times H}\n\\end{aligned}\nhh′f​=w1​x=ReLU(h)=max(0,h)=w2​h′​w1​∈RH×Dw2​∈RC×H​\n第一层加非线性激活(ReLU)(矩阵逐元素进行转变)，再让第二层对第一层进行线性变换。\n实际训练中会加上 bias：\nf=w2max⁡(0,w1x+b1)+b2\\mathbf{f} = \\mathbf{w}_2\\max(0, \\mathbf{w}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2\nf=w2​max(0,w1​x+b1​)+b2​\n iii. 多层神经网络\n多层神经网络(Multi-layer network) 通过堆叠多个隐藏层来增加模型的表达能力。\n\n\n\n自定义参数\n说明\n\n\n\n\n隐藏层数Depth\n网络中隐藏层的数量\n\n\n每层神经元数Width\n每个隐藏层中神经元的数量\n\n\n激活函数Activation Function\n每个隐藏层使用的非线性函数\n\n\n目标函数Objective Function\n用于训练的损失函数\n\n\n正则化方法Regularization Methods\n防止过拟合的技术，如 Dropout、L2 正则化等\n\n\n\n使用多少层神经网络:\n\n\n\n类型\n说明\n\n\n\n\n理论\n1. 只要有 1 个隐藏层，神经网络就是一个“通用函数逼近器”2. 对于任意连续函数 g(x)g(x)g(x) 都存在只有 1 个隐藏层的神经网络 f(x)f(x)f(x) 使得 ∣f(x)−g(x)∣&lt;ϵ\\mid f(x) - g(x)\\mid &lt; \\epsilon∣f(x)−g(x)∣&lt;ϵ3. 前提使用 Sigmoid 激活函数\n\n\n经验\n1. 在很多问题上，深度网络反而比浅层网络更容易训练、效果更好2. 必须知道并使用正确的训练技巧\n\n\n\n多个较窄的隐藏层通常优于单个较宽的隐藏层。\n","slug":"笔记/深度学习/深度学习导航","date":"2025-12-25T10:00:01.000Z","categories_index":"导航","tags_index":"Machine Learning,Deep Learning","author_index":"zExNocs"},{"id":"dc615baa143d37d131ed30ed421863b0","title":"优化算法导航","content":" 一. 导航\n 二.\n","slug":"笔记/算法和数据结构/算法/优化算法导航","date":"2025-12-13T01:00:25.000Z","categories_index":"导航","tags_index":"Algorithm,Optimization Algorithm,Heuristic,Metaheuristic,Hyper-heuristic","author_index":"zExNocs"},{"id":"d7933c3a12e262bd3d27948191e50918","title":"问题导航","content":" 一. 导航\n\n\n\n导航\n说明\n\n\n\n\n规划问题\n确定需要执行哪些动作（计划）以使某些条件（目标）为真\n\n\n可满足性问题 SAT\n找到满足该命题公式的解释或证明该命题不可满足\n\n\n约束满足问题 CSP\n是SAT问题的扩展，为满足所有约束对每个变量在定义域中寻找一个值\n\n\n图染色问题\n所有的顶点都拥有一个颜色，并且没有两个相邻的顶点颜色相同\n\n\n\n","slug":"笔记/问题/问题导航","date":"2025-05-24T09:00:00.000Z","categories_index":"导航","tags_index":"Problem","author_index":"zExNocs"},{"id":"75f1311d03df0221199bcef8867d80d0","title":"计算机安全","content":" 介绍\n 作用 ++\n\n预防 Prevention：防止资产被访问和损坏\n检测 Detection：检测资产被访问或损坏的步骤\n恢复 Recovery：使我们能够从资产(asset)损坏中恢复的措施\n\n资产可以是实物，也可以仅仅是信息\n\n\n\n 定义 (CIA) ++\n\n机密性 Confidentiality：防止未经授权的信息披露 prevention of unauthorised disclosure of information\n\n防止未经授权的用户读取私人或秘密信息\n\n\n完整性 Integrity：防止未经授权的信息修改 prevention of unauthorised modification of information\n\n真实性 Authenticity = Integrity + Freshness (时效性)\n\n\n可用性 Availability：防止未经授权的信息或资源被扣留 prevention of unauthorised witholding of information or resources\n\n确保授权用户在需要时能访问资源，防止拒绝服务（DoS）\n问责 Accountability：用户应对其行为负责\n\n系统应识别和验证用户，并确保合规\n必须保留审计跟踪\n大学制定了详细的审计跟踪政策！\n\n\n不可否认性 Non-repudiation\n\n提供不可伪造的证据，证明某人做了某事\n主要为法律概念\n可由受信任的第三方验证的证据\n\n例如：公证人、数字证书\n\n\n适用于物理安全：例如：钥匙卡\n\n\n\n\n\n安全可以被视为控制信息访问。\n\nData 数据：一种表示信息的方式 (means)\nInformation 信息：对数据的解读 (interpretation)\n\n 根本矛盾\n\nSecurity vs. usability (~ availability)\nAccountability vs. privacy (e.g. audit trails)\nAvailability vs. authentication (e.g. account lockout)\nOverhead (computation, communication, storage)\n\n 安全系统设计\n\n系统化方法：安全要从需求阶段就纳入设计，事后补安全往往难以奏效。\n风险与成本评估：通过威胁建模和成本—效益分析决定防护深度。\n\n 策略\n\nFocus of control 控制重点\n\n明确针对谁进行保护。\n例如：\n\nData：例如一致性检查\nOperations：允许的调用\nUsers: 特定用户的权限\n\n\n\n\nComplexity vs. Assurance 复杂性与保障性\n\n简单机制易验证、高可信，但功能有限；复杂系统功能丰富，却难以全面评审。\n功能丰富的安全系统和高可靠性并不容易兼顾\n例如，Linux 权限 vs. Windows 权限\n\n\nCentralised or Decentralised Controls 集中式或分散式控制\n\n安全定义和执行应该由中央实体执行，还是应该留给系统中的各个组件？\n中央实体——潜在的瓶颈\n分布式解决方案——效率更高，但管理难度更大\n\n\nLayered Security 分层安全\n\n每一层都保护着一个边界\n假设其下层是安全的\n层级一般是：应用 → 服务 → 操作系统 → 内核 → 硬件\n若内核或硬件遭破坏，上层安全机制即失效（如提权漏洞、硬盘偷取、恶意固件）。\n\n\n\n 密码学 Cryptography\n密码学（Cryptography）是研究如何在不安全信道中，利用数学算法和协议保障信息的机密性、完整性和不可抵赖性的学科，涵盖对称加密、非对称加密及相关协议设计等内容 。\n\nCryptology 密码学\nCryptography 密码学\n\nSymmetric 对称性密码：加密和解密算法都使用相同的密钥。\n\n加密技术保障信息保密性\n对称加密 ≈ “挂锁”\n用于一般通信\n轻量级且快速\n密码管理困难\n使用分组密码或流密码实现\n\n流密码(Stream ciphers)：使用初始种子密钥来生成无限的随机比特密钥流。\n\n消息和密钥流通常使用 XOR 进行组合，如果应用两次，则可以逆向。\n优点：\n\n加密长连续流，可能长度未知\n速度极快，占用内存少，非常适合低功耗设备\n如果设计得当，可以定位到流中的任意位置\n\n\n缺点：\n\n密钥流必须具有统计随机性\n无法保护密文 ciphertext\n\n\n\n\n分组密码(Block Ciphers)：使用密钥将固定大小的明文块加密为固定大小的密文块\n\n根据密钥更改和排列分组的位\n可以通过拆分消息和填充来处理不同长度的消息\n可以使用混合密钥来防止攻击者逆转该过程\n\n\n\n\n\n\nAsymmetric 非对称密码：使用一对密钥，一个公钥，一个私钥。\n\n使用一对密钥——公钥（公开）和私钥（保密），一方用公钥加密，只有对应私钥能解密；或用私钥签名，任何持公钥者可验证签名。\n简化了密钥分发，用户只需保管私钥；常用于建立安全通道、数字签名和证书体系，但运算开销较大。\n非对称加密 ≈ “邮箱”\n公钥（非对称）密码学基于以下前提：\n\n通过公钥计算私钥在计算上是不可行的\n在实践中，这是通过简化为难以解决的（NP）数学问题来实现的\n\n\n\n\nProtocols 协议\n\nTransport Layer Security (TLS) 传输层安全性\n\nTLS 是一种提供端到端加密的握手和记录协议。它用于 HTTPS\n具有 机密性、完整性、服务器真实性\n\n\n\n\n\n\nCryptanalysis 密码分析学\n\n 验证 Authentication\n允许用户接入资产，必须保证：\n\n他们有权限接入该资产\n他们的身份与其声明相符\n\n我们可以尝试使用凭证(credentials)来验证身份\n\nSomething the user is\nSomething the user has\nSomething the user knows\n\n\n\n\n\n 凭证\n\n定义\n\n身份识别（Identification）：用户通过用户名或其他标识表明“我是谁”。\n身份验证（Authentication）：使用凭证（“知道的”如密码、“拥有的”如令牌、“生物的”如指纹）确认用户确实是其所声明的身份。\n从检查时间到使用时间：\n\n重复身份验证\n会话开始时和会话期间\n\n\n\n\n类型\n\n知识因素 (Knowledge Factors)\n\n例如：\n\nUsernames and Passwords\n\n\n问题：\n\n易忘：用户常常忘记复杂密码，被迫写下或重复使用。\n可猜测：弱密码（如“123456”、“qwerty”）极易被暴力或字典攻击破解。\n社会工程与钓鱼：攻击者通过伪装网站或邮件窃取凭证。\n凭证泄露：密码数据库被盗、管理员可读明文或加密存储方式不当均会导致大规模泄露。\n键盘记录（Keylogging）：恶意软件记录输入的字符，直接获取密码。\n\n\n密码管理器 (Password Managers)\n\n优点：\n\n只需记住一个主密码，便可自动生成并填充高强度密码；\n集中管理、提高密码熵和使用便利性。\n\n\n缺点：\n\n主密码成为单点故障，一旦泄露或忘记，所有账户受影响；\n管理器本身或其云端存储可能遭受攻击或漏洞；\n需确保客户端设备无恶意软件，防止自动填充被劫持。\n\n\n\n\n\n\nInherent / Biometrics 生物识别技术\n\n定义：\n\n生物特征识别是指个体独有的、可识别(unique, identifiable)的生物特征\n个体固有的\n\n\n例子：\n\nFingerprints\nFacial recognition\nIris recognition\nVoice recognition\nBehavioral biometrics\n\nDynamics (mouse, keystroke, etc.)\nGait, posture\nUsage patterns\n\n\n\n\n优点：\n\n无需担心丢失访问权限，不会忘记、丢失、放错、被盗等。\n非常方便（当它运行良好时）\n永远无法改变！\n\n\n缺点：\n\n传感器的可用性？（成本、合规性等）\n误差 Errors: false positives (FAR), false negatives (FRR)\n并非万无一失，例如对抗性机器学习 (adversarial ML)（但在实践中很难攻击）\n道德/隐私问题\n\n\n\n\nPossession Factors 占有因素\n例子：\n\n钥匙\n硬件\n\nOne-Time Passward (OTP)\n\n传统的：\n\n用户输入的基于时间/计数器的一次性密码 (OTP)\n临时代码（通常为 6-8 位数字），例如每 30 秒更改一次\n需要人工干预\n\n\n\n\n“安全密钥”，例如FIDO/YubiKey\n\n\n智能卡\n智能机/可穿戴的\n\n通常存在 single point of failure\n\n\n\n\n问题：\n\n物理丢失、损坏（可用性）或盗窃（定向攻击）\nUsability 可用性（有时良好，例如智能卡，有时不佳，例如一次性密码）\n成本\n\n\nMulti-Factor Authentication 多重验证\n\n\n\n 哈希函数\n\n定义：接收任意长度的消息，并返回固定长度的伪随机哈希值，广泛用于完整性校验、消息认证和密码存储。\n\nh(M):{0,1}n→{0,1}128h(M): \\{0, 1\\}^n \\to \\{0,1\\}^{128}\n\n\n特性：\n\n不可逆：从哈希值无法还原原文；\n抗碰撞：难以找到不同输入产生相同哈希值。\n常见算法：MD5、SHA-1、SHA-256 等，但应优先选择抗碰撞、抗预映像攻击性能更强的新算法。\n\n\n\n密码加盐 (salting)\n\n我们可以通过在哈希运算之前在密码前添加随机“盐”来提高安全性。\n加盐就是在用户的明文密码和哈希函数之间，加入一段随机数据（称为“盐”，salt），然后对组合后的结果进行哈希运算\n盐值以未加密的形式与哈希值一起存储\n如果不加盐，多个用户若使用相同密码，其哈希值完全一致，攻击者得知一个账户密码后，就可以轻易推断出其他账户也使用了相同密码。\n加盐对破解单个密码的速度没有影响\n\n\n\n 破解密码 Cracking Passwords\n\n密码破解分为两种基本类型：\n\n离线：您在本地拥有密码哈希值的副本\n\n离线密码破解很简单，就是尝试可能的密码，看看是否与密码列表发生哈希碰撞\n可能是暴力破解，但是时间复杂度很高\n\n\n在线：您没有哈希值，而是试图访问实际的登录终端\n\n在线破解通常通过网络钓鱼进行\n\n\n\n\n破解类型\n\n字典攻击:\n\n现在大多数密码破解都是通过字典攻击实现的\n使用常用单词和密码字典，对此列表进行细微修改，并尝试所有修改\n组合来自两个不同列表的单词\n\n\n伪装 Pretexting：\n\n以某些“借口”获取个人信息\n\n\n\n\n\n 参考监视器 Reference Monitors\n 定义\n\n定义\n\n参考监视器是一种抽象机制，用于在主体（subjects）对客体（objects）的所有访问之间进行强制执行的访问控制中介。\n必须满足：\n\n防篡改 tamper proof / resistant\n需要访问对象时必须始终调用(invoked)\n足够小以便可验证/可分析(verifiable / subject)，以确保正确性\n\n\n\n\n放置位置\n\n可放置在系统内的任何位置，但是越低越好\n硬件 – 用于定义权限的专用寄存器\n操作系统内核 – 例如虚拟机管理程序\n操作系统 – Windows 安全参考监视器\n服务层 – JVM、.NET\n应用层 – 防火墙\n\n\n意味着：\n\n可以确保更高的安全性\n实现简单的结构\n降低性能开销\n减少底层攻击的可能性\n然而，访问控制决策与应用程序相距甚远\n\n\n\n 操作系统完整性 OS Integrity\n操作系统既是访问控制的仲裁者(Arbitrates access requests)，也是受保护的资源。\n这本身是一种冲突，我们希望使用操作系统但又不希望破坏它。\n要保证完整性，需要：\n\n操作模式 Modes of Operation\n\n定义在哪种模式下允许哪些操作，例如系统调用、机器指令、I/O\nCPU 内的状态标志允许操作系统以不同的模式运行\n区分不同人员执行的计算（操作系统、用户）\n\n\n受控调用 Controlled Invocation\n\n允许我们在返回用户代码之前安全地执行特权指令\n许多函数在内核层执行，但也可以合理地在用户层代码中调用\n\n网络和文件 IO\n内存分配\n特权指令（例如 HLT）\n\n\n我们需要一种在内核模式（Ring 0）和用户模式（Ring 3）之间切换的机制\n中断 (Interrupts)：\n\n由中断处理程序处理，解决问题并返回到原始代码\n接到中断后，CPU 将切换到中断描述符表中指定的位置执行\n\n\n描述符和选择器 Descriptors and Selectors\n\n描述符保存关键系统对象的信息，例如内核结构位置\n描述符保存在描述符表中\n\n包含描述符特权级别 (DPL)\n\n\n描述符由选择器索引\n\n在需要时加载，例如在跳转调用时\n\n\nCPU 通过在加载选择器时检查当前特权级别 (CPL) 来保护内核\n\nx86 CPU 中的代码段 (CS) 寄存器保留了 2 位用于存储当前特权级别 (CPL)。\n特权级别高于其指向的描述符被称为门。\n由于这些描述符由内核创建，因此它们提供了一种安全的进入 Ring 0 的方法。\n\n\n\n\n\n\n\n 内存保护\n\n进程之间进行隔离。使用操作系统对进程之间进行联系。\n内存分段 – 将数据划分为逻辑单元\n\n有利于安全性\n内存管理挑战\n在现代操作系统中不常用\n\n\n内存分页 – 将内存划分为大小相等的页面\n\n所有进程都拥有独立的线性地址空间\n页表将线性地址空间映射到物理地址空间\n高效的内存管理\n访问控制效果较差\n在现代操作系统中极为常见\n\n\nMeltdown\n\n利用 CPU 推测执行 和 缓存侧信道 漏洞读取本应只在内核态或不同进程中可见的数据\n在大多数操作系统中，整个内核都存储在上层地址空间中。此区域中的页面被标记为主管，并且无法在 Ring 0 之外访问。\nMeltdown 漏洞允许我们读取此特权内存。触发访问违规后续缓存残留，侧向泄露内核内存内容。\n利用侧信道side-channel来实现这一点\n从内核读取\n屏蔽单个位\n访问该位置的用户内存\n\n\nSpectre\n\n与 Meltdown 非常相似\n诱导分支预测越界访问，同样借助缓存时间差获取敏感信息\n推测性评估以绕过应用程序边界检查\n屏蔽单个位\n访问该位置的用户内存\n\n\n\n 操作系统\n\n职责\n\nIdentification\nAuthentication\n\n身份验证让我们能够向系统验证我们的身份\n一些文件是私有的，一些是公开的\n系统文件应该受到保护\n我们需要能够访问某些应用程序\n\n\nAccess control\n\n我们需要一种机制来强制执行访问控制\n\n\nAuditing\n\n\n特性\n\n主体/委托人 Subject / Principal – 活跃实体\n\nPrincipal\n\n可被授予对象访问权限或可发出影响访问控制决策的语句的实体。\n例如，操作系统中的用户身份\n用于讨论安全策略 security policies\n\n\nSubject\n\nIT 系统中的活动实体\n以用户身份运行的进程\n用于讨论执行策略的操作系统 operational systems enforcing policies\n\n\n\n\n客体 Object – 被访问的资源\n\n两种控制方式：\n\n允许主体执行的操作\n可以对对象执行的操作\n\n\n执行操作：\n\n读：Simply viewing (Confidentiality)\n写：包括更改、附加和删除 (Integrity)\n执行： 可以在不知道文件内容的情况下运行文件\n\n\n\n\n访问操作 Access operation\n引用监视器 Reference monitor – 授予或拒绝访问\n\n\n所有权 Ownership\n\n谁负责制定安全策略？\nDiscretionary 自主决定: 可以为每个资源定义所有者\n所有者控制谁有权访问\n强制决定：可以制定系统级策略\n\n\n\n UNIX and Linux\n\nUnix 通过仅考虑所有者、组和其他用户来简化访问控制\n\n用户是当前所有者\n组是一个命名的组实体\n\n可以将具有相似访问权限的用户分组\n为组授予访问对象的权限\n\n\n其他所有人\n\n\nUnix 提供读取、写入和执行访问控制\nUnix / Linux 中的用户名是软别名，您的 UID 决定了权限\n\n用户身份：UID\n组身份：GID\nID 存储在 /etc/passwd 中\nRoot 拥有一个特殊的 UID：0\n\n\n\n Windows\n\n\nWindows 自 NT 时代起，安全子系统分为用户态（User Mode）与内核态（Kernel Mode）两部分\n\n用户态（User Mode）\n\n环境子系统（Environment Subsystems）：提供不同 API 环境（如 Win32、POSIX 等）的兼容层。\n安全子系统（Security Subsystem）：运行在用户态，负责登录进程（Winlogon、LogonUI）、本地安全机构（LSA）、安全账户管理器（SAM）等。\n参考监视器（Reference Monitor）：以 LSA 为核心，通过访问令牌（Access Token）和安全描述符（Security Descriptors）对所有主体—客体访问执行检查。\n\n\n内核态（Kernel Mode）\n\nWindows Executive：包括进程管理、内存管理、安全与审计等核心服务。\n内核驱动与 HAL：硬件抽象层（HAL）屏蔽底层硬件差异，驱动程序在内核态执行。\nHyper-V（若启用）：作为最底层的虚拟化监视器，支持虚拟机的隔离与管理。\n\n\n\n\n\n权限模型与访问控制\n\n访问控制列表（ACL）\n\n每个可保护对象（文件、注册表键、进程、线程、互斥体等）都有一个安全描述符，内含DACL（Discretionary ACL）和SACL（System ACL）。\n\nDACL：列出允许或拒绝各个主体（SID）的访问权限；支持“读取”、“写入”、“执行”、“删除”、“修改权限”、“取所有权”等多达 32 位的访问掩码。\nSACL：定义哪些访问要被审计。\n支持继承，子对象可从父目录或父容器自动继承 ACL 条目。\n\n\n\n\n访问控制矩阵（ACM）与能力（Capabilities）\n\n理论上的 ACM 为主体—客体的二维权限表；Windows 实际通过对象存储的 ACL（对应 ACM 的列）和令牌中的 SID 列表（对应 ACM 的行）来实现。\n能力（Capability）模型可视为 ACM 的行，指某主体对各对象的权限集合；Windows 则把主体拥有的权限封装在访问令牌中。\n\n\n\n\n\n访问令牌（Access Tokens）\n\n当用户或服务登录成功后，LSA 为其创建一个访问令牌，内含：\n\n用户 SID\n所属组 SID 列表（包括本地组、域组及内置别名）\n特权列表（如 SeShutdownPrivilege、SeDebugPrivilege 等）\n默认 DACL（用于创建新对象时的默认权限）\n其他标志（是否启用 UAC 限制令牌、是否为主令牌等）\n\n\n进程和线程：Windows 中主体为进程和线程，新进程继承父进程令牌（可基于策略做出修改），令牌不可变。\n用户账户控制（UAC）：Vista 及以后，管理员用户登录时会同时获得两个令牌——一个完整令牌（用于提权场景），一个受限令牌（默认用于运行应用），减少攻击面。\n\n\n\n Kerberos 在 Windows 中的应用\nKDC（Key Distribution Center）：运行在域控制器上，包含认证服务器（AS）和票据授权服务器（TGS）。\n认证流程：\n\n客户端向 AS 请求 TGT；\nAS 验证凭据后返回加密的 TGT；\n客户端使用 TGT 向 TGS 请求访问某服务的票据；\nTGS 发放服务票据；\n客户端携带服务票据向目标服务（如文件服务器、SQL Server）进行身份验证。\n\n票据缓存：用户登录后，TGT 存于本地缓存；访问域内其他服务时无需重复输入凭据。\n 恶意软件（Malware）\n\n定义：恶意软件是指任何以破坏、窃取或中断计算机系统正常功能为目的的软件。\n分类依据：\n\n传播方式（Proliferation）：是否自我复制，是否依赖用户操作。\n功能效果（Payload）：如展示广告、窃密、僵尸网络、数据加密勒索等。\n\n\n载体（Vectors）与有效载荷（Payloads）\n\n载体（Vectors）\n\n软件漏洞：利用操作系统或应用的安全缺陷自动传播。\n社会工程：钓鱼邮件、恶意网页、附件诱导用户点击。\n\n\n有效载荷（Payloads）\n\n无明显作用的“无害”程序\n弹出广告或消息\n加入僵尸网络进行 DDoS 或垃圾邮件发送\n窃取凭证和敏感信息\n加密或破坏文件，要求赎金（勒索）\n\n\n\n\n常见类型包括：\n\n病毒（Virus）\n\n必须附着在宿主文件、磁盘或文档上，借助宿主程序运行时激活，自我复制并传播。\n\n\n蠕虫（Worm）\n\n独立的可执行程序，无需宿主或人为干预即可自我复制，通过网络漏洞自动传播。\n\n\n特洛伊木马（Trojan）\n\n伪装成合法应用或文件，需用户主动执行，不具自我复制能力。\n流行形式：远程控制工具（RAT）、广告/间谍插件、勒索软件\n\n\n后门（Backdoor）\nRootkit\n逻辑炸弹（Logic Bomb）\n勒索软件（Ransomware）\n\n工作流程：\n\n感染阶段：通过钓鱼邮件、恶意网页或利用漏洞下载执行。\n加密阶段：本地文件或网络共享资源被对称加密；加密密钥再用攻击者的公钥加密。\n勒索阶段：弹出赎金通知，要求用户支付比特币等才能获得私钥解密。\n\n\n防御难点：即使杀毒软件事后清除进程，加密操作已完成，数据往往无法恢复。\n\n\n广告软件（Adware）\n间谍软件（Spyware）\n\n\n防护\n\n漏洞管理：及时打补丁，关闭不必要的服务与端口。\n邮件与网页防护：部署反钓鱼、反垃圾邮件、Web 安全网关。\n终端保护：使用行为检测、白名单策略、勒索软件防护工具。\n备份恢复：定期离线或异地备份，确保在遭遇加密/破坏时可快速恢复。\n安全教育：提高用户对可疑附件、链接和权限请求的警惕。\n\n\n\n 利用（Exploits）\n“利用”指的是利用软件或硬件漏洞，绕过操作系统的安全边界，从而获得对目标系统的未授权访问权限或执行任意代码的能力\n\n内存管理（Memory Management）\n\n在 C/C++ 中，程序员必须手动管理内存，虽然灵活高效，但易引入严重漏洞\n常见错误包括\n\n缓冲区（Buffer Overflows）溢出\n\n程序执行时，会在内存中为数组（缓冲区）分配连续的空间；如果写入的数据超过缓冲区大小，多余部分将覆盖相邻内存，造成溢出\n典型场景：使用 strcpy 等不做边界检查的库函数，将过长的字符串拷贝到固定长度的缓冲区中。\n\n\n栈溢出与栈破坏（Stack Overruns &amp; Stack Smashing）\n\n栈用于存储函数的局部变量和调用信息，每次函数调用会在栈上压入一个栈帧，返回时再弹出\n若 str 长度超过 128 字节，多余部分将覆盖返回地址（ret），使得函数返回时跳转到攻击者控制的地址，进而执行任意代码\n\n\n堆（Heap Overflows）溢出\n\n\n防护机制\n\n栈哨兵（Canaries）：在返回地址前插入随机值（canary），函数返回时检查该值是否被篡改，如被修改则终止程序，防止简单的栈破坏攻击\n数据执行保护（Data Execution Prevention, NX/XD/XN）：将栈段和数据段标记为不可执行，阻止注入的 shellcode 在此区域执行；断点：可通过“返回到 libc”（ret2libc）等技巧绕过\n地址空间布局随机化（ASLR）：随机化可执行文件和库的加载地址，使攻击者无法预测目标地址，增加利用难度\n\n\n\n\n返回导向编程（Return-Oriented Programming, ROP）\n\n利用程序或库中已有的短代码片段（gadgets），通过构造伪造的返回地址链，串联执行所需操作，无需注入新的代码。\n常见 gadget 如：pop %eax; ret、movl %eax, (%edx); ret 等，可在工具如 ROPgadget 中搜索\n\n\n竞争条件（Race Conditions）\n\n多线程或多进程并发执行时，攻击者可操纵执行顺序，在检查权限与实际使用资源间插入恶意操作。\n示例：先 access(“file”, W_OK) 检查无写权限，随后在 open(“file”, O_WRONLY) 前，将目标文件替换为敏感文件的符号链接，造成写入敏感文件\n\n\n\n 网络安全\n TCP/IP模型\n-不同层级的协议依次封装与解封装。上层协议的数据被下层协议通过添加头部（header）封装。\n\n\n应用层：为应用提供标准化数据交换接口\n\nHTTP、SMTP、FTP\n\n\n\n传输层：负责端到端通信，以及差错控制和流量控制\n\nTCP、UDP\n\n\n\n网络层：负责逻辑寻址、路由选择，将数据包从源发送到目的\n\nIP（v4/v6）、IPSec、ICMP\nIP 协议是无连接、无状态的。IPv4 本身不提供安全保障。IPv6 中安全支持为必选，借助 IPSec 实现\nIPSec\n\nIPv4 中可选；IPv6 中强制支持\n在网络层提供安全服务\n两大机制：\n\nAH（Authentication Header）：仅提供报文源身份认证\nESP（Encapsulating Security Payload）：提供身份认证与加密\n\n\n不阻止流量分析（包的来源、目的等信息仍可见）\n\n\n\n\n\n数据链路层：在同一网络内通过 MAC 地址和校验和封装帧传输\n\n以太网、Wi-Fi\n\n\n\n物理层：使用电气或机械接口进行比特传输\n\n以太网、Wi-Fi\n\n\n\n模式\n\n传输模式（Transport Mode）：\n\n仅对原始 IP 报文的有效载荷进行加密/认证，保留原 IP 头\n提供主机到主机加密，但仍可被流量分析\n\n\n隧道模式（Tunnel Mode）：\n\n将整个原始 IP 包（包含头部和有效载荷）封装为新 IP 包的有效载荷\n提高对流量分析和报文篡改的抵抗力\n常用于 VPN 网关间的加密通信\n\n\nvs\n\n传输模式：主机间加密，使用原 IP 头。防止内容被读取，但不防流量分析或 IP 头篡改。\n隧道模式：网关间或主机网关间加密，隐藏原 IP 头。不仅保护内容，也保护原 IP 头免被篡改。\n\n\n\n\n\n网络分段与零信任架构\n\n将大型网络划分为更小的子网（物理或逻辑），便于管理与策略控制\n利用 VLAN、SDN 等技术，实现资源隔离与故障/攻击限域\n零信任原则：“永不信任，始终验证”——内部外部一视同仁，强制多重认证、最小权限\n\n\n\n无线网络安全\n\n主流加密协议：WEP（已弃用）、WPA2（现行标准）、WPA3（更安全，兼容性待完善）\n常见威胁：流量嗅探、恶意接入点（Evil Twin/KARMA）、中间人劫持、密码破解（KRACK 漏洞）\n最佳实践：关闭 WPS、使用强口令与企业级认证、分离 IoT 设备 VLAN、部署 WIPS/WIDS\n\n\n\n拒绝服务（DoS/DDoS）攻击\n\n通过海量或特制请求耗尽目标资源，使合法用户无法访问\n常见类型：\n\nSYN 洪水：大量半开连接挤满服务器\n放大攻击（DNS、NTP）：利用开放解析器将小请求放大成大流量\n“低速缓慢”：Slowloris、R-U-Dead-Yet？通过不完成请求拖慢连接\n\n\n防护策略：部署流量限速、出口过滤、加强协议实现、使用抗 DDoS 服务\n\n\n\n整体安全思路\n\n分层防御（Defense in Depth）：多重隔离与检测，保障任一层失守时不致全面崩溃\n持续更新与补丁管理，及时修复已知漏洞\n上下文与角色感知的访问控制，确保最小权限原则贯彻始终\n\n\n\n 防火墙\n\n定义：一种硬件和/或软件系统。防止未授权的数据包从一个网络访问另一个网络。所有离开任何子网的数据都必须经过它\n实现“单点”安全措施，通过数据包分析和日志记录进行安全事件监控，通过实施规则集实现基于网络的访问控制\n类型：\n\n网络防火墙：部署在子网与互联网之间\n\n家用路由器就是网络防火墙的典型例子\n\n\n主机防火墙：部署在单台主机上\n\n\n基本功能：\n\n防止外部主机访问内部服务\n可限制内部主机访问外部服务（如 IRC、P2P）\n网络地址转换（NAT）：使用私有地址隐藏内部主机\n\n\n缺点\n\n无法防护绕过防火墙的攻击（如隧道技术）\n无法防御内部威胁或恶意内部人员\n出口过滤可提供一定帮助\n网络防火墙无法始终阻止携带病毒的程序或文件传输\n\n\n分组过滤器\n\n指定允许或丢弃哪些数据包\n规则可基于：\n\n源/目的 IP 地址\nTCP/UDP 源端口和目的端口号\n\n\n可同时过滤入站和出站流量\n仅检查包头即可在路由器上实现\n\n\n分组过滤规则\n\n规则执行顺序依赖实现\n\nIPTABLES：匹配到的第一个规则即应用\n\n用于访问 Linux 防火墙规则表的命令行工具\n本身并非防火墙，而是配置由 netfilter 模块实现的防火墙\nIPTABLES 使用“表”来存储“链”\n默认表为 filter（过滤）表\n链是一系列有序规则\n\n匹配则跳转（jump）到指定目标\n否则继续检查下一条规则\n\n\n一张表可包含多个链\n\n链可以针对协议细分（如专门处理 TCP）\n跳转目标可设为 ACCEPT、DROP、LOG，或另一个链\n通过组合链与跳转可构建复杂行为\n\n\n默认提供四张表：\n\nfilter（过滤）\n\n包含三条内置链：INPUT、OUTPUT、FORWARD\n\n\nnat（地址转换）\nmangle（报文修改）\nraw（跳过连接跟踪）\n\n\n策略\n\n宽松（Permissive）策略：默认允许所有流量，仅阻止危险服务\n\n易于管理，但可能因遗漏规则而导致安全漏洞\n\n\n严格（Restrictive）策略：默认阻止所有流量，仅允许指定服务端口\n\n安全性更高，但可能误将自己“锁死”\n\n\n\n\n优势和局限：\n\n简单、开销小、易于验证正确性\n无法阻止利用应用层协议漏洞的攻击\n无法支持更高级的身份验证\n配置错误时易出现误放或误阻\n\n\n\n\nPF：检查所有规则，最后一个匹配的规则应用\n\n\n规则按“链”组织，每个链是一组逻辑相关的规则\n不同数据包会激活不同的链\n\n\n应用层网关（Application-level Gateways）\n\n分组过滤器仅依据头部信息，而应用层网关可深入理解应用层协议\n例如允许 HTTP、SSH，阻止 BitTorrent\n可实现比固定端口规则更复杂的控制\n\n\n代理服务器 Proxy servers\n\n代理代表内部主机发起连接\n可阻止特定访问，并扫描文件或网页中的恶意内容\n客户端 → 代理 → 服务器\n服务器 → 代理 → 客户端\n问题：\n\n每个连接开销大\n比分组过滤器更昂贵\n配置复杂\n每种服务通常需独立代理服务器\n\n\n\n\n网络地址转换（NAT）\n\nIP 地址短缺导致大多数路由器自动执行 NAT\n隐式优势\n\n内部主机对外几乎完全隐藏\n仅将已建立的连接转发到内部主机\n或根据特定的端口转发规则\n防止任何未经请求的随机端口攻击\n\n\n\n\n\n 互联网安全\n\n其他威胁模型的不同之处：\n\n攻击者并不控制网络\n攻击者无法访问目标操作系统\n\n\nCookie\n\nHTTP 是无状态协议\n而我们的大多数在线操作需要保持“会话状态”\nCookie 是浏览器用来维持状态的小型文本文件\n工作机制：\n\n服务器在 HTTP 响应中通过 Set-Cookie 头部下发 Cookie\n浏览器在后续对同一域名的 GET/POST 请求中会自动携带相应 Cookie\n\n\n类型:\n\nSession Cookie（会话型）\n\n浏览器关闭时删除\n不包含过期时间\n\n\nPersistent Cookie（持久型）\n\n到达设置的过期时间后失效\n\n\nSecure Cookie\n\n仅在 HTTPS 连接下发送\n\n\nHttpOnly Cookie\n\nJavaScript 无法访问\n\n\n\n\nCookie 与其所属的域名绑定。许多网站会在页面中嵌入第三方广告或资源请求\nCookie 漏洞：\n\nCookie 窃取/劫持：攻击者获取用户 Cookie 信息，从而假冒用户身份，访问账户或数据\nCookie 投毒：篡改或修改 Cookie 内容，注入恶意代码或数据，可能引发各种安全问题\n\n\n\n\n攻击类型\n\n跨站脚本（XSS）\n\n一种注入攻击，类似于 SQL 注入\n浏览器解析 HTML 内容时，将标签视为结构与脚本执行\n如果能够将恶意 HTML/JavaScript 注入页面内容，浏览器会执行这些脚本\n\n\n反射型 XSS\n利用精心构造的 URL，将恶意脚本作为参数直接反射到服务器返回的页面中\n持久型 XSS\n\n更危险，无需诱导用户点击恶意链接\n任何对用户输入未正确过滤 HTML 标签的地方都易受影响\n\n如博客评论区、论坛发帖、购物网站评价等\n\n\n\n\n防御 XSS\n\n定位所有处理不可信数据的入口\n根据上下文对输出内容进行适当的 HTML 转义\n\n在文本节点、属性值、JavaScript 代码、CSS 中的转义方式各有不同\n\n\n对于交互式网站上的所有用户输入，必须彻底转义，才能防止脚本注入\n\n\n跨站请求伪造（CSRF/XSRF）\n\n用户发起 HTTP 请求时，会自动携带相关的会话 Cookie（如登录后的 sessionID）\n如果用户已登录，攻击者只需诱导或自动触发一个请求，即可在用户授权下执行操作\n\n\n\n\n\n 数据库\n\n数据库安全是一系列工具、控制机制和措施，旨在建立并维护数据库的：\n\n机密性（Confidentiality）：保护数据库中的敏感数据不被未授权访问。\n完整性（Integrity）：确保数据库中的数据准确、完整、一致且有效。\n\n内部一致性（Internal consistency）：数据库条目遵循预定义的规则。\n外部一致性（External consistency）：数据库条目内容正确，符合现实世界的数据。\n\n\n可用性（Availability）：确保需要时能够及时访问数据库数据。\n\n\nSQL 安全\n\nSQL 用于在关系型数据库中检索和更新信息。\n基于三要素实现访问控制：\n\n用户（Users）\n操作（Actions）\n对象（Objects）\n用户对对象执行操作\n\n\n\n\n\n","slug":"笔记/计算机安全/计算机安全","date":"2025-05-20T10:00:00.000Z","categories_index":"笔记-计算机安全","tags_index":"Computer Security","author_index":"zExNocs"},{"id":"6771626c94a6c99bb57f253564040531","title":"安卓开发","content":" 一.安卓介绍\n安卓(Android)是一个主要为手机和平板设计的操作系统(operating system)。其主要是基于 Linux 内核的修改版本构建，针对移动设备架构进行了优化，使用类似Java的环境进行应用开发（与传统Java平台不同）。\n其软件设计主要采用 Apache 许可证开源，鼓励社区开发和贡献。特定部分不是开源的。\n 0. 平台架构：\n从下到上：安卓内核(kernel)、硬件层、ART(Android Runtime)、Java API 架构、用户应用。\n 1. 灵活性 (Flexibility)\n其具有灵活性(Flexibility)和定制性(Customization)：\n\n允许硬件制造商进行定制，从而实现跨设备的多样化 Android 体验。\n支持自定义引导加载程序，实现更深层次的系统修改和 root 权限（在某些情况下）。\nroot 权限提供 super 用户访问权限。\n\n 2. 兼容性 (Compatibility)\n具有向前和向后(Forward &amp; backward)兼容性支持。\n向后兼容性限制：\n\n无法使用早期版本中不存在的 API。\n开发人员可以为其应用指定最低 API 级别。\n\n许可和品牌：\n\nAndroid logo 已获得 Creative Commons (CC) 许可。\n标有“Android 手机”的设备必须：\n\n通过特定的兼容性测试。\n支持 Android 设定的 API 标准。\n\n\nAndroid”品牌仅授权给开放移动联盟(Open Mobile Alliance)成员。\n\n 3. 安卓内核 (kernel)\n安卓内核是基于 linux 的，但有一些特定的修改。\n\n电源管理增强(Power Management Enhancements)，包括唤醒锁(wakelock)。\nBinder IPC (进程间通信 Inter-Process Communication) 驱动程序：一种用于高效进程通信的自定义机制。\nLow Memory Killer：一种在内存不足的情况下优雅地终止进程的机制，根据重要性确定优先级。\nAshmem (Anonymous Shared Memory 匿名共享内存)：\nAlarm Driver (警报驱动程序)：用于根据定时事件唤醒设备。\nOOM（Out of Memory 内存不足）调整：针对前台和后台进程采用不同的 OOM 处理方式。\n\n 4. 安卓硬件 (hardware)\n\nBluetooth：BlueZ\nGPS：制造商提供的 libgps.so\nWifi：wpa_supplicant\nDisplay：标准帧缓冲驱动程序 Standardframebuffer driver\nKeyboard：标准输入事件 Standardinput event\nLights：制造商提供的 liblights.so\nAudio 音频：制造商提供的 libaudio.so\nCamera：制造商提供的 libcamera.so\nPowerManagement：“wakelocks” kernel patch\nSensors：制造商提供的 libsensors.so\nRadio 无线电：制造商提供的 libril.so\n\n 5. 安卓应用\n安卓应用使用的是沙盒(sandboxing)模式：\n\n确保应用程序在隔离环境中运行，以确保安全。\n利用 Linux 多用户功能。\n\n通常一部手机只有一个用户。\n\n\n每一个应用都有自己的进程，运行在自己的虚拟机中，拥有独一无二的 UID/AID。\n确保应用程序无法访问其他应用程序的文件、数据或进程。\n利用内置的 Linux 安全措施，包括标准用户和 root 用户权限。\n\n标准用户没有根访问权限。\n根访问权限涵盖整个系统，但通常拥有限制以增强安全性。\n\n\n\n 6. 安卓启动过程\n\n启动 ROM/Bootloader：将引导加载程序加载到 RAM 中，检测外部 RAM、设置网络、内存等。\n启动内核：配置缓存、保护内存、调度并加载驱动程序。\n初始化：挂载 /sys、/dev 或 /proc 等目录，运行 init.rc 脚本。\nZygote &amp; VM：允许在 Android VM 之间共享代码，以便快速启动不同应用的单独 VM。\n系统服务应用：电源管理器、活动管理器、电话、注册表、包管理器、上下文管理器、系统联系人提供程序等\n\n 7. Zygote\nZygote 是 Android 启动序列的一部分，可帮助启动应用程序。系统通过 Zygote 进行初始化，从而在 Dalvik（旧版 Android）或 ART（新版 Android）上执行应用。\n出于安全和隔离的原因，Android 应用程序在单独的进程中运行。每次启动应用程序时，启动一个新进程并从头开始设置 Android runtime (ART) 的新实例效率很低。\nZygote 是辅助其启动的一个系统进程，初始化时包含所有必要系统库的主进程，这可确保常用资源已预先加载并在应用程序之间共享。在系统启动时，它会加载基本 Java 和 Android 类（java.、android.）。此预加载可缩短后续应用的启动时间。\nAndroid应用在启动时会从Zygote进程派生一个新的进程，以减少应用启动时的初始化开销。减少了应用进程从零开始的初始化时间，提高了Android设备多任务或快速切换应用时的性能。\n安卓虚拟机创建依赖 Zygote:\n\nSingle Instance: 系统启动后，Zygote 会创建 Android 虚拟机（Dalvik 或 ART，具体取决于版本）的单实例。\nClass Reference: 此 VM 已准备好在 Zygote 初始化期间加载的类，确保用户应用快速启动和响应。\n\n 8. ART\nART是用来替代Dalvik虚拟机的东西，其特点如下：\n\nAOT 预编译（Ahead-Of-Time Compilation）：在应用安装时就会进行预编译，将应用的字节码（.dex 文件）编译为本地机器码并存储下来。这样在应用运行时，省去了重复的 JIT（Just-In-Time）编译过程，启动更快，性能更高。\n更高效的垃圾回收（Garbage Collection）：ART 采用分代式垃圾回收（Generational GC）或其他更先进的 GC 技术，减少卡顿并降低对应用运行的干扰。\n更好的调试工具和诊断能力：ART 提供了更丰富的调试选项和运行分析工具，如更详细的内存分析、线程分析等，方便开发者进行性能优化。\n兼容性：虽然 ART 采用的是与 Dalvik 不同的执行机制，但大多数应用无需进行改动就可在 ART 上运行，Google 在实现中保留了较高程度的向后兼容。\n\n 二. 安卓硬件\n 1. 一芯片系统 (System On A Chip, SOCs)\n\n仅使用芯片上的部分晶体管组成 CPU\n\n使用其余晶体管构建系统的其他组件\n外部引脚直接连接到外围硬件\n\n\n在芯片上集成多个异构组件\n\n减少通信开销\n减少热量\n\n\n晶体管提供计算、存储\n\n将芯片划分为多个通信区域\n\n\n通用架构\n\nCPU 核心：ARM Cortex 系列（例如 Cortex-A78）\n用于 AI 任务的神经处理单元 (NPU)\n\n\n差异\n\n人工智能和机器学习的集成：SoC 越来越多地包含用于机器学习和人工智能的专用核心，从而增强了面部识别和增强现实等功能。\n\n\n根据用例进行定制\n\n不同的 SoC 针对不同的性能需求进行了优化，例如游戏、摄影或能效。可以拥有独特的配套模块。\n\n\n封装层叠 (Package on Package, PoP)：\n\n背景：传统 SoC 中的 RAM集成度各不相同，在现代 SoC 中，集成 RAM 更为常见，但配置可能因应用需求而异。\n定义：一种将内存封装直接安装在 SoC 顶部的封装技术。\n生产灵活性：允许独立生产和测试内存和逻辑组件。\n空间优化：垂直堆叠组件以节省水平空间，这对于紧凑型设备设计至关重要\n\n\n\n 2. ARM CPU vs Intel CPU (x86)\n约 95% 的智能手机使用 ARM CPU，其余设备使用 Intel、MIPS 等。\n\n为什么使用ARM？\n快速高效的操作。\n更高的代码密度可实现更紧凑的设计。\n减少组件堆积，提高空间利用率。\n\nARM 指令的特点：\n\n每条指令尽可能只使用一个周期。\n每条指令的长度为 32bit\n几乎每条指令都具有条件执行功能。\n一般寄存器：\n\nR0-R12：一般目的的寄存器\nR13：栈寄存器\nR14：链接寄存器\nR15：PC\n\n\n并不是所有的指令都用到 32bit，这会导致潜在的低效率，因此使用 Thumb 指令：\n\nThumb 指令使用紧凑的 ARM 16-bit 指令集。\n使用可变长度指令集，保留最常用的ARM指令并将它们编码成16位。\n\n通过减少指令大小，实现了更快的速度。\n一次32-bit内存可以检索两个16位 Thumb 指令。\n\n\n\n\n\nARM bit.LITTLE：\n\nARM 使用大小核，通过集成强大的 (big) 内核和高效的 (LITTLE) 内核，将高性能与能效相结合。\n系统可以根据当前任务的需求在核心之间无缝切换，确保最佳性能和效率。\n\n 三. 安卓软件的核心组件\n安卓与传统Java应用不同，可以有多个入口点。这些入口点定义了安卓系统可以与应用程序交互或者进入应用程序的方式，包括活动(Activity)、服务(Service)、广播收集器(Broadcast Receiver)和内容提供程序(Content Provider)。\n传统的操作系统通常只有一个 Main 入口点，并且操作系统将程序加载到进程中并执行，实例化 Java VM 加载应用程序使用的所有类 执行 main。\n而安卓中是基于component的模型，拥有多个应用程序入口点：\n\nZygote fork 后仍执行 main 函数，但在此基础上进行抽象。\n并非所有都是用户的入口点。\n每个component都作为逻辑上独立的唯一实体存在\n\n一个 Android 应用程序应该包含多个组件，并且用户经常在短时间内与多个应用程序进行交互，因此应用程序需要适应不同类型的用户驱动的工作流程和任务。\n安卓的特定组件通常是由特定interface进行通信，在运行时进行绑定。每个组件都有特定的生命周期，根据需要动态加载和卸载。\n 1. Activity\n包括UI组件，View。是 android.app.Activity 的子类。其提供一个虚拟的UI，每一个 Activity 都拥有自己的窗口。\n a. View\nUI layout属于一个View，是一种资源，在单独的通过编程构建的XML文件中指定。\n\nView的子类包含：\n\nViews: 用来显示一些内容\nWidgets: 执行某些操作\nViewGroups：布局子视图\n\n\n\n I. Views\n\n设计规范：\n\n首选浅布局层次结构。\n尽可能减少嵌套布局（遍历成本）。\nwide over deep。\n\n\n编程方面：\n\n可以使用使用 setContentView() 来指定Activity的View。\n可以使用 addView()、removeView() 来改变视图的层次结构。\n根据 XML 布局定义生成的ID，使用 R.layout.ID 来找到特定的视图。\n可以将 View 和数据绑定在一起。\n\n\n策略：\n\n手机很少是相同的，通常具有不同的屏幕尺寸和宽高比、各种分辨率。所以应该避免硬编码 UI 组件配置细节。\n布局应该适应其所填充的屏幕：\n\n将布局定义为层次结构和关系。\n使用以密度无关像素 (density-independent pixels,dp) 而不是 px 定义测量值\n\n1dp 相当于 160dpi 上的一个像素。\n\n\n可以定义特定于配置的布局\n\nSmall, normal, large and extra large.\n使用最小宽度限定符\n使用方向限定符\n进行规划，例如 res/layout-sw600dp/main_activity.xml\n\n\n\n\n\n\n\n II. ViewGroups - Layouts\nLayouts通常包含：\n\nFrameLayout：最简单的layout，只包含一个物体。\nLinearLayout：根据方向属性，将所有子项对齐到单一方向。\nTableLayout：将子项定位到行和列中\nConstraintLayout / RelativeLayout：让子视图指定它们相对于父视图或彼此的位置，使用对齐进行约束。\nScrollView：垂直滚动视图。\nSwipeRefreshLayout：实现“下拉刷新”交互效果的控件。检测垂直滑动、显示进度条并触发回调方法。\n\n III. Widgets\n用于可以交互的子View：\n\nButton\nTextView\nEditText\nCalendarViewer\nImageView\n\n这些组件可以处理UI事件：\n\n代码中，可以使用 setOnClickListener()。\n在XML中，可以设置 android:onClick=&quot;&quot; 参数。\n\n这些组件可以文字展示：\n\n代码中，可以使用 .getText() 和 .setText()\n在XML中，设置 android:text=&quot;&quot; 参数，例如 android:text=&quot;@string/hello_world&quot;\n\n b. 数据绑定\n传统model中，设计大量样本代码(boilerplate code)：\n\nActivity 和 Model 紧密耦合\n处理 Activity 中的 onClick 事件\n查找对相关视图的引用\n检索内容（例如文本字段的内容）\n将结果写回视图\n\n一个数据模型应该：\n\n将 POJO 绑定到View组件以进行初始填充\n使对象可观察，以便在更改时自动更新 UI\n\n如继承&quot;BaseObservable&quot;，或者使用&quot;ViewModel + Livedata&quot;。\n使用可观察域，即 @{} 和 @={}\n可观察的objects, fields and collections\n\n\nData Binding Library：\n\n是一种直接在XML中与类的成员进行绑定的方法，减少了在Activity中进行初始化的方法。\n即从layout中引用model对象。\n会生成必要的绑定类。\n包括基本格式表达式。\n\n\n双向数据绑定，允许从View更新Object，反之亦然\n\n数据绑定的实现方法：\n引用数据：\n123456&lt;data&gt;  &lt;variable    name=\"viewmodel\"    type=\"com.example.myapplication.MyModel\"  /&gt;&lt;/data&gt;\n可观察域：\n\n@{}：接收数据变化，是单项绑定。\n@={}：接收属性的数据更改并监听用户更新，是双向绑定。\n\n1android:progress = \"@={viewmodel.progress}\"\n c. Manifest文件\n是应用的components列表，用于指定程序的入口点，通常包含应用的信息(需要什么、什么可以做)。不在这个列表的不能称之为 components。那么该组件无法作为入口点。\n通常要考虑：\n\n如何启动（默认的启动 activity）\n如何向其他人展示\n哪些内容可以访问它 （程序内部和外部）\n用户的权限。\n\n d. Navigation\nActivity 可以启动其他的 Activity，此时新的Activity就在旧的Acitivity上方。Activity 类似于一个栈结构。\n i. 分层活动导航 (Hierarchical Activity Navigation)\n种类：\n\nDescendant navigation 子级导航：指从一个层级（通常是父级）“深入”到下一个层级（子级）的导航方式。也可理解为“层级向下”或“钻取”式导航。\nLateral navigation 横向导航：在同一层级内进行“左右”或“并列”切换的导航方式。也可理解为“同级别的兄弟页面之间”的移动。通常是同一父级下不同子页面之间的切换。\n\n形式：\n\n列出可选择的 activity\n屏幕之间 Tab\n在屏幕或页面之间滑动\n按钮切换\n导航（graphs, destinations）\n\n ii. Back 和 UP 导航\n都属于撤消横向和后代导航。\n\nBack: 可以在同一级中返回。\n\n关闭当前的 activity。\n恢复堆栈中的下一个活动\n\ntaps 和 swipes 会更改当前屏幕显示的信息，而不是activity，因此不会影响历史记录。\n\n\n\n\nUp: 返回父级。\n\n关闭当前的 activity。\n启动（或恢复）相应的父级活动。\n\n在 manifest 中表示。一般使用 android:parentActivityName=&quot;.ParentActivity&quot; 表示。\n如果它在后台堆栈中，则将其移到最前面。\n可以创建一个“假”返回堆栈\n\n\n\n\n\n e. Fragments\nFragments 是比 Activity 更小的 UI 单元。\n\n多个 Fragments 可以组成一个 Activity，这样可以支持大屏幕 （例如平板）。\n可以在多个 Activity 复用一个 Fragments。\n管理逻辑上属于堆栈中同一 Activity 的项目集合\n\n例如翻阅照片。\n\n\n在输入、事件、生命周期方面与宿主 Activity 的关系有些复杂\n\n f. Intents\nIntent用来描述一个操作(operator)，包括要执行的Action以及所要处理的数据（可以用 URI 表示）。\nAndroid 的设计理念是通过 Intent 在不同的组件（尤其是 Activity 之间）进行通信，而不是让我们手动去实例化 Activity。\nIntent 还可以实现延迟绑定（Late Runtime Binding），并能把多个 Activity 逻辑“黏合”到一起。\n系统会根据AndroidManifest.xml中注册的信息来确定该如何响应并处理 Intent。\n i. 使用 Intent 启动/停止一个 Activity\n如何使用 Intent 启动一个 Activity：\n\n创建一个新的 Intent 对象。\n指定想要将 Intent 发送给谁（可以是显式或者隐式方式）。\n调用 startActivity() 函数，并把这个 Intent 传递进去。\n系统会根据传入的 Intent，启动一个新的 Activity（即由系统的运行时来负责真正的启动过程）。\n\n停止一个 Activity：\n\n被启动的 Activity 完成任务后，可以通过调用 finish() 方法来销毁自己，返回到原先的 Activity。\n当用户按下“返回”键时，系统也会销毁当前 Activity，将界面返回到上一个 Activity（或退出应用）。\n\n ii. 显式 vs 隐式 Intent\n显式(Explicit) Intent：\n\n提供要启动的 Activity 的完全类名。\n如 Intent myIntent = new Intent(context, otherActivity.class);\n\n隐式(Implicit) Intent:\n\n只指定一个操作（Action）和数据类型（Data/Category/Type）。\n由系统或其他应用中符合条件的组件来响应。\n必须要在 manifest 中声明，具体看下面 Intent Filters\n\n iii. Intent Filters for Deep Linking\nIntent Filters 是在 AndroidManifest.xml 中为 Activity、Service 或 BroadcastReceiver 指定的过滤条件，用于匹配特定的 Intent。它们决定哪些 Intent 可以由某个组件（如 Activity）处理。\n匹配条件：\n\nAction：定义意图的操作（例如 android.intent.action.VIEW 表示“查看某个资源”）。\nCategory：补充描述意图的分类（例如 android.intent.category.DEFAULT 是默认分类）。\nData：指定数据 URI（例如 URL 的协议或路径）或 MIME 类型，用于匹配数据内容。\n\n通过在 AndroidManifest.xml 中声明 Intent Filters，可以：\n\n指定某个 Activity 可以处理的 Intent 类型。\n实现 Deep Linking：允许应用直接打开特定的内容（如通过 URL 直接跳转到应用内的某个界面）。\n支持隐式 Intent：当其他应用或系统发送隐式 Intent 时，只有符合声明的 Activity 会被匹配到。\n\n当多个应用或组件的 Intent Filters 都匹配某个 Intent 时，系统会弹出一个选择对话框，允许用户选择使用哪个应用来处理。\n例子：\n1234567&lt;activity android:name=\"com.example.martinactivities.ForthActivity\"&gt;    &lt;intent-filter&gt;        &lt;action android:name=\"android.intent.action.VIEW\" /&gt;        &lt;category android:name=\"android.intent.category.DEFAULT\" /&gt;        &lt;data android:scheme=\"http\" /&gt;    &lt;/intent-filter&gt;&lt;/activity&gt;\n这个配置表示 ForthActivity 是一个可以处理特定 Intent 的 Activity。\n\n表示此 Activity 可以处理“查看某些内容”的操作，例如点击链接或打开文件。\n表示这个 Activity 是默认分类，用于普通的隐式 Intent 调用。\n表示此 Activity 能够处理 HTTP URL 的数据。\n\n g. Activity 之间的通信\n\nstartActivity()的限制：\n\n用于启动另一个 Activity，但不允许返回结果到启动它的 Activity。\n应用通常需要维持用户在多个 Activity 中的状态。\n在复杂的场景中，Activity 可能需要跨进程通信（IPC），或者作为其他应用的入口点。\n\n\nstartActivityForResult() 已过时：\n\n启动另一个 Activity，并允许子 Activity 在完成后将结果返回到启动它的 Activity。\nstartActivityForResult() 需要提供一个数值型的请求代码（requestCode），以便区分返回的结果来源。\n子类使用 setRusult() 和规定一个返回结果的 intent 来返回结果。\n父类使用重写 onActivityResult(int requestCode, int resultCode, Intent) 方法来处理结果。\n\n\nregisterForActivityResult()\n\n是 startActivityForResult() 的现代替代方法，更加简洁、安全，避免了旧方法中繁杂的 onActivityResult() 回调逻辑。\n基于合约：例如使用 ActivityResultContracts.StartActivityForResult 来指定启动的 Activity 和结果处理逻辑。\n回调处理：在启动 Activity 时注册一个回调函数，当子 Activity 结束后返回时，该回调会被调用。\n\n\n\n h. Tasks vs Activities vs Processes\n\n\n\n名字\n介绍\n\n\n\n\nActivities\n1. 定义一个应用组件(application component)中屏幕的信息2. 应用是一个Activities的集合，包括创建的和从其他应用中复用的\n\n\nTasks\n1. 为了实现一个目标而用到的一系列activity2. 单个任务可以只使用一个app中的activity，也可以利用多个不同app中的activity。\n\n\nProcesses\n1. 创建特定应用的 host component2. 一个任务可以跨越多个进程\n\n\n\n i. 安卓开发中的任务管理。\n为什么要管理任务？\n\n用户频繁切换任务/堆栈：比如从一应用跳转到另一个应用或在同一个应用的不同活动中切换。\n大多数任务由多个活动 (Activities) 组成：\n\n调用另一个应用中的 Activity：\n\n并不总是局限于一个应用，而是跨应用的任务。\n通过松散绑定的方式提供连贯的用户体验，依赖于 Intent 等机制。\n\n\n非确定性的用户路径 (Non-deterministic User Journeys)：\n\n应用的启动位置不一定相同。\n用户可能在一段时间内“离开”应用，然后返回。\n\n\n\n\n\n任务管理的好处\n\n利用其他应用的组件：\n\n通过 Intent 调用其他应用的 Activity，简化复杂功能的实现。\n\n\n细粒度的资源管理\n\n单个任务的活动状态可以根据需求调整，以优化内存、CPU、存储、屏幕和电池的使用。\n例如，释放后台任务的资源来提高设备性能。\n\n\n\n如何应对任务管理？\n\n组件具有生命周期 (Lifecycles)：\n\n每个 Activity 和 Fragment 都有生命周期，通过管理生命周期事件（如 onCreate, onStart, onDestroy 等），可以更高效地控制任务和资源使用。\n\n\n\n Activity 与进程管理的核心概念\n\n系统不会直接杀死活动 (The OS never kills an Activity)：\n\nAndroid 系统不会单独终止活动，而是会杀死托管这些活动的进程以释放内存。\n系统更倾向于通过终止后台进程来回收资源。\n\n\n应用如何响应内存压力：\n\nonTrimMemory 回调：应用可以通过该回调管理自己的内存资源，例如清除缓存或释放不必要的数据。\n\n\n进程被终止的可能性取决于其状态：\n\n系统会选择对用户影响最小的进程进行终止。\n决策依据包括：\n\n进程最近是否被使用。\n进程是否与主屏 (Home Activity) 相关联。\n\n\n\n\n\n j. Activity 的生命周期管理\n I. Activity 的三种主要状态：\n\nActive (活跃状态)：\n\nActivity 位于前台并获得焦点。\n用户正在与该 Activity 交互。\n\n\nPaused (暂停状态)：\n\nActivity 仍然可见，但不在顶部。\n例如，出现一个半透明窗口或分屏模式中另一个应用获得焦点时，Activity 进入暂停状态。\nActivity 处于活动状态但未获得焦点。\n\n\nStopped (停止状态)：\n\nActivity 被其他 Activity 完全遮挡。\n此状态下，Activity 不再可见。\n\n\n\n II. 优先级降低的 Activity\nAndroid 系统如何管理资源：\n\n当 Activity 进入 Paused 或 Stopped 状态时，系统可能会降低其资源分配优先级。\n具体表现：\n\n停止状态的 Activity 被挂起 (Suspended)：不会主动执行任何代码，但其状态会被保留。\n非活动的 Activity 可能被销毁 (Destruction)：\n\n如果系统需要额外内存，可能会销毁处于 Stopped 或 Paused 状态的 Activity 来回收资源。\n重要性：必须保存 Activity 的状态以便用户返回时恢复。\n\n\n\n\n\n III. 注意事项\n状态转换会触发事件，确保避免以下问题：\n\n因为用户接听电话导致的崩溃：\n\n当用户接听电话或应用失去焦点时，系统可能会暂停或停止当前的 Activity。\n如果没有妥善管理生命周期，可能会导致崩溃。\n\n\n在用户不使用应用时消耗资源：\n\n停止或暂停的 Activity 不应占用系统资源（如内存、CPU），以避免浪费和影响其他任务的性能。\n\n\n丢失用户进度：\n\n确保在 Activity 状态变化时保存用户数据（如输入内容、进度）。\n提供一致的用户体验非常重要。\n\n\n在配置更改期间的崩溃（例如屏幕旋转）：\n\n设备从纵向切换到横向或其他配置变化（如语言、屏幕大小）时，Activity 会重新创建。\n如果没有保存状态或正确处理生命周期事件，可能会导致崩溃或用户数据丢失。\n\n\n\n IV. 生命周期\nonCreate() → onStart() → onResume() → onPause() → onStop() → onDestroy()\n\nonCreate()：Activity 的初始化方法，相当于组件的“构造函数”。用于执行基础设置，例如加载 UI 布局。\nonStart()：Activity 对用户可见，但尚未获得焦点。此时 UI 已加载，但用户还不能与之交互。\nonResume()：Activity 已进入前台，并可供用户交互。Activity 将一直处于此状态，直到被其他 Activity 遮挡或用户切换应用。\nonPause()：Activity 不再是用户的焦点，但仍部分可见（如弹出窗口覆盖部分界面）。停止不需要运行的任务，例如暂停视频播放或相机预览。\nonStop()：Activity 完全不可见，处于后台状态。应释放资源并保存持久数据（如数据库或文件）。\nonDestroy()：Activity 被销毁时调用。可能不会正常调用，所以不要在这里保存状态。\n\n当一个活动启动另一个活动后，必然有一个活动进入 onPause()，另一个进入 onResume()。\n关于 onDestroy() 注意事项：\n\n配置更改时，例如设备的屏幕方向、语言或者输入设备发生变化时，当前 Activity 会被摧毁并重新创建。\n当一个 Activity 正常完成时，用户按下返回键或者调用 finish() 方法，onDestroy() 会被调用。\n如果系统因为资源不足终止后台进程，则不会调用。\n因此，应该在 onPause() 或 onStop() 中提前保存关键数据，避免因进程被杀死导致数据丢失。\n\n V. UI保存\n主要在于如何使用 onSaveInstanceState() 方法。\n该方法不应依赖 Activity 保存 UI 或状态，配置更改和进程终止都可以恢复。例如当设备旋转会导致配置更改。\n在 Activity 被停止 onStop() 之前，系统会调用 onSaveInstanceState()。目的是保存 UI 的瞬态状态（transient state），以便在 Activity 重新创建时恢复。\n例如：保存用户输入内容、滚动位置或临时选择项。\n\n保存瞬态 UI 状态：\n在 Activity 被重新创建时，保存的状态会通过 Bundle 传递：\n\n12onCreate(Bundle savedInstanceState)onRestoreInstanceState(Bundle savedInstanceState)\n\n保存非瞬态 UI 状态：\n\n使用 SQL 数据库。\n使用 SharedPreferences。\n\n\n\n实例状态 Instance State：\n\n目的：\n\n用于保存小型的、与 UI 相关的数据，这些数据易于序列化或反序列化。\n比如文本输入框的内容、滚动位置等。\n\n\n序列化开销：\n\n由于需要序列化和反序列化，存在一定的性能消耗。\n因此，数据量不宜过大，否则会影响性能。\n\n\n快速存取需求：\n\n状态保存需要足够快，否则会导致 UI 卡顿或帧丢失。\n\n\n\nBundle 特点：\n\n结构：Bundle 是一个键值对（Key/Value）的集合。\n适用场景：适合保存小型的瞬态状态（transient state），如当前页面索引、简单的用户数据。\nBundle 中存储的数据量有限，过大的数据可能导致 TransactionTooLargeException。\n复杂类（如自定义对象）需要实现 Parcelable 接口以优化性能。\n\n建议：\n\n只适用于储存小数据，适用于 UI 瞬态状态的数据存储，避免存储复杂和大规模的数据。\n由于序列化过程的开销，建议尽量保持 Bundle 的存储简单、快速。\n对于复杂数据（如网络请求结果、列表数据），建议使用 ViewModel 或持久化存储（如数据库、SharedPreferences）。\n\n VI. ViewModel：\nViewModel 可以在配置更改时保留状态，例如屏幕旋转不会导致数据丢失。\nViewModel 设计用于与 Activity 或 Fragment 的生命周期绑定，但独立于 UI 控件。在配置更改（如屏幕旋转）中，Activity 或 Fragment 会被销毁并重新创建，而 ViewModel 不会被销毁，从而保持数据的一致性。\n局限性：\n\n对配置更改免疫：ViewModel 可以在配置更改时保留状态，例如屏幕旋转不会导致数据丢失。\n不对资源管理终止免疫：如果系统由于资源不足（如内存不足）终止整个应用进程，ViewModel 中的数据会丢失。此时需要储存在数据库中。\n不对导航操作免疫：当用户导航离开当前 Activity 或 Fragment 时，ViewModel 会被销毁。\n\n VII. LiveData：\n\nLifecycleOwner：Activity 或 Fragment 实现了 LifecycleOwner 接口。提供生命周期对象，用于跟踪当前的生命周期状态。\nLifecycleObserver：注册为观察者的对象会感知生命周期事件。开发者可以将逻辑放入观察者中，而不是直接嵌入生命周期回调。\n\n好处：\n\n逻辑分离：生命周期逻辑与核心业务逻辑分离。\n减少错误：通过系统管理的组件状态减少数据更新冲突。\n\nLiveData 是一种可观察的数据容器，专为生命周期感知设计。与 ViewModel 配合使用，提供数据驱动的 UI 更新。\n特点：\n\n自动管理观察者：\n\n当 UI 组件处于非活动状态时，不会触发观察者回调，避免浪费资源或引发错误。\n当组件被销毁时，自动移除观察者。\n\n\n提供活动数据：\n\n只有当组件处于活动状态时，LiveData 才会分发数据更新。\n\n\n\n适用场景：\n\n数据绑定：实时更新 UI，适合动态数据流。\n简化生命周期处理：减少因生命周期变化导致的数据不一致问题。\n\n 2. Service\n在后台执行长时间运行操作的机制。没有用户界面（UI），用于长时间运行的操作。服务不受限于某个活动（Activity）的生命周期。一个服务可以被多个应用程序使用，避免资源重复。\n服务运行在主线程。\nActivity 经常会从前台转到后台（如用户切换任务）。当 Activity 被停止或销毁时，运行中的任务可能会中断。那么就出现一个问题：如何在 Activity 的生命周期内，或超过其生命周期的情况下处理任务。\n处理方法：\n\n基于生命周期的处理：主要由 Activity 的生命周期（如 onStop() 和 onPause()）及其 UI 组件的回调函数驱动。\n线程执行：Android 使用单线程模型，所有 UI 操作都在主线程（UI 线程）上完成。当需要运行耗时操作（如网络请求或复杂计算）时，必须使用辅助线程来避免阻塞 UI。\n\n线程与 Activity 的关系：\n\nActivity 销毁时，未正确管理的线程可能继续运行，导致内存泄漏或应用崩溃。\n\n为了处理超过 Activity 生命周期的任务，那么需要用到 Service。Service 是一个独立于 Activity 的应用组件，专门用于处理长时间运行的任务。即使 Activity 被销毁，Service 仍然可以继续运行。\n例如：\n\n播放后台音乐。\n定期从网络获取数据。\n上传或下载文件。\n\n服务的限制（What Services are not）\n\n服务不是单独的进程，而是运行在声明它的应用进程中。\n服务不是线程，需要手动启动工作线程以处理后台任务。\n\nAndroid 的 WorkManager\n\n主要用于处理可延迟的后台任务，同时遵守系统的后台限制。\n\n任务、服务与活动的分工（Tasks, Services, and Activities）\n\n服务处理长时间运行或后台任务，例如检查邮件、播放音乐。\n活动负责用户交互，例如显示邮件列表或音乐播放界面。\n\n a. 服务的生命周期（Service Lifecycle）\n\nonCreate()：服务初始化时调用。\nonStartCommand() 或 onBind()：根据启动方式调用。\nonDestroy()：服务结束时调用。\n\n绑定服务（Bound Services）：与其他组件绑定，生命周期与绑定组件相关联。\n非绑定服务（Unbound Services）：独立运行直到被显式停止。\n b. 服务的类型\n\n\n\n类型\n描述\n生命周期\n启动方式\n是否绑定\n\n\n\n\n前台服务\n用户可见的操作（如播放音乐）。\n操作或任务活动期间。\nstartForegroundService()\n可绑定，也可独立运行。\n\n\n后台服务\n用户不可见。\n可能在系统内存不足时终止。\nstartService()\n可绑定。\n\n\n绑定服务\n用于组件间通信。\n绑定的组件存在期间。\n使用 bindService()\n必须绑定。\n\n\nIntentService\n处理异步任务，工作完成后停止。\n任务完成后自行终止。\nstartService()\n很少绑定。\n\n\n\n c. Activity与Service的通信\n\n通过Intent通信：活动（Activity）可以通过发送Intent启动一个服务，触发onStartCommand。\n服务与用户通信：服务可以通过通知（Notifications）与用户交互，因为服务本身没有UI。\n服务与Activity通信：\n\n服务无法通过Intent直接与Activity通信。\n例如，发送邮件或MP3播放任务可通过绑定（Binding）实现通信。\n\n\n保证任务完成：服务的生命周期（Lifecycle）管理至关重要。\n\n d. Notifications\n作用：\n\n提醒用户应用程序的事件，例如任务提醒或系统状态。\n需考虑用户的情绪和专注力，避免过度打扰。\n\n跨平台通知设计原则：\n\n遵循Material Design或Human Interface Guidelines。\n避免无意义通知，例如“很久没见到你了”。\n目标是创建简洁易读、用户友好的通知。\n\n与服务的关系：\n\n服务可以通过通知告知用户正在运行的任务。\nPendingIntent 允许用户通过通知返回到 Activity。\n服务可以在通知中提供交互按钮，例如暂停/停止功能。\n\n e.服务的生命周期\n\n服务的启动方式：\n\nStarted Service（启动服务）：通过startService启动，会独立运行，直到显式调用stopService停止。\nBound Service（绑定服务）：通过bindService绑定，运行于与之绑定的Activity之间的通信通道中。\n\n\n生命周期的特点：\n\n可以同时启动和绑定服务。\n不同的生命周期责任：\n\n启动服务需要开发者管理生命周期。\n绑定服务的生命周期由系统自动管理。\n\n\n\n\n终止服务：\n\n自行调用stopSelf。\n通过Intent调用stopService。\n避免终止：\n\n可将服务设置为前台服务（Foreground Service），通过startForeground提高优先级。\n\n\n\n\nonStartCommand返回值决定服务重启行为：\n\nSTART_NOT_STICKY：服务不再自动重启。\nSTART_STICKY：服务重启但不重新传递Intent。\nSTART_REDELIVER_INTENT：重启服务并重新传递上一次的Intent。\n\n\n\n f. 远程服务 Remote Service\n\nIntent的作用：用于在应用或进程之间通信，但在远程服务中需要进一步扩展。\n跨进程通信的特点：\n\n远程服务允许在不同应用/进程之间共享服务。\n需要将任务交给其他进程中的线程处理。\n可能被多个进程同时使用，因此必须声明为exported服务（在Manifest文件中定义）。\n隐式 Intent 在远程服务中不能使用（可能由于权限或安全性原因）。\n\n\n通信方式：\n\n使用Messenger：用于消息传递（异步）。\n定义接口：支持注册回调和将系统服务封装到API中以供客户端调用。\n\n\n\n g. 服务间通信（Communicating with Services）\n\nMessenger 的角色：\n\nMessenger 是服务的一个 IPC（进程间通信）接口。\n基于消息的通信模型，而不是直接的方法调用。\n支持异步通信，使用消息（Message）携带数据包。\n\n\n消息队列与线程：\n\n消息被排队到单个线程中，按顺序处理。\n使用 Handler 管理线程间的通信和并发。\n服务可以通过定义自己的 Handler 来响应不同类型的消息对象。\n\n\nIBinder 的使用：\n\nMessenger 实际上是一个带有“发送消息”功能的 IBinder。\n服务端通过 IBinder 共享与客户端的连接。\n客户端通过 IBinder 发送消息给服务。\n\n\n双向通信（Bi-directional Communication）：\n\n客户端也可以拥有自己的 Messenger。\n客户端在发送消息时提供一个返回的 Messenger 引用，以实现双向通信。\n\n\n\n h. Messenger/Handler/Looper 关系\n\n\n\n名称\n比喻\n\n\n\n\nProcesses\n将每个应用程序类比为社区中的独立房子。\n\n\nMessenger\n如果一个房子（应用中的活动）需要向另一个房子（远程服务）发送消息，它会写信交给邮递员（Messenger），邮递员将其递送到目的地。\n\n\nHandler\n负责检查信箱的人，阅读信件并根据内容采取行动。\n\n\nLooper\n定期检查信箱的例行工作，确保不会遗漏重要邮件。\n\n\n\n i. 消息传递（Message Transfer） 的机制\n\n服务端的准备（Inside the Service House）\n\n服务端在开始前，会通知邮递员（Messenger）：“如果有人要给我发送信件（消息），这是我的信箱地址，以及我喜欢的信件格式。”\n这一步相当于服务端设置了消息接收的规则和流程。\n\n\n客户端的操作（Inside the Activity House）\n\n当客户端（Activity House）想要向服务端（Service House）发送消息时：\n\n客户端写好信件（消息）并交给邮递员（Messenger）。\n由于服务端之前提供了详细的地址和说明，邮递员能够准确地将信件送达。\n\n\n\n\n回复消息的机制\n\n如果客户端希望收到服务端的回复，它需要在信件中附带回信地址（自己的 Messenger 信息）。\n这样，服务端可以通过回信地址，将响应（回复消息）送回客户端。\n\n\n\n g. Parcelable\n如果服务是绑定到同一个进程内，它们共享相同的内存空间。在这种情况下，可以轻松调用方法并传递对象或引用，效率更高。\n那么需要跨进程通信，如何传递对象？\n\n使用java.io.Serializable\n\n通过反射或自省技术写入对象ID和字段。\n如果类或变量名称发生变化，可能会导致错误。\n缺点：\n\n慢：性能较低。\n脆弱：对类定义的修改敏感。\n\n\n\n\n使用 Parcelable\n\n定义简单的协议用于写入基本数据类型。\n通过传递关键数据重新创建对象（类似深拷贝）。\n具有抗小改动能力（类定义发生小变化不会出错）。\n优点：\n\n快速：比Serializable高效得多。\n由Android内核驱动提供支持。\n\n\n适合在Android应用中使用，特别是涉及跨进程数据传递时。\n\n\n\n h. Defining Remote Interface\n通过 AIDL 定义远程接口\n\n主要功能：\n\n为服务的功能指定一个接口\nAIDL 的作用：\n\n生成一个 代理对象（Proxy Object），允许在本地使用，像远程服务是本地的一样。\n生成一个 存根实现（Stub Implementation）：负责处理远程事务的服务端。\n生成 通信协议：包括对象的序列化（parcelling）和反序列化（unparcelling）步骤，作为拷贝和重新创建对象的传输协议。\n\n\n\n\n与 Java 接口定义的相似性\n\n方法参数标签：\n\nin：传递到远程方法。\nout：返回给调用者。\ninout：既作为输入又作为输出。\noneway：异步调用。\n\n\n允许的数据类型：\n\nJava 的基本数据类型：如 int, float, boolean。\n列表（List）、映射（Map）等集合。\n实现 Parcelable 协议的类。\n\n\n\n\n作用\n\nAIDL 编译器自动为开发者生成 Java 代码。这些代码处理进程间通信（IPC）的细节，确保接口中的方法调用能够正确地分发并在目标远程进程中执行。\n当调用生成代码中的某个方法时，它会使用 Binder 框架 与远程进程通信。Binder 框架确保方法调用被正确传递到目标位置，并在需要时返回结果。\nBinder 提供了底层的 IPC 机制。AIDL 简化了 Binder 的使用，开发者通过定义熟悉的接口，而无需直接处理复杂的 IPC 细节。AIDL 自动处理大部分 IPC 的实现。\n\n\n与 Messenger 相比：\n\n支持复杂接口。\n性能高，多线程支持多客户端访问。\n支持自定义数据类型。\n支持双向通信和回调。\n\n\n\n J. IPC 进程间通信\n\n\n每个进程有自己的地址空间，提供数据隔离。\n\n\nIPC 用于实现模块化，但需要克服跨进程直接交互的限制。\n\n\nBinder作为Android的IPC核心，支持远程过程调用和数据传输。\n\nBinder简化了进程间通信。Binder是Android的底层IPC机制，通过数据打包和解包实现安全通信。\n使用 AIDL 定义通信接口，通过代理 (proxy) 和存根 (stub) 实现高效的组件交互。\n内容：\n\n调用 (Calls)： 支持一对一和一对多的简单进程消息传递（单向或双向）。\n身份识别 (Identifying)： 管理进程 ID（PID）和用户 ID（UID）。\n管理 (Managing)： 提供引用计数和跨进程对象映射功能。\n间接功能：\n\n作为一个令牌（Token）。\n共享文件描述符（File Descriptor）以实现共享内存区域。\n\n\n线程管理：控制工作线程的休眠和唤醒。\n\n\n\n\n\nBinder 实现 (Binder Implementation)：\n\nAPI 层面：\n\n使用 AIDL 和 Java API 暴露 IBinder 接口。\n提供 Parcelable 接口用于对象的序列化和反序列化。\n\n\n中间件层：\n\n管理用户空间的 Binder 框架功能。\n处理数据的序列化/反序列化以及与内核驱动的交互。\n\n\n内核驱动层：\n\n通过 ioctl 系统调用支持中间件。\n实现跨进程文件操作和内存映射。\n为每个服务应用程序提供线程池。\n\n\n\n\n\nBinder 事务 (Binder Transactions)\n\n事务启动：进程 A 调用 IBinder.transact() 请求操作。\n事务处理：进程 B 的 Binder 对象通过 onTransact() 方法接收和处理请求。\n线程管理：每个进程的线程池处理所有 IPC 请求。\n阻塞与响应：发起请求的进程 A 在 transact() 调用期间阻塞，等待进程 B 返回结果。\n\n\n\nBinder 安全性\n\nBinder 本身不负责安全，但提供了可信执行环境。\n内核通过 UID/PID 管理客户端身份。\n\n\n\nBinder 性能\n\n显式限制：\n\n每个进程的事务缓冲区大小为 1MB，限制了并发事务的数据量。\n建议保持事务数据较小。\n\n\n隐式限制：\n\n数据传输需要复制，增加了内存资源的重复使用。\n不适合传输大规模数据流。\n\n\n优化方式：\n\n使用共享内存（如 Ashmem）传递大数据。\n\n\n\n\n\n k. 系统服务 (System Services)\n\n电源管理 (Power Manager)。\n包管理器 (Package Manager)。\n位置服务 (Location Manager)。\n通知管理器 (Notification Manager)。\n蓝牙服务 (Bluetooth Service)。\n音频服务 (Audio Service)。\n\n l. 服务管理器\n\n功能：\n\n跟踪和管理所有系统服务的引用。\n允许客户端通过服务名检索远程服务的 Binder 句柄。\n\n\n特性：\n\n首个通过 Binder 注册的服务。\n仅允许可信系统服务注册（如系统、媒体服务）。\n\n\n\n 3. Broadcast Receiver\n响应来自操作系统/其他应用程序的广播消息。\n 4. Content Provider\n使数据可供其他应用程序使用/利用来自其他应用程序的数据。\n 三. 其他\n 1. Kotlin\nKotlin 是 2011 年 JetBrains 创建的语言。\n i. 为什么使用 Kotlin？\n\n现代语言特性。\n与 Java 的互操作性。\n提高开发人员的工作效率。\n\n ii. 为什么 Kotlin &gt; Java？\n\n简洁性 Conciseness\n\n显著减少样板代码 (boilerplate code)。\n例如 数据类自动生成 getter、setter 和其他实用方法\n\n\n安全功能 Safety Features\n\nNull Safety 空安全：在编译时消除 NullPointerException。\n\n默认情况下，变量不能为空值，除非显式声明为空。(加一个问号?)\n\n\nImmutability 不可变的：鼓励使用不可变的数据结构。\n\n默认情况下，Kotlin 提倡不可变性。\n被声明为 val 的变量是只读的，不能重新分配。\n这可以避免可变变量引起的意外副作用，从而使代码更安全。\n\n\n\n\n互操作性 Interoperability\n\n与现有 Java 代码无缝集成。\n允许从 Java 逐步迁移到 Kotlin。\n\n\n扩展方法\n\nKotlin 允许使用新功能扩展类，而无需从该类继承或使用 Decorator 等设计模式。\n\n\n\n iii. Coroutines 协程\n协程本质上是轻量级线程 (Lightweight Threads)。它们允许我们执行后台任务，而无需与传统线程相关的大量资源成本。协程是实现异步的一种方式。\n线程是由操作系统管理和调度的，而协程则是由程序代码自身进行管理和调度。\n与线程不同，协程由 Kotlin 运行时而不是操作系统管理，从而使其效率更高。\nSequential Asynchronous Code（顺序异步代码）：\n\n协程使异步代码看起来像同步代码，协程的特性允许我们以顺序的方式编写异步逻辑。\n这种顺序风格的代码更加直观，不会阻塞主线程，适用于高并发和 I/O 密集型任务。\n\n优点：\n\n协程支持非阻塞操作 (Non-blocking Code Execution)：\n\n在执行长时间运行的任务时，协程不会阻塞主线程，保持应用的响应能力\n非阻塞操作可以避免出现“应用无响应”（Application Not Responding，ANR）错误，这是移动应用开发中的常见问题。\n\n\n简化代码结构：\n\n避免嵌套回调和“回调地狱”（Callback Hell）。\n\n协程提供了更直观的方式来处理异步操作，而不需要依赖复杂的嵌套回调。\n\n\n以顺序方式编写代码：\n\n协程的代码结构更接近同步代码，简化了逻辑流程，提升了可读性和维护性。\n\n\n\n\n可扩展并发性 (Scalable Concurrency)\n\n高效处理多任务并发：协程能够高效地管理和执行大量并发任务，且资源开销极小。\n便于应用扩展：相比传统线程，协程更容易管理并发任务，尤其适用于需要高并发的场景。\n\n\n\n 2. Jetpack Compose\nJetpack Compose 是一个现代工具包，用于通过声明式方式构建原生 Android 用户界面。完全使用 Kotlin 构建，紧密集成，利用现代语言特性实现简洁和可表达的代码。\n是基于 Jetpack 的一个工具，基于模块化组件，组成成分有：\n\nFoundation：提供核心工具，例如AppCompat\nArchitecture：提供生命周期感知组件，例如 ViewModel, LiveData 和 Room\nUI：包括用于导航、Fragment和动画的工具。\nBehavior：提供后台任务和数据处理的工具。\n\n特性：\n\n使用 Kotlin 编写 UI 和业务逻辑，无需切换上下文。\n支持 实时预览 (Live Previews) 和 热重载 (Hot Reloading)，在开发过程中实时更新 UI。\n使用Composable 函数：通过函数式组件减少冗余代码。\n无需 XML 布局和 findViewById，可直接引用 UI 组件。\n\n优点：\n\n提高生产力 (Improved Productivity)\n\n减少样板代码：减少开发中的重复性代码，提高应用质量。\n直观的 API：采用声明式编程风格，学习曲线平滑，更易于掌握和使用。\n\n\n更高的性能 (Better Performance)：\n\n高效的渲染 (Efficient Rendering)：仅更新受影响的组件，避免不必要的重绘，从而提升性能。\n异步处理 (Asynchronous Handling)：提供流畅的 UI 交互体验，适配现代异步工作流。\n\n\n轻松集成 (Easy Integration)：\n\n与现有视图兼容 (Interoperable with existing Android views)\n向后兼容：支持旧版本的 Android，同时提供生命周期感知的功能。\n\n\n\n缺点：\n\n关注点分离减少 (Reduced Separation of Concerns)：\n\n在 Composable 函数中将布局和逻辑结合在一起，可能会模糊 UI 结构与功能代码之间的界限。\n这可能会增加隔离特定部分的难度。\n\n\n复杂 UI 中的复杂性增加 (Increased Complexity in Large UIs)：\n\n对于复杂的用户界面，将布局和内容结合在同一代码中，可能会导致函数变得庞大且难以阅读。\n开发者需要将 UI 拆分成更小、更可重用的组件以提高可读性。\n\n\n文件体积膨胀的可能性 (Potential for Large Files)：\n\n由于将 UI 和逻辑放在一起，文件可能变得非常庞大，尤其是在大型应用中。\n查找特定组件或逻辑可能会变得困难。\n\n\n\n i. Composable Function\n使用 @Composable 注解，用于定义 UI 组件。\n\n声明式 UI 定义：描述 UI 应该显示什么，而不是如何构建。\nComposable 函数可以嵌套，并组合起来构建复杂的用户界面。\n\nComposables 中管理状态：\n\n维护状态以跨越多次重组 (recomposition)。\n响应用户交互和数据变化，动态更新 UI。\n\n状态管理工具：\n\nremember：用于在相同的 Composable 中保持状态，避免因重组丢失数据。\nmutableStateOf：持有一个可变的值，当值发生变化时会触发 UI 的重新绘制 (recomposition)。\n\n 3. 零日攻击 (Zero-day Attack)\n零日攻击是一种利用软件漏洞的攻击行为，这些漏洞对于供应商或开发者来说是未知的。“零日”意味着开发者在漏洞被利用之前，没有任何时间（0天）来修复这个问题。\n特点：\n\n未被发现的漏洞 (Undetected Vulnerabilities)：\n\n攻击者利用在安全团队或开发人员发现之前存在的漏洞。\n这些漏洞通常处于未知状态，未被公开\n\n\n高风险 (High Risk)：\n\n在漏洞被修复之前，攻击者可能造成严重的损害。\n高危攻击可能泄露敏感信息、破坏系统或造成经济损失。\n\n\n隐蔽性 (Stealthy Nature)：\n\n零日攻击往往具有高度隐蔽性，通常在攻击发生后才被察觉。\n由于未被识别，受害者可能无法立即采取防御措施。\n\n\n\n 4. 线程执行\n I. 单线程模型：\n\n主线程 (Main Thread)：Android 应用程序默认使用单线程模型，所有组件（Activity、Service、BroadcastReceiver 等）都在主线程上运行。\n主线程启动时，负责处理：\n\n用户界面的绘制。\n响应用户交互事件（如 onClick()）。\n处理 Activity 生命周期事件（如 onCreate()、onDestroy()）。\n\n\n适合主线程的任务：\n\n短时间的 UI 更新。\n生命周期事件的调度。\n\n\n不适合主线程的任务：\n\n网络请求。\n文件读写。\n数据库操作。\n\n\n\n II. 线程管理：\n\n避免在主线程运行耗时任务：\n\nAndroid 会抛出 NetworkOnMainThreadException，阻止在主线程上执行网络操作。\n耗时任务会导致应用卡顿甚至无响应（ANR：Application Not Responding）。\n\n\n推荐的做法：\n\n使用辅助线程（如 Thread 或 ExecutorService）。\n使用异步工具类（如 AsyncTask，但已被弃用）。\n使用现代 Android 框架（如 Coroutines 或 WorkManager）。\n\n\n\n III. Looper and Handle\nHandlerThread：HandlerThread 是对普通线程的扩展，支持 Looper。它是一个后台线程，与主线程分离。提供一个线程，并自动管理与该线程相关的 Looper 和消息队列（MessageQueue）。通常用于在后台线程中处理任务。\nLooper：每个 HandlerThread 都包含一个 Looper，Looper 维护了一个消息队列（MessageQueue），用于管理事件队列。Looper.loop() 运行循环，从队列中取出消息并调度执行。\nMessage：消息是需要处理的任务，可以包含数据或者对某个 Runnable 对象的引用。是线程间通信的基础单元。消息队列中的每个消息会被 Looper 分发给相应的 Handler 进行处理。\nHandler：Handler 绑定到某个 Looper 上，负责与 Looper 的消息队列交互。用于向线程的消息队列发送消息或任务。常用于在辅助线程中执行任务后，将结果发送到主线程更新 UI。Handler 是线程安全的，可以从不同线程发送消息到绑定的 Looper。\n 四. 代码相关\n 1. 活动类 AppCompatActivity\n\n生命周期管理：\n\n\n\n\n方法名\n调用时机\n主要用途\n\n\n\n\nonCreate()\n创建时调用。是生命周期的起点。\n一般用于设计布局、初始化控件、配置数据等。\n\n\nonStart()\n启动时调用，即用户即将看到它，但还没有与用户互动时。\n即将于用户交互时调用。是活跃状态。\n\n\nonResume()\n即将于用户交互时调用。是活跃状态。\n常用于恢复onStop中被释放的资源。\n\n\nonPause()\n失去焦点时调用。\n一般用于暂停动画、保存数据或释放资源。\n\n\nonStop()\n完全不可见时调用。比如说被另一个Activity全屏覆盖或应用被最小化。\n可以释放更多资源。\n\n\nonRestart()\n从停止变回可见状态时调用。此方法在onStart()之前被调用。\n释放掉所有的资源。\n\n\nonDestroy()\n被摧毁之前调用。是生命周期的终点。\n释放掉所有的资源。\n\n\n\n\n子活动相关：\n\n\n\n\n方法\n说明\n样例\n\n\n\n\nIntent getIntent()\n获取启动激活的intent。没有则返回Null。\nvar intent = getIntent()\n\n\nregisterForActivityResult(ActivityResultContracts&lt;T, O&gt;, Callback)\n用于注册ActivityResultLauncher。\n\n\n\nsetResult(RESULT, Intent)\n返回启动它的Activity，并返回结果\nRESULT_OK: 操作成功RESULT_CANCELED: 操作取消RESULT_FIRST_USER: 自定义结果代码\n\n\n\n\n其他辅助类：\n\n\n\n\n方法\n说明\n样例\n\n\n\n\nView findViewById()\n根据控件id寻找控件\nvar a = findViewById(R.id.a);\n\n\n\n 2. 视图类 View\n 列表视图 ListView\n 回收视图 RecyclerView\n可以动态创建并回收视图，提升性能。\n 3. 资源类 R\n\n\n\n子类\n说明\n样例\n\n\n\n\nlayout\n当前布局相关信息\nR.layout.activity_main\n\n\nid\n控件id相关\nR.id.buttonCalculate\n\n\n\n 4. 意图类 Intent\n用于给其他的 Activity 传递参数。\n\n\n\n方法\n说明\n样例\n\n\n\n\nputExtra(key, value)\n传递数据。\nintent.putExtra(&quot;result&quot;, 10);\n\n\ngetStringExtra(key)\n根据key获取字符串数据。\nname = intent.getStringExtra(&quot;name&quot;);\n\n\n\n\n父类打开子类，并返回给子类信息。\n子类返回父类，并返回给父类信息。\n需要用到 startActivityForResult(Intent, int)，或者 Activity Result API 的 ActivityResultLauncher 打开子活动。\n\n","slug":"笔记/安卓开发/安卓开发","date":"2024-12-01T10:00:00.000Z","categories_index":"笔记-安卓开发","tags_index":"Android","author_index":"zExNocs"},{"id":"394fd5a3335536c852ff71e4cfa7feb9","title":"问题-图染色","content":" 一. 问题描述\n给定一个无向无权图 G=(V,E)G = (V, E)G=(V,E)，其中 VVV 是顶点集，EEE 是边集，此外给定 kkk 种颜色。或者给每一个顶点 v∈Vv \\in Vv∈V 拥有一个颜色集合 L(v)L(v)L(v)，顶点 vvv 只能在该集合中选择颜色。\n问题是让所有的顶点都拥有一个颜色，并且没有两个相邻的顶点颜色相同。\n","slug":"笔记/问题/图染色","date":"2024-10-18T10:00:53.000Z","categories_index":"笔记-问题","tags_index":"Vertex Colouring Problem,Graph Theory","author_index":"zExNocs"},{"id":"231b0a29aaffe1d85196dc478b61b9e8","title":"问题-可满足性问题 SAT","content":"&lt;返回问题导航\n\n 一. 定义\n布尔可满足性问题(Boolean satisfiability problem, SAT) 属于决定性问题，也是第一个被证明属于 NP-Complete 的问题。\n问题：给定 CNF 公式，找到满足该命题公式的解释或证明该命题不可满足。\n 1. 问题分析\n令 SSS 为基于 CNF 的知识库，SSS 是有限数量的命题变量。\n尝试所有可能的组合值，如果找到一个组合使得 SSS 满足，则解决了这个问题。否则 SSS 是不可满足的。\n但事实上，我们不需要枚举出所有可能的组合。一旦我们有了部分分配，我们可能就已经能够得到结论，这种部分分配不满足 SSS，例如 S=α∧βS = \\alpha \\land \\betaS=α∧β，那么在 α=False\\alpha = Falseα=False 时无论 β\\betaβ 为什么都不会满足 SSS，此时就无需枚举 β\\betaβ 的组合。\n 二. 解决算法\n 1. DPLL 算法\nDPLL 是基于深度优先搜索 (DFS) 的：\n\n选择一个命题变量文字，将搜索分为两个分支：当变量为 True 时和 False 时。\n根据值进行简化公式。如果简化后的公式包含空句，那么它是不可满足的，无需在此分支进行进一步搜索。\n\nDPLL算法描述：\n\n\n\n步骤\n说明\n\n\n\n\n函数名\nDPLL(S)\n\n\n输入\n子句的集合 SSS\n\n\n输出\n可满足返回 True否则返回 False\n\n\n111\n如果 [ ]∈S[\\ ] \\in S[ ]∈S，返回 False\n\n\n222\n如果 SSS 为空，返回 True\n\n\n333\n选择一个文字 A∈SA \\in SA∈S\n\n\n444\n令 S' = Propagate(S, A)\n\n\n555\n令 S'' = Propagate(S, !A)\n\n\n666\n返回 DPLL(S') | DPLL(S'')\n\n\n\nPropagate(S, A) 函数：\n\n\n\n步骤\n说明\n\n\n\n\n函数名\nPropagate(S, A)\n\n\n输入\n子句的集合 SSS，文字 AAA\n\n\n输出\n一个新集合 S′S&#x27;S′\n\n\n111\n初始化 S′=∅S&#x27; = \\varnothingS′=∅\n\n\n222\n遍历子句 c∈Sc \\in Sc∈S：\n\n\n333\nif\\text{if}if A∈cA \\in cA∈c，那么跳过该句子因为此时 AAA 让整个 ccc 为真\n\n\n444\nelif\\text{elif}elif A‾∈c\\overline{A} \\in cA∈c，那么让 S′=S′∪{c−A‾}S&#x27; = S&#x27; \\cup \\{c - \\overline{A}\\}S′=S′∪{c−A}因为 AAA 对 ccc 的评估没有影响了\n\n\n555\nelse\\text{else}else S′=S′∪{c}S&#x27; = S&#x27; \\cup \\{c\\}S′=S′∪{c}该子句不受到 AAA 分配的影响\n\n\n\nDPLL 也实现了一些加速：\n\n\n如果一个子句只包含一个文字，那么就将这个文字设置为 True\\text{True}True。\n\n例如 {[A],[B,C]}\\{[A], [B, C]\\}{[A],[B,C]} 中可以令 A=TrueA = \\text{True}A=True\n\n\n\n如果某个命题变量在所有子句中仅出现某一个极性 (polarity) 中，例如只出现 AAA 或者只出现 A‾\\overline{A}A，那么我们就将这个命题变量设置为 True\\text{True}True (正极性) 或者 False\\text{False}False (负极性)。\n\n例如 {[A,B],[A,C]}\\{[A, B], [A, C]\\}{[A,B],[A,C]} 中可以令 A=TrueA = \\text{True}A=True\n\n\n\n算法极大地依赖于选择哪个变量进行赋值，即 DPLL(SSS) 中的第 3 行。使用智能启发式算法来选择变量 AAA。\n\n\n 2. 命题逻辑的归结系统\n具体查看 命题逻辑 文章。\n\n&lt;返回问题导航\n","slug":"笔记/问题/可满足性问题SAT","date":"2024-10-18T10:00:20.000Z","categories_index":"笔记-问题","tags_index":"SAT,Satisfiability Problem,Boolean Satisfiability Problem","author_index":"zExNocs"},{"id":"fe24996d1d2428bc217d0b396f632e99","title":"问题-规划 Planning","content":"&lt;返回问题导航\n\n 一. 定义\n规划是一种推理问题，确定需要执行哪些动作（计划）以使某些条件（目标）为真。\n规划是人工智能的核心：作为通过计算手段研究智能行为的重要领域。\n总体上，规划是一个非常复杂的现实世界问题：解决现实中的规划问题通常充满挑战。\n 二. 例子\n 1. 经典规划\n经典规划(Classical Planning)方法为了简化问题，作出了一些假设：\n\n\n\n假设\n说明\n\n\n\n\n确定的\n每个动作的结果都是确定的\n\n\n可观察的\n代理能够完全感知环境的状态\n\n\n静态的\n环境只会因代理的动作而改变，不受外部因素的干扰\n\n\n\n i. 规划领域定义语言（Planning Domain Definition Language）\n组成成分：\n\n\n\n成分\n说明\n\n\n\n\n系统状态\n使用流项（fluents）描述，可以是正的或负的谓词\n\n\n目标Goals\n流项的合取\n\n\n动作模式Action schemas\n定义可以改变系统状态的原子操作\n\n\n\n每个动作模式由以下部分组成：\n\n\n\n成分\n说明\n\n\n\n\n前置条件Precondition\n动作发生所需满足的条件一个流项的列表\n\n\n效果Effect\n动作对系统状态的改变包括一个删除列表（delete list）和一个添加列表（add list）的流项\n\n\n\n ii. 规划问题（Planning Problem）\n\n\n\n成分\n说明\n\n\n\n\n定义\n给定一个规划领域和一个目标\n\n\n目标\n找到一系列动作，将初始状态转换为满足目标的状态\n\n\n解决方法\n可以使用搜索算法来求解\n\n\n\n 2. 贝叶斯网络（Bayesian Networks）\n\n\n现实生活中的不确定性：\n\n如果 A 是去年参加了 C++ 模块考试的学生，那么 A 很可能通过了 C++ 考试。\n如果某人 A 发烧且咳嗽，那么 A 很可能患有新冠肺炎。\n\n\n\n概率知识库：\n\n使用 P(x1,x2,...,xN)P(x_1, x_2, ..., x_N)P(x1​,x2​,...,xN​) 表示关联 N 个事实的概率函数。\n其中 xix_ixi​ 表示第 i 个事件的概率\n此函数展示了整个系统的联合概率分布。\n\n\n\n挑战：\n\n当 NNN 很大时，此函数会变得非常复杂。\n同时，也不清楚如何构建或具体化该函数。\n\n\n\n网络结构：\n\n节点(Node)\n概率\n\n\n&lt;返回问题导航\n","slug":"笔记/问题/规划","date":"2024-10-18T10:00:15.000Z","categories_index":"笔记-问题","tags_index":"Planning,Bayesian Network","author_index":"zExNocs"},{"id":"9b0cbed62cb3ddeef03b83c99d21ae2c","title":"SAI-编程库","content":"&lt;返回符号人工智能导航\n\n 一. Z3 python库\n 1. 安装\npip install z3-solver\n 2. 使用 smt2 作为z3的输入\nsmt2作为输入的python样例：\n1234567891011121314smt2program = &quot;&quot;&quot;(declare-const A Bool)(declare-const B Bool)(assert (and (xor A B) A))&quot;&quot;&quot;import z3s = z3.Solver()                           # z3解算器s.add(z3.parse_smt2_string(smt2program))  # 将smt2解析成约束条件，并添加到解算器中status = s.check()                        # 获取是否可满足print(status)                             # 打印可满足状态if status == z3.sat:                        print(s.model())                        # 打印模型\n输出结果：\n12sat[A = True, B = False]\n其中变量 smt2program 内容就是smt2的语法：\nsmt2 采用 括号前缀表示法 (parenthesised prefix notation)，由最早的函数式编程语言之一 LISP 使用而闻名。在这种表示法中，每对括号都是一个函数调用，括号中的第一个单词是函数名称，其余都是参数。形式如：(&lt;function name&gt; &lt;parameter 1&gt; &lt;parameter 2&gt; ...)\n\n\n\n函数\n描述\n例子\n\n\n\n\n注释\n;\n; Hello\n\n\nand\n逻辑和，可以用两个或多个参数\n(and A B C)\n\n\nor\n逻辑或，可以用两个或多个参数\n(or A B C)\n\n\nnot\n逻辑否\n(not A)\n\n\n=&gt;\n推导 (Implication)，即→\n(=&gt; A B)\n\n\n=\n等于，即 ⇐⇒\n(= A B)\n\n\ndeclare-const\n声明一个变量。第一个参数是变量名称，第二个参数是变量类型\n(declare-const A Bool)\n\n\ndeclare-fun\n声明一个函数。第一个参数是函数名；第二个参数是一个括号组，内部分别是函数参数类型；第三个参数是返回类型\n(declare-fun square (Int Int) Int)\n\n\ndeclare-datatype\n声明一个枚举 (enumeration) 数据类型。第一个参数是数据类型名；第二个参数是一个括号组，其中第一个参数是数据类型名；第二个参数是一个构造函数的括号组，内部第一个参数是构造函数名，其余是变量类型\n(declare-datatypes () ((Animal dog cat fox)))\n\n\nforall\n全称量词。第一个参数是一个变量的括号组；表示变量和变量类型；第二个参数是谓词函数\n(forall ((x MyType) (y MyType)) (P x y))\n\n\nexists\n存在量词。第一个参数是变量括号组；第二个参数是谓词函数\n(exists ((x MyType) (y MyType)) (P x y))\n\n\nassert\n一个参数，断言该语句为 true\nassert A\n\n\n\n 3. 使用python直接作为z3的输入\n相关api如下，或参考该网站：\n\n\n\n类型\napi\n解释\n例子\n\n\n\n\n结算器\nSolver()\n获取结算器\ns = z3.Solver()\n\n\n变量声明\n布尔：Bool(name)整型：Int(name)\nname是一个字符串，表示变量名字\nA = z3.Bool('A')a = z3.Int('a')\n\n\n变量类型\n布尔：BoolSort()整型：IntSort()\n用于函数类型声明\n\n\n\n布尔操作符\n逻辑与：And(?, ??, ??)逻辑或：Or(??, ??, ..., ??)逻辑否：Not(??)逻辑推理：Implies(??, ??)逻辑等于：?? == ??\n返回True或者False\nAnd(A, B, C)A == B\n\n\n非布尔操作符\n1. 可以直接使用Python操作符 +, &gt;, &gt;= 等2. 求和：z3.Sum([])，参数是数组3. 三元运算符：z3.If(A, B, C)，相当于 A ? B : C\n返回数值\na + b\n\n\n函数\nFunction(name, paraType, ..., paraType, returnType)\n函数名、参数类型、返回类型类型使用变量类型\n1. OnTop = Function('OnTop', IntSort(), IntSort(), BoolSort())2. s3.add(OnTop(A, B))\n\n\n断言/公式输入\nSolver().add(&lt;formula&gt;)\n每一个 add 相当于加入一个断言\ns.add(And(A, B))\n\n\n推理\nSolver().check()\n返回是否满足1. z3.sat 满足2. z3.unsat 不满足\nif s.check() == z3.sat:\n\n\n模型输出\nSolver().model().model().eval(??)\n1. 返回符合断言的一种模型2. 返回模型里的特定变量，??是python的变量\nprint(s.model())print(s.model().eval(A))\n\n\n量词\nForAll(list, fun(param, ..., param))Exists(list, fun(param, ..., param))\n1. list 是量词参数，使用的应该是python的变量2. fun 是定义的Function()函数\nx = z3.Int('x')y = z3.Int('y')P = z3.Function('P', z3.IntSort(), z3.IntSort(), z3.BoolSort())s.add(z3.ForAll([x, y], P(x, y)))\n\n\n枚举型\n创建type = Datatype(type_name)type.declare(param_name)type = type.create()访问type.param_nametype.constructor(int)()z3.Const(const_name, type)\n创建类型及其应用\n\n\n\n\n 4. 技巧\n\n证明蕴含(entailment):\n\nZ3只能证明可满足性，即是否存在某一种例子可以使得所有断言都成立，也可以说是 ∃J,J⊨S∧α\\exists J, J \\models S \\land \\alpha∃J,J⊨S∧α。\n但是无法直接证明蕴含，即 ∀J,J⊨S→J⊨α\\forall J, J \\models S \\rightarrow J \\models \\alpha∀J,J⊨S→J⊨α。\n简单的说就是只能证明存在，不能证明全部。\n此时我们可以将待查证的公式 S⊨αS \\models \\alphaS⊨α 变成 S∧¬αS \\land \\neg \\alphaS∧¬α 不可满足，将我们想要证明的公式 α\\alphaα 反转 ¬α\\neg \\alpha¬α 再证明是否是不可满足的即可。\n\n增加语句的数量/减少限制从而提高效率\n\n例如要想一个队列中不能有重复的数，可以想到语句为 ∀x,y,x≠y→list(x)≠list(y)\\forall x, y, x \\neq y \\rightarrow list(x) \\neq list(y)∀x,y,x=y→list(x)=list(y)\n很明显这个语句的声明复杂度是 Θ(n2)Θ(n^2)Θ(n2)\n可以修改为 mi≠mk,∀i&lt;km_i \\neq m_k, \\forall i &lt; kmi​=mk​,∀i&lt;k，其中 1&lt;k≤n1 &lt; k \\leq n1&lt;k≤n 进行枚举。\n例如长度为 3 的队列中，语句表示为：\nk=2,m1≠m2k = 2, m_1 \\neq m_2k=2,m1​=m2​\nk=3,m1≠m3,m2≠m3k = 3, m_1 \\neq m_3, m_2 \\neq m_3k=3,m1​=m3​,m2​=m3​\n时间复杂度为 O(n2)O(n^2)O(n2)，效率要比 Θ(n2)Θ(n^2)Θ(n2) 好得多。\n\n考虑代表表：\n\n\n一旦决定一个代表解的时候：\n\n引入符号(notation)\n定义规则\n定义参数\n\n\n\n公式顺序为：\n\n辅助符号 NNN，例如集合，在公式上方定义。\n每项规则都单独书写的定义文。\n如果引用参数，每个参数的数域范围要在定义域右侧定义。\n\n例如 mi≠mk,∀i&lt;km_i \\neq m_k, \\forall i &lt; kmi​=mk​,∀i&lt;k，其中 i,ki, ki,k 是参数而不是限制。\n\n\n公式索引在括号中给出，以供以后引用（即使从未引用过这个公式，也给出索引）\n\n使用 equation (i) 引用单个公式。\n使用 formulation (i)-(j) 引用多个公式。\n\n\n非逻辑符号定义在公式末尾。\n\n包括定义域 D\\mathcal{D}D，例如 D=N\\mathcal{D} = \\mathbb{N}D=N\n\n\n公式中的一行行在公式下面公式下方进行解释。\n\n例如 equation (1) define ... rules\n\n\n\n\n\n 二. OR-Tools python库\n 1. 安装\npip install ortools\n 2. 相关api\n\n\n\n类型\napi\n解释\n样例\n\n\n\n\n模型\ncp_model.CpModel()\n模型，用于输入限制\nmodel = cp_model.CpModel()\n\n\n求解器\ncp_model.CpSolver()solver.Solve(model)solver.Value(var)\n获取求解器和求解器cp_model.OPTIMAL: 最佳的cp_model.FEASIBLE: 可行的\nsolver = cp_model.CpSolver()status = solver.Solve(model)\n\n\n变量声明\n整型: model.NewIntVar(min, max, name)布尔: model.NewBoolVar(name)\nmin 最小值, max 最大值,name 是变量名\n整型: x = model.NewIntVar(0, 10, 'x')布尔: a = model.NewBoolVar('a')\n\n\n一般限制\nmodel.Add(constraint)\n限制可以是 python 相关语法，不能添加变量。如果要添加某个变量为 true，应该写为 a == 1\nmodel.Add(x &gt; y)\n\n\n推理限制\nmodel.Add(con).OnlyEnforceIf(a)model.AddImplication(a, b)（bool only）\n如果 a 成立，那么 con 才成立等价于 a → con只能用于在线性限制中，如 Add(), AddBoolOr, AddBoolAnd\nmodel.Add(x &gt; y).OnlyEnforceIf(a)\n\n\n不等于限制\nmodel.AddAllDifferent([])\n[] 内部的所有变量都不相同\nmodel.AddAllDifferent([a, b, c])\n\n\n布尔限制\n或: model.AddBoolOr([])否定: bool_var.Not()\n[] 内部的所有变量使用或运算\nmodel.AddBoolOr([a, b, c])\n\n\n最大最小值限制\n最大值: model.AddMaxEquality(var, list)最小值: model.AddMinEquality(var, list)\nvar 是 list 中的最大或者最小值\nmodel.AddMaxEquality(a, [b, c, d])\n\n\n属于内部值限制\nmodel.AddAllowedAssignments(list, set)\nlist 所有值必须在 set 中\nmodel.AddAllowedAssignments([a, b], [c])\n\n\n\n 3. 技巧\n\n布尔变量与语句链接：\n\n布尔无法直接赋值为相关符号操作，例如 a == (b &gt; c) 是错误的。\n布尔应该使用 ⟺\\Longleftrightarrow⟺ 进行赋值。如上一个例子，可以转化为 a⟺(b&gt;c)a \\Longleftrightarrow (b &gt; c)a⟺(b&gt;c)\nOrTool 中没有 ⟺\\Longleftrightarrow⟺，但是有 →\\rightarrow→。那么上述的代码就可以为：\n\n\n\n12model.Add(b &gt; c).OnlyEnforceIf(a)model.Add(b &lt;= c).OnlyEnforceIf(a.Not())\n\n可以使用 sum 计算布尔值为 True 的数量。\n\n\n&lt;返回符号人工智能导航\n","slug":"笔记/符号人工智能/编程库","date":"2024-10-18T10:00:12.000Z","categories_index":"笔记-符号人工智能","tags_index":"Symbolic Artificial Intelligence,z3,python,Or-Tools","author_index":"zExNocs"},{"id":"fe7944109dd5437e82fba961634fce51","title":"SAI-描述逻辑","content":"&lt;返回符号人工智能导航\n\n 一. 介绍\n描述逻辑（Description logics, DLs）是围绕对象的类别构建的。描述逻辑具有类似集合的特性：\n\n\n\n符号\n说明\n\n\n\n\n∪\\cup∪\n类别的并集\n\n\n∩\\cap∩\n类别的交集\n\n\n⊆\\subseteq⊆\n子类\n\n\n\n优点：\n\n直观\n无变量\n限制充分，以支持高效推理\n\n 二. 带补集的属性概念语言 ALC\n带补集的属性概念语言 (Attributive Concept Language with Complements, ALC) 是描述逻辑的一种。\n 1. ALC 语法\n语法包括逻辑符号和非逻辑符号。\n\n逻辑符号\n\n\n\n\n类型\n符号\n\n\n\n\n概念生成操作符\n∀,∃,∪,∩,¬\\forall, \\exists, \\cup, \\cap, \\neg∀,∃,∪,∩,¬\n\n\n连接符\n⊆,≡\\subseteq, ≡⊆,≡\n\n\n断言操作符\n:::\n\n\n括号\n( )(\\ )( )\n\n\n\n\n非逻辑符号\n\n\n\n\n类型\n说明\n表示\n\n\n\n\n概念Concepts\n1. 也称为类，是个体的集合2. 相当于一阶逻辑（FOL）中的一元谓词\n以大写字母开头例如：Person 表示人Dog 表示狗\n\n\n个体Individuals\n1. 是概念的具体实例 (元素)2. 相当于一阶逻辑（FOL）中的零元函数（常量）\n以小写字母开头例如 bottle 表示一个瓶子个体\n\n\n角色Roles\n1. 定义概念之间的关系2. 一个概念的个体经过角色映射后会获得另一个概念的个体的集合3. 相当于一阶逻辑（FOL）中的二元谓词\n以小写字母开头例如 child 表示个体的孩子映射\n\n\n\n 2. ALC 个体概念定义方式\n一个个体的概念 (Concept) 可以通过以下方式定义，以下 rrr 表示角色，C,C′C, C&#x27;C,C′ 表示概念：\n\n\n\n定义方式\n说明\n\n\n\n\n原子概念Atomic concept\n1. 一个命名的概念2. 例如 Person 表示 人\n\n\nC1≡C2C_1 ≡ C_2C1​≡C2​\n1. 概念 C1C_1C1​ 与 C2C_2C2​ 等价2. 表示 C1C_1C1​ 下所有的个体也是 C2C_2C2​ 的个体\n\n\n∀r. C\\forall r.\\ C∀r. C\n1. 表示个体经过关系 rrr 后所有的个体属于概念 CCC2. 例如 ∀child. Girl\\forall \\text{child}.\\ \\text{Girl}∀child. Girl 表示该个体所有子女都是女孩\n\n\n∃r. C\\exists r.\\ C∃r. C\n1. 表示个体经过关系 rrr 后的个体存在属于概念 CCC3. 例如 ∃child. Girl\\exist \\text{child}.\\ \\text{Girl}∃child. Girl 表示该个体存在子女是女孩\n\n\n¬C\\neg C¬C\n1. 表示非 CCC 的概念2. 例如 ¬Female\\neg \\text{Female}¬Female 表示非女性3. 如果个体包含家具，那么一张桌子也可以属于 ¬Female\\neg \\text{Female}¬Female\n\n\nC∩C′C \\cap C&#x27;C∩C′\n表示概念的交集\n\n\nC∪C′C \\cup C&#x27;C∪C′\n表示概念的并集\n\n\n\n 3. ALC 的量词形式化解释\n假设\n\nC1,C2C_1, C_2C1​,C2​ 是概念，rrr 是一个角色\nP1(x)P_1(x)P1​(x) 和 P2(x)P_2(x)P2​(x) 对应概念 C1C_1C1​ 和 C2C_2C2​ 在 FOL 下的谓词，R(x,y)R(x, y)R(x,y) 是角色 rrr 的谓词\n\n则：\n\n\n\n符号\n解释\n\n\n\n\nC2≡∀r. C1C_2 ≡ \\forall r.\\ C_1C2​≡∀r. C1​\n1. 如果个体 xxx 属于概念 C2C_2C2​，当且仅当所有 xxx 经过关系 rrr 相连的对象 yyy 都属于概念 C1C_1C1​2. 等价于 ∀x. (P2(x)  ⟺  ∀y.(R(x,y)→P1(y)))\\forall x.\\ (P_2(x) \\iff \\forall y. (R(x, y) \\to P_1(y)))∀x. (P2​(x)⟺∀y.(R(x,y)→P1​(y)))\n\n\nC2≡∃r. C1C_2 ≡ \\exists r.\\ C_1C2​≡∃r. C1​\n1. 如果个体 xxx 属于概念 C2C_2C2​，当且仅当所有 xxx 经过关系 rrr 相连的对象 yyy 存在属于概念 C1C_1C1​2. 等价于 ∀x. (P2(x)  ⟺  ∃y.(R(x,y)→P1(y)))\\forall x.\\ (P_2(x) \\iff \\exists y. (R(x, y) \\to P_1(y)))∀x. (P2​(x)⟺∃y.(R(x,y)→P1​(y)))\n\n\n\n 4. ALC 的句子\n句子(Sentences) 定义了概念和/或角色之间的关系。\n\n新概念的定义：例如 HasDaughter  ⟺  ∃child. Female\\text{HasDaughter} \\iff \\exists \\text{child}.\\ \\text{Female}HasDaughter⟺∃child. Female 表示一个个体由女儿的概念\n公理(Axioms)：表示必须成立的陈述。例如 Mother⊆Female\\text{Mother} \\subseteq \\text{Female}Mother⊆Female\n\n 5. ALC 断言\n\n\n\n类型\n说明\n\n\n\n\n概念断言Concept assertions\nind:Concept\\text{ind} : \\text{Concept}ind:Concept表示个体属于类\n\n\n角色断言Role assertions\n(ind1,ind2:role)(\\text{ind}_1, \\text{ind}_2 : \\text{role})(ind1​,ind2​:role)表示个体之间的关系\n\n\n\n 6. TBOX 和 ABOX\n\n\n\n类型\n描述\n实现\n类比\n\n\n\n\n项声明TBOX\n描述适用于整个知识库的事实——与具体个体无关\n使用句子实现\n可类比于数据库中的模式（schema）\n\n\n断言声明ABOX\n关于具体个体的知识——与一般知识无关\n使用断言实现\n可类比于数据库中的表内容\n\n\n\n 7. ALC 的推理系统\nALC的解释、蕴含和可满足性定义域 FOL 类似。\n\nALC的设计使其是可判定的。\n可满足性的证明是 NP完全问题。\n一些变体具有更好的性能保证\n一些扩展在一般情况是不可判定的\n\n\n&lt;返回符号人工智能导航\n","slug":"笔记/符号人工智能/描述逻辑","date":"2024-10-18T10:00:04.000Z","categories_index":"笔记-符号人工智能","tags_index":"Symbolic Artificial Intelligence,Description logics,Attributive Concept Language with Complements,ALC,DL","author_index":"zExNocs"},{"id":"c737a565fb398403e5aa2113085bdc9f","title":"SAI-限制满足问题CSP","content":"&lt;返回符号人工智能导航\n\n 一. 介绍\n限制满足问题(Constraint Satisfaction Problem, CSP) 是一个 SAT 问题的扩展。但同时 CSP 也是一种描述知识的语言。\n特点：\n\n对于有限的定义域中，其表达能力等价于 SAT (CNF)\n复杂的知识可以紧凑地表达出来\n拥有有效的求解器\n\n 1. 定义\nCSP 的组成成分：\n\n\n\n成分\n符号\n说明\n\n\n\n\n定义域\nD1,D2,…D_1, D_2, \\dotsD1​,D2​,…\n1. 规定变量的范围，每个变量一个定义域2. 可以是有限也可以是无限3. SAT 问题的定义域则是 {True,False}\\{\\text{True}, \\text{False}\\}{True,False}\n\n\n范围Scope\nS={x1,x2,… }S = \\{x_1, x_2, \\dots\\}S={x1​,x2​,…}\n1. 是变量的集合2. 用于描述限制3. xi∈Dix_i \\in D_ixi​∈Di​\n\n\n约束的集合Constraints\nCCC\n对于每一个变量 xi∈Dix_i \\in D_ixi​∈Di​ 找到一个值使得约束 CCC 满足，或证明这个值不存在\n\n\n\n 2. 约束的描述形式\n约束是一个或多个变量的关系 (Relation)。也就是说，一个特定组合的变量（范围）是否满足它。\n\n约束定义方法：\n\n\n\n\n定义\n说明\n\n\n\n\n紧凑定义Compact definition\n1. 使用公式的方式表示2. 更偏向于使用紧凑定义\n\n\n使用表格Tabular\n1. 表格法只能够定义在有限的定义域中2. 表格定义通常不切实际。\n\n\n\n\n例子：\n\n对于两个范围 “不相等” 约束，可以被描述为：\n\n\n\n定义\n描述\n\n\n\n\n范围Scope\n{x1,x2}\\{x_1, x_2\\}{x1​,x2​}\n\n\n紧凑定义Compact definition\nx1≠x2x_1 \\not = x_2x1​=x2​\n\n\n\n该问题的表格定义：\n\n\n\nx1\\x2x_1 \\backslash x_2x1​\\x2​\n111\n222\n333\n\n\n\n\n111\n−-−\n+++\n+++\n\n\n222\n+++\n−-−\n+++\n\n\n333\n+++\n+++\n−-−\n\n\n\n多个范围定义的 “不相等” 约束：\n\n\n\n定义\n描述\n\n\n\n\n范围Scope\n{x1,x2,…,xk}\\{x_1, x_2, \\dots, x_k\\}{x1​,x2​,…,xk​}\n\n\n紧凑定义Compact definition\nxi≠xj,∀i≠j∈{1,2,…,k}x_i \\neq x_j, \\forall i \\neq j \\in \\{1, 2, \\dots, k\\}xi​=xj​,∀i=j∈{1,2,…,k}\n\n\n\n该问题很难使用表格定义。\n 3. FOL 和 CSP 知识系统的区别\nFOL 和 CSP 的对应表如下：\n\n\n\nFOL\nCSP\n\n\n\n\n常数零元函数Constant\n变量\n\n\n零元谓词\n定义域为 2 的变量\n\n\n非零元函数变量\n多个变量，每种情况对应一个变量需要有限的定义域\n\n\n等价Equality\n“等于” 限制\n\n\n存在 xxx\nSkolemisation变量 xxx\n\n\n全部 xxx\n需要有限的定义域\n\n\n\n最大的区别是，FOL 是假设它的句子或公式为真。而 CSP 是假设它的限制为真。\n例如我们判断某一个谓词变量 PPP 为真，FOL 的表达是 PPP，而 CSP 的表达为 P=1P = 1P=1。\n 二. 问题\n约束满足问题是 SAT 问题的扩展，常用于建模数学问题。CSP 公式比 SAT 公式包含更多关于问题结构的信息。\nSAT 问题则是将定义域限制为 True\\text{True}True 和 False\\text{False}False。\n\n如果 CSP 问题拥有有限的定义域，那么其表达能力等同于 SAT 问题。（可以双向翻译）\n\n有些求解器可能会将 CSP 转化为 SAT。但通常会向 SAT 求解器提供有关问题结构的提示。\n\n\n无限定义域的 CSP 问题无法转化为 SAT。\nCSP 的公式通常更为紧凑和直观。\n\nCSP 是一个面向现实生活的问题描述。CSP 不像 FOL 那样具有表现力；它不太可能是一个好的选择，例如，对于定理证明。\n\n&lt;返回符号人工智能导航\n","slug":"笔记/符号人工智能/限制满足问题CSP","date":"2024-10-18T10:00:03.000Z","categories_index":"笔记-符号人工智能","tags_index":"Symbolic Artificial Intelligence,Logic,Formal Language,Propositional Logic","author_index":"zExNocs"},{"id":"0c447b18e6031b5eb3d00eecbf4db4fb","title":"SAI-命题逻辑","content":"&lt;返回符号人工智能导航\n\n 一. 介绍\n命题逻辑(Propositional Logic) 是一阶逻辑 FOL 的一个子集。它是不带有论域的FOL：\n\n\n\n组成成分\n符号\n\n\n\n\n零元谓词Zero-arity Predicates\n大写字母，例如 PPP\n\n\n逻辑连接符\n∧,∨,¬,(,),→,  ⟺  \\land, \\lor, \\neg, (,), \\rightarrow, \\iff∧,∨,¬,(,),→,⟺\n\n\n不支持\n量词、变量、函数符号和多元谓词符号\n\n\n\n存在命题逻辑的推理系统，是完备并合理的\n 二. 合取范式 CNF\n任何一个命题公式都可以转化为合取范式(Conjunctive Normal Form, CNF) 和 析取范式(Disjunctive Normal Form, DNF)。\n\n\n\n范式\n介绍\n例子\n\n\n\n\n合取范式CNF\n是析取子句(clauses)的合取\n(P∨Q)∧(Q∨S)∧T(P \\lor Q) \\land (Q \\lor S) \\land T(P∨Q)∧(Q∨S)∧T\n\n\n析取范式DNF\n是合取子句(clauses)的析取\n(−P∧R)∨(Q∧¬S)(-P \\land R) \\lor (Q \\land \\neg S)(−P∧R)∨(Q∧¬S)\n\n\n\n一般 CNF 更符合事实逻辑，所以一般我们主要使用 CNF 作为知识库：\n\n每一个 CNF 由一系列的析取子句(clause)表示:\n\n(A∨¬B)⏟Clause 1∧(¬C∨D)⏟Clause 2∧(¬A∨B∨C)⏟Clause 3\\underbrace{(A\\vee\\neg B)}_{\\text{Clause 1}}\\wedge\\underbrace{(\\neg C\\lor D )}_{\\text{Clause 2}}\\wedge\\underbrace{(\\neg A\\lor B\\lor C)}_{\\text{Clause 3}}\nClause 1(A∨¬B)​​∧Clause 2(¬C∨D)​​∧Clause 3(¬A∨B∨C)​​\n\n每一个子句包含一系列的文字(literal):\n\n(A⏟Literal1∨¬B⏟Literal2)\\left( \\underbrace{A}_{Literal 1} \\vee \\underbrace{ \\neg B}_{Literal 2} \\right)\n(Literal1A​​∨Literal2¬B​​)\n每一个文字要么是一个原子语句(atomic sentence)，要么是它的否定。\n 1. 转化 CNF\n\n\n\n步骤\n说明\n\n\n\n\n111. 将   ⟺  \\iff⟺ 和 →\\rightarrow→ 转化为基本运算符\n1. 替换所有的 α  ⟺  β\\alpha \\iff \\betaα⟺β 为 (α→β)∧(β→α)(\\alpha \\rightarrow \\beta) \\land (\\beta \\rightarrow \\alpha)(α→β)∧(β→α)2. 替换所有的 α→β\\alpha \\rightarrow \\betaα→β 为 (¬α∨β)(\\neg \\alpha \\lor \\beta)(¬α∨β)\n\n\n222. 将 ¬\\neg¬ 写入括号内\n1. 替换所有的 ¬(α∧β∧…)\\neg (\\alpha \\land \\beta \\land \\ldots)¬(α∧β∧…) 为 (¬α∨¬β∨…)(\\neg \\alpha \\lor \\neg \\beta \\lor \\ldots)(¬α∨¬β∨…)2. 替换所有的 ¬(α∨β∨…)\\neg (\\alpha \\lor \\beta \\lor \\ldots)¬(α∨β∨…) 为 (¬α∧¬β∧…)(\\neg \\alpha \\land \\neg \\beta \\land \\ldots)(¬α∧¬β∧…)3. 替换所有的 ¬¬α\\neg \\neg \\alpha¬¬α 为 α\\alphaα\n\n\n333. 使用分配律将析取分布到合取中\n替换所有的 ((α∧β)∨γ)((\\alpha \\land \\beta) \\lor \\gamma)((α∧β)∨γ) 为 ((α∨γ)∧(β∨γ))((\\alpha \\lor \\gamma) \\land (\\beta \\lor \\gamma))((α∨γ)∧(β∨γ))\n\n\n444. 删除所有重复的文字\n替换所有的 (α∨α)(\\alpha \\lor \\alpha)(α∨α) 和 α∧α\\alpha \\land \\alphaα∧α 为 α\\alphaα\n\n\n\n然而，将析取范式转化为合取范式或者将析取范式转化为合取范式可能会增加子句的数量，可能会指数型增加。\n 2. CNF 子句表示法\n因为 CNF 拥有简单的结构，所以可以使用一种紧凑的形式，叫做子句表示法(Clausal Representation)。\n子句表示法允许我们将公式作为数学对象（集合的集合）来处理。\n i. 规则定义\n\n\n\n符号\n解释\n特殊\n\n\n\n\n{… }\\{\\dots\\}{…}\n公式Formula\n空括号 { }\\{\\ \\}{ } 表示 True因为 True 是合取的中性元素(neutral element)也就是 X∧True  ⟺  XX \\land \\text{True} \\iff XX∧True⟺X\n\n\n[… ][\\dots][…]\n子句Clause\n空括号 [ ][\\ ][ ] 表示 False因为 False 是析取的中性元素也就是 X∨False  ⟺  XX \\lor \\text{False} \\iff XX∨False⟺X此外，空析取子句也写作 ⊥\\bot⊥\n\n\nA‾\\overline{A}A\n否定\n\n\n\n\n例如 (A∨¬B)∧(¬C∨D)∧(¬A∨B∨C)(A\\vee\\neg B)\\wedge(\\neg C\\lor D )\\wedge(\\neg A\\lor B\\lor C)(A∨¬B)∧(¬C∨D)∧(¬A∨B∨C) 可以表示为 {(A,B‾),[C‾,D],[A‾,B,C]}\\{(A, \\overline{B}), [\\overline{C}, D], [\\overline{A}, B, C]\\}{(A,B),[C,D],[A,B,C]}\n ii. 推理\n如果 CNF SSS 是不可满足的：\n\n\n\n结果\n解释\n\n\n\n\n显式矛盾\n[ ]∈S[\\ ] \\in S[ ]∈S\n\n\n隐式矛盾\n归结推导出空子句 [ ][\\ ][ ]即 S⊢FalseS \\vdash \\text{False}S⊢False 或写作 S⊢⊥S \\vdash \\botS⊢⊥只有 {[P],[P‾]}\\{[P], [\\overline{P}]\\}{[P],[P]} 才能一步推导出 [ ][\\ ][ ]\n\n\n\n 3. CNF 扩展\n为了适应所有 FOL，我们使用扩展的 CNF：\n\n\n\n顺序\n扩展\n\n\n\n\n111\n扩展文字转化为 CNF:1. 多元谓词、函数和其否定2. 量词形式3. 量词形式的否定\n\n\n222\n对于每个量词使用独一无二的变量名\n\n\n333\n使用德摩根将量词的否定写到里面\n\n\n444\n将量词写在外面这依赖于将每个量词使用第一无二的变量名\n\n\n555\n将每一个子句转化为 CNF 的子句表示法\n\n\n\n\nCNF 扩展的例子\n考虑该公式：A∧(B∨∀x.∀y.P(f(x),y)∨Q(x))∧¬∃x.Q(x)A \\land (B \\lor \\forall x. \\forall y. P(f(x), y) \\lor Q(x)) \\land \\neg \\exists x. Q(x)A∧(B∨∀x.∀y.P(f(x),y)∨Q(x))∧¬∃x.Q(x)\n\n\n\n步骤\n结果\n\n\n\n\n扩展文字\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x.Q(x)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\neg \\exists x. Q(x)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x.Q(x)]}\n\n\n变量名\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x2.Q(x2)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\neg \\exists x_2. Q(x_2)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x2​.Q(x2​)]}\n\n\n删否定\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[∀x2.¬Q(x2)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\forall x_2. \\neg Q(x_2)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[∀x2​.¬Q(x2​)]}\n\n\n量词写外面\n∀x.∀y.∀x2.{[A],[B,P(f(x),y)∨Q(x)],[Q(x2)‾]}\\forall x. \\forall y. \\forall x_2. \\{[A], [B, P(f(x), y) \\lor Q(x)], [\\overline{Q(x_2)}]\\}∀x.∀y.∀x2​.{[A],[B,P(f(x),y)∨Q(x)],[Q(x2​)​]}\n\n\n转化为 CNF\n∀x.∀y.∀x2.{[A],[B,P(f(x),y),Q(x)],[Q(x2)‾]}\\forall x. \\forall y. \\forall x_2. \\{[A], [B, P(f(x), y), Q(x)], [\\overline{Q(x_2)}]\\}∀x.∀y.∀x2​.{[A],[B,P(f(x),y),Q(x)],[Q(x2​)​]}\n\n\n\n\n\n 三. 命题逻辑的推理系统\n存在命题逻辑的推理系统是合理且完备的。也就是说此推理系统是 S⊢α  ⟺  S⊨αS \\vdash \\alpha \\iff S \\models \\alphaS⊢α⟺S⊨α。\n命题逻辑的推理系统主要是通过归结推理完成的。\n归结推理(Resolution Reasoning)是一种推理方法，它是基于 CNF 的 归结(Resolution) 进行的，归结推理是合理且完备的。\n 1. 归结定义\n假设 PPP 为一个文字(literal)，α,β\\alpha, \\betaα,β 文字的集合。\n如果一个 CNF 公式包含两个子句 [P,α][P, \\alpha][P,α] 和 [P‾,β][\\overline{P}, \\beta][P,β]，那么子句 [α,β][\\alpha, \\beta][α,β] 就为该 CNF 的归结。\n在语义上，归结是被该 CNF 蕴含的(entailed)，即\n(P∨α)∧(¬P∨β)⊨(α∨β)(P \\lor \\alpha) \\land (\\neg P \\lor \\beta) \\models (\\alpha \\lor \\beta)\n(P∨α)∧(¬P∨β)⊨(α∨β)\n\n归结推理的蕴含证明\n假设知识系统为 KB=(P∨α)∧(¬P∨β)\\text{KB} = (P \\lor \\alpha) \\land (\\neg P \\lor \\beta)KB=(P∨α)∧(¬P∨β)，此时有两种解释：\n\n当 J[P]=False\\mathcal{J}[P] = \\text{False}J[P]=False 时，为了满足知识系统则 J[α]=True\\mathcal{J}[\\alpha] = \\text{True}J[α]=True\n当 J[P]=True\\mathcal{J}[P] = \\text{True}J[P]=True 时，为了满足知识系统则 J[β]=True\\mathcal{J}[\\beta] = \\text{True}J[β]=True\n\n综上所述，只要满足知识库始终有 α∨β  ⟺  True\\alpha \\lor \\beta \\iff \\text{True}α∨β⟺True。\n也就是说\n∀J. J⊨KB→J⊨α∨β\\forall \\mathcal{J}.\\ \\mathcal{J} \\models \\text{KB} \\to \\mathcal{J} \\models \\alpha \\lor \\beta\n∀J. J⊨KB→J⊨α∨β\n那么有 KB⊨(α∨β)\\text{KB} \\models (\\alpha \\lor \\beta)KB⊨(α∨β)。\n\n\n此外，有且只有 [P][P][P] 和 [P‾][\\overline{P}][P] 才能归结出空语句 [ ][\\ ][ ]，即 P∧¬P⊨FalseP \\land \\neg P \\models \\text{False}P∧¬P⊨False。\n 2. 归结推理系统定义和性质\n命题逻辑的归结推理系统的每一个子句的推导都是由归结得来。\n对于一个 CNF 包含子句 (P∨α)∧(¬P∨β)(P \\lor \\alpha) \\land (\\neg P \\lor \\beta)(P∨α)∧(¬P∨β) 和归结 (α∨β)(\\alpha \\lor \\beta)(α∨β)，归结推理的推导有\n(P∨α)∧(¬P∨β)⊢(α∨β)(P \\lor \\alpha) \\land (\\neg P \\lor \\beta) \\vdash (\\alpha \\lor \\beta)\n(P∨α)∧(¬P∨β)⊢(α∨β)\n命题逻辑的归结推理系统性质有：\n\n\n\n性质\n是否\n说明\n\n\n\n\n合理性Sound\n✔️\n归结是被 CNF 蕴含的，因此是合理的即 S⊢C→S⊨CS \\vdash C \\to S \\models CS⊢C→S⊨C\n\n\n完备性Complete\n✔️\n如果 S⊨CS \\models CS⊨C，那么一定可以归结出子句 CCC即 S⊨C→S⊢CS \\models C \\to S \\vdash CS⊨C→S⊢C\n\n\n反驳完备性Refutation-Complete\n✔️\n1. 如果 SSS 不可满足，那么一定可以归结出空子句 [ ][\\ ][ ]2. 常用于 S⊨C  ⟺  S∪{¬C}⊢⊥S \\models C \\iff S \\cup \\{\\neg C\\} \\vdash \\botS⊨C⟺S∪{¬C}⊢⊥3. 归结推理系统证明蕴含是通过本性质进行的\n\n\n可判断性Decidability\n✔️\n但是时间是根据子句数量呈指数型增长\n\n\n\n 3. 归结推理系统证明蕴含\n归结演绎是基于反驳的证明方法。通过将否定的假设添加到知识库中找出矛盾(不可满足的)。\n即如果我们要证明 S⊨αS \\models \\alphaS⊨α，那么就只需要证明 S∧¬αS∧¬\\alphaS∧¬α 是不可满足的。\n因此我们只需要将假设的否定加入到知识库中，找到其的矛盾(证明不可满足性)，从而证明蕴含。\n即证明 (S∪¬α)⊢⊥(S \\cup ¬\\alpha) \\vdash \\bot(S∪¬α)⊢⊥，则说明 S⊨αS \\models \\alphaS⊨α，否则 S⊭αS \\not \\models \\alphaS⊨α。\n归结推理算法的步骤：\n\n\n\n步骤\n详细\n\n\n\n\n输入\n1. 知识库/子句集合 SSS2. 需要证明蕴含的子句 α\\alphaα\n\n\n111\n将否定加入知识库即 S′=S∪{¬α}S&#x27; = S \\cup \\{\\neg \\alpha\\}S′=S∪{¬α}\n\n\n222\n如果此时 [ ]∈S′[\\ ] \\in S&#x27;[ ]∈S′，返回 Entailed\n\n\n333\n若 ∃P. [P,β],[P‾,γ]∈S′\\exists P.\\ [P, \\beta], [\\overline{P}, \\gamma] \\in S&#x27;∃P. [P,β],[P,γ]∈S′，则将 [β,γ][\\beta, \\gamma][β,γ] 加入 S′S&#x27;S′，并跳转步骤 222\n\n\n444\n否则返回 Not Entailed\n\n\n\n 4. FOL的归结推理系统\nFOL 的归结推理系统请看 一阶逻辑 FOL 中六.3.。\n 四. 霍恩子句\n霍恩子句（Horn Clauses） 是 CNF 子句的一种特殊情况。\n命题逻辑中的霍恩子句是一个最多包含一个正文字（未取反文字）的子句。\n 1. 霍恩子句的正负性\n命题逻辑中的霍恩子句分为 正霍恩子句 (Positive Horn clause) 和 负霍恩子句 (Negative Horn clause)。\n\n\n\n子句\n定义\n\n\n\n\n正霍恩子句Positive Horn Clause\n1. 正霍恩子句是指只有一个正文字 AAA，和负文字集合 NNN2. 如果正霍恩子句 α\\alphaα 为真，且其他负文字为真，那么这个正文字 AAA 必须为真3. 即 (⋀N)→A(\\bigwedge N) \\to A(⋀N)→A 4. 例如 [A,B‾,C‾,D‾][A, \\overline{B}, \\overline{C}, \\overline{D}][A,B,C,D]，N={B,C,D}N = \\{B, C, D\\}N={B,C,D}，即 B∧C∧D→AB \\land C \\land D \\to AB∧C∧D→A，读作 “If B and C and D then A”\n\n\n负霍恩子句Negative Horn Clause\n1. 负霍恩子句没有正文字2. 例如 [A‾,B‾,C‾,D‾][\\overline{A}, \\overline{B}, \\overline{C}, \\overline{D}][A,B,C,D]3. 也被称之为目标 (goal)，也就是要验证的对象4. 用于检测是否存在不可满足性，即所有这些条件不能同时为真5. 空子句 [ ][\\ ][ ] 属于负霍恩子句\n\n\n\n 2. 霍恩子句的归结推理系统\n霍恩子句的归结分为下面三种情况：\n\n\n\n情况\n归结\n说明\n\n\n\n\n正子句 + 正子句\n正子句\n抵消了一个正，剩下一个正\n\n\n正子句 + 负子句\n负子句\n抵消了一个正，没有正了\n\n\n负子句 + 负子句\n无法归结\n因为不存在正，所以无法归结\n\n\n\n i. SLD 归结推理系统\n\n\n\n\n\n\n\n\n\n如果可以从一个霍恩子句集合中推导出一个负子句，那么一定可以通过归结操作将该负子句推导出来。\n对于霍恩子句集合 SSS 归结出负子句 ccc，存在以下形式的推理过程(SLD, Selected literals, Linear pattern, over Definite clauses)：\n\n\n\n步骤\n详细\n\n\n\n\n选择文字Selected Literals\n从集合 SSS 中选择一个负子句 c1∈Sc_1 \\in Sc1​∈S\n\n\n线性模式Linear Pattern\n1. 对于 i∈[2,n]i \\in [2, n]i∈[2,n]，cic_ici​ 是 ci−1c_{i - 1}ci−1​ 和集合 SSS 中某个正子句归结的结果2. 一般寻找该负子句中第一个负文字作为正文字的子句\n\n\n定义子句Definite Clauses\n满足 cn=cc_n = ccn​=c\n\n\n\n该过程是基于 “正霍恩子句中，若其他负文字为真，那么这个正文字必须为真” 进行的。\n因为空语句也是负语句，因此SLD推导是反驳完备的：\n\n如果SLD推导产生了一个空子句，则集合 SSS 是不可满足的\n如果集合 SSS 是不可满足的，SLD保证最终会得到一个空子句\n\n ii. 基于 SLD 的推理算法\n存在两种算法：\n\nBackward Chaining（反向推理）\nForward Chaining（正向推理）\n\n这两种算法的设计目标是验证一组已知事实：\n\n给定一个由 正霍恩子句 组成的集合 SSS\n给定一个命题文字集合 L={Q1,Q2,…,Qn}L = \\{Q_1, Q_2, \\dots, Q_n\\}L={Q1​,Q2​,…,Qn​}\n证明 Q1∧Q2∧⋯∧QnQ_1 \\land Q_2 \\land \\dots \\land Q_nQ1​∧Q2​∧⋯∧Qn​ 被集合 SSS 蕴含，即所有命题文字被 SSS 蕴含\n\n根据反驳完备性，证明这种蕴涵关系等价于将一个负霍恩子句 [Q1‾,Q2‾,…,Qn‾][\\overline{Q_1}, \\overline{Q_2}, \\dots, \\overline{Q_n}][Q1​​,Q2​​,…,Qn​​] 加入到集合 SSS 中，并尝试推导出空子句 [ ][\\ ][ ]。\n a. Backward Chaining（反向推理）\n反向推理是从 Q1∧Q2∧⋯∧QnQ_1 \\land Q_2 \\land \\dots \\land Q_nQ1​∧Q2​∧⋯∧Qn​ 开始，然后沿着推理链向回推理进行归结的算法。该算法需要依次证明 Q1Q_1Q1​ 到 QnQ_nQn​ 单个文字的蕴含性，并且在证明的过程中需要证明其他单个文字的蕴含性。\n对于证明文字 QQQ 的蕴含性，其步骤如下：\n\n\n\n步骤\n详细\n\n\n\n\n寻找子句\n在 SSS 中找到一个以 QQQ 为正的正霍恩子句假设该子句为 c=[Q,P1‾,P2‾,...Pk‾]c = [Q, \\overline{P_1}, \\overline{P_2}, ... \\overline{P_k}]c=[Q,P1​​,P2​​,...Pk​​]\n\n\n证明蕴含\n如果 P1,P2,...,PkP_1, P_2, ..., P_kP1​,P2​,...,Pk​ 全部被蕴含，则可以证明 Q1Q_1Q1​ 也被蕴含\n\n\n\n证明一个命题文字 QQQ 的蕴涵被归约为证明其他命题文字 {P1,P2,...,Pk}\\{P_1, P_2, ..., P_k\\}{P1​,P2​,...,Pk​} 的蕴涵，并将这些新的命题文字并到剩下的命题文字集合 L−{Q}L - \\{Q\\}L−{Q} 中，也就是得到归结 [Q2,Q3,…,Qn,P1,P2,…,Pk‾][\\overline{Q_2, Q_3, \\dots, Q_n, P_1, P_2, \\dots, P_k}][Q2​,Q3​,…,Qn​,P1​,P2​,…,Pk​​] 。\n\n\n\n\n\n\n\n\n\n反向推理会不断地归结产生新的负子句，也就是 SLD 中 L 的 cic_ici​。这是一种 递归算法。\n\n\n\n算法步骤\n详细\n\n\n\n\n输入\n1. 正霍恩子句集合 SSS2. 证明蕴含的子句集合 L={Q1,Q2,…,Qn}L = \\{Q_1, Q_2, \\dots, Q_n\\}L={Q1​,Q2​,…,Qn​}\n\n\n输出\n如果 Q1,Q2,...,QnQ_1, Q_2, ..., Q_nQ1​,Q2​,...,Qn​ 都被 SSS 蕴含，则返回 True，否则返回 False\n\n\n算法名\nbackwardChaining(S, L)\n\n\n111\n1. 如果 length(L) = 0，即 n=0n = 0n=0，那么返回 True2. 也就是归结出空子句\n\n\n222\n如果 ∃c∈S\\exists c \\in S∃c∈S 且 c=[Q1,P1‾,P2‾,…,Pk‾]c = [Q_1, \\overline{P_1}, \\overline{P_2}, \\dots, \\overline{P_k}]c=[Q1​,P1​​,P2​​,…,Pk​​]：得到归结的文字集合 L′={Q2,Q3,…,Qn,P1,P2,…,Pk}L&#x27; = \\{Q_2, Q_3, \\dots, Q_n, P_1, P_2, \\dots, P_k\\}L′={Q2​,Q3​,…,Qn​,P1​,P2​,…,Pk​}如果 backwardChaining(S, L') 返回 True 则 返回 True\n\n\n333\n不存在，则返回 False\n\n\n\n性质：\n\n该算法是合理的，它做出的任何结论都是正确的。\n即使在命题逻辑情景下，也不保证终止。即是不可判断的。\n在相对简单的情况下，它可能会非常低效。\n\n记忆搜索优化：\n当函数返回 True 时将该要证明蕴含文字标记成 “被证明为真”。当再次证明该文字时可以通过该记忆法进行剪枝。\n b. Forward Chaining（正向推理）\n核心思想是：当我们证明某个命题变量被知识库蕴涵时，将其标记为“已解决”，然后将这个信息传播到其他子句中。\n每次循环都检查 SSS 所有的子句，如果有一个子句 c=[Q,P1‾,P2‾,...Pk‾]∈Sc = [Q, \\overline{P_1}, \\overline{P_2}, ... \\overline{P_k}] \\in Sc=[Q,P1​​,P2​​,...Pk​​]∈S 中 QQQ 是尚未解决的，但是其他文字 P1,P2,…P_1, P_2, \\dotsP1​,P2​,… 被标记已解决，那么 QQQ 也被标记为已解决。最后验证 LLL 是否所有的文字都已经被标记为已解决，否则再次遍历检查 SSS。\n\n\n\n\n\n\n\n\n\n正向推理是基于 SLD 进行推理的，但实际上算法不会进行构造归结来产生新的子句。\n\n\n\n算法步骤\n详细\n\n\n\n\n输入\n1. 正霍恩子句集合 SSS2. 证明蕴含的子句集合 L={Q1,Q2,…,Qn}L = \\{Q_1, Q_2, \\dots, Q_n\\}L={Q1​,Q2​,…,Qn​}\n\n\n输出\n如果 Q1,Q2,...,QnQ_1, Q_2, ..., Q_nQ1​,Q2​,...,Qn​ 都被 SSS 蕴含，则返回 True，否则返回 False\n\n\n算法名\nforwardChaining(S, L)\n\n\n111\n将字母表中所有的命题文字初始化为 未解决 的\n\n\n222\n1. 遍历集合 SSS2. 若存在 c=[Q,P1‾,P2‾,...]c = [Q, \\overline{P_1}, \\overline{P_2}, ...]c=[Q,P1​​,P2​​,...] 中 QQQ 未解决 而 P1,P2,…P_1, P_2, \\dotsP1​,P2​,… 已解决, 将 QQQ 标记为已解决3. 可将 ccc 剔除出遍历名单中\n\n\n333\n检测 LLL 中所有文字是否已经被标记为已解决，如果是则返回 True\n\n\n444\n1. 若上次遍历至少将一个文字标记为已解决，则返回 222 再次遍历2. 否则返回 False\n\n\n\n性质：\n\n正向推理在命题逻辑情况下是合理且反驳完备的。\n正向推理速度比反向推理要快得多。对于命题逻辑情况，正向推理是一个多项式时间算法。\n其时间复杂度是 O(∣S∣)O(|S|)O(∣S∣)，即迭代次数与知识库的大小成线性关系。\n\n\n&lt;返回符号人工智能导航\n","slug":"笔记/符号人工智能/命题逻辑","date":"2024-10-18T10:00:02.000Z","categories_index":"笔记-符号人工智能","tags_index":"Symbolic Artificial Intelligence,Logic,Formal Language,Propositional Logic,FOL,Resolution,Horn Clauses,CNF","author_index":"zExNocs"},{"id":"a68477efaef57eb4dc7da835a61c6b80","title":"SAI-一阶逻辑FOL","content":"&lt;返回符号人工智能导航\n\n 一. 介绍\n\n\n\n\n\n\n\n\n\n一阶逻辑 (First Order Logic, FOL) 是声明性语言 (Declarative Language)，是一种知识的表达(Representation)。\n用声明式语言编写的程序描述了问题的逻辑结构或解决问题的目标，而不必明确地规定如何逐步执行这个目标的细节步骤，是高度抽象的。与其相对的是命令式语言，命令式语言程序员需要明确地定义执行过程的每一步。\n 1. 基础定义\nFOL 主要由变量、函数符号(包含常量)、谓词、操作符组成。\n i. FOL 数据类型\n\n\n\n数据类型\n说明\n操作符\n\n\n\n\n布尔Boolean\n1. 值只有 True 和 False2. 可以看作元数为 0 的谓词符号3. 其符号通常叫做 命题变量符号\n∧,∨,¬\\land, \\lor, \\neg∧,∨,¬\n\n\n论域Domain of DiscourseDomain\n1. 表示非布尔类型，例如数字2. 其符号通常叫做 变量符号\n∀,∃,=\\forall, \\exist, =∀,∃,=作为谓词/函数的参数\n\n\n\n本文中下文所有直接的 “变量” 表示论域中的域元素，而布尔类型被称为 命题变量。\n ii. FOL 字母表(Alphabet)\nFOL 的字母表分为 逻辑符号(变量和操作符) 和 非逻辑符号(函数)。\n a. 逻辑符号\n\n\n\n名称\n符号\n范围\n说明\n\n\n\n\n域元素Domain Element\n小写字母表示例如 x,y1,z3,8x, y_1, z_{3, 8}x,y1​,z3,8​\n论域Domain\n1. 数据类型是论域(非布尔型)2. 通常使用 === 或者 谓词符号(元素作为变量) 转化为布尔型\n\n\n量词逻辑\n∀,∃\\forall, \\exist∀,∃\n论域Domain\n1. 用于定义一个新的论域中的域元素2. 后面要跟布尔(包含谓词和函数的公式表达式)而不能跟变量3. 表示对于该论域的所有/某一个值，布尔为 True\n\n\n等号\n===\n论域Domain\n1. 比较两个变量的值是否等价，并转化成布尔型2. 布尔是没有相等这个说法的，两边一定是变量布尔通常使用   ⟺  \\iff⟺ 或 ≡≡≡\n\n\n逻辑连接\n∧,∨,¬,→,  ⟺  \\land, \\lor, \\neg, \\to, \\iff∧,∨,¬,→,⟺\n布尔Boolean\n具体含义请看离散数学中的命题逻辑\n\n\n括号\n(,),[,],{,}(,),[,],\\{,\\}(,),[,],{,}\n任何表达式\n提高优先级\n\n\n\n b. 非逻辑符号\n非逻辑符号是使用 论域 Domain 作为参数并返回某一个数据类型具体值的方法。\n非逻辑符号的参数个数称之为元数(Arity)。\n根据返回数据类型又分为谓词符号和函数符号。\n\n\n\n名称\n符号\n说明\n特殊\n\n\n\n\n谓词符号Predicate\n大写字母开头例如 P,Q(x,y),IsTasty(x)P, Q(x, y), IsTasty(x)P,Q(x,y),IsTasty(x)\n1. 返回布尔值方法的符号代表2. 将一个变量转化成布尔\n1. 元数为 000 的谓词称为命题变量(Propositional Variables)2. 命题变量通常不使用括号，即使用 PPP 而不是 P()P()P()\n\n\n函数符号Function\n小写字母开头例如 f(x),g,madeOf(y)f(x), g, madeOf(y)f(x),g,madeOf(y)\n1. 返回域元素方法的符号代表2. 将一个变量转化为另一个变量\n1. 元数为 000 的函数称为常量符号(Constant Symbols)2. 例如字符串 &quot;Cat&quot;，数字 733. 常量符号通常不使用括号，即使用 fff 而不是 f()f()f()\n\n\n\n在没有解释器的情况下，非逻辑符号仅仅只是个具有语义的符号代表，不具有实际的函数映射的功能。\n 2. 域元素/变量的范围\n变量的范围(scope)是公式中变量可使用的范围。\n\n\n\n范围Scope\n说明\n\n\n\n\n绑定的Bound\n1. 由 量词 定义的变量是绑定的2. 例如公式 ∀x.P(x)\\forall x. P(x)∀x.P(x) 的 xxx\n\n\n自由的Free\n1. 在公式之外定义的变量是自由的2. 这些值需要解释器额外给定值3. 例如公式 P(x)∨Q(x)P(x) \\lor Q(x)P(x)∨Q(x) 中的 xxx\n\n\n\n自由的变量其范围是整个表达式，而绑定的变量只能在该量词的范围内访问。\n如果变量名被重复利用，其一般使用的是最内层的定义。例如 P(x)∨∃x.Q(x)P(x) \\lor \\exist x. Q(x)P(x)∨∃x.Q(x) 中有两个 xxx，其中 P(x)P(x)P(x) 的 xxx 是自由的，而 Q(x)Q(x)Q(x) 的是绑定的。\n 3. FOL知识库\nFOL 不支持数据类型，只支持布尔和域值。但是可以使用谓词，如 Person(x)Person(x)Person(x)，表示 xxx 是一个人。\n如果 FOL 知识库具有一个层级系统，例如 xxx 是一条狗，同时 xxx 是一个哺乳动物、动物，那么有：\nDog(x), Mammal(x), Animal(x)Dog(x),\\ Mammal(x),\\ Animal(x)Dog(x), Mammal(x), Animal(x)。\n但是如果我们对每一个生物这样描述，所有层级都手动添加，会很耗时并且容易出现错误。\n因此我们添加如下的规则：\n\n∀x, Dog(x)→Mammal(x)\\forall x,\\ Dog(x) \\rightarrow Mammal(x)∀x, Dog(x)→Mammal(x)\n∀x, Mammal(x)→Animal(x)\\forall x,\\ Mammal(x) \\rightarrow Animal(x)∀x, Mammal(x)→Animal(x)\n\n 4. 性质\n\n\n\n特点\n说明\n\n\n\n\n精确Precise\n1. 与模糊逻辑(fuzzy logic)不同，每个事实只能是 true 或 false2. 定义哪些字符串是有效语句(syntax)和如果它们为true意味着什么(semantics)\n\n\n抽象Abstract\n1. 使用抽象符号2. 与自然语言(如英语)不同，不直接描述自然现象3. 这些符号与现实世界现象的映射取决于用户\n\n\n严谨Strictness\n1. 没有办法为规则设置例外2. 例如说所有狗都是哺乳动物，就不能添加例外某些狗可能不是哺乳动物\n\n\n单调性Monotonicity\n1. 如果 S⊨αS \\models \\alphaS⊨α，那么 ∀β. S∪{β}⊨α\\forall \\beta.\\ S \\cup \\{\\beta\\} \\models \\alpha∀β. S∪{β}⊨α2. 即如果一系列公式可以推导出一个结论，那么为这个公式添加任何条件都可以推导出该结论3. 也可以说是无法添加任何例外反驳推定的结论\n\n\n\n 4. 与命题逻辑的比较\n\n\n\n\n命题逻辑\n一节逻辑\n\n\n\n\n符号\n只包含命题和逻辑操作符号\n包含常量、谓词符号、函数符号、变量、量词\n\n\n推理\n真值表、推理规则\n可满足性问题\n\n\n\n 二. 完善公式 (WFF)\n并不是所有的字母表排序都是有意义的，例如 ¬∧x∀\\neg \\land x \\forall¬∧x∀ 是无意义的语句。\n而有效排序的公式我们称为 完善公式 (Well-Formed Formulas, WFF)。\n 1. 公式术语定义\n完善公式 WFF 的 形式规则(Formation Rules)/语法(grammar) 是其通过 公式 和 项 归纳定义：\n\n\n\n术语\n归纳定义\n说明\n\n\n\n\n项Term\n1. 一个域元素/变量是项；包括自由的和绑定的2. f(t1,t2,…,tn)f(t_1, t_2, \\dots, t_n)f(t1​,t2​,…,tn​) 是项其中 fff 是元数为 nnn 的函数符号，tit_iti​ 是项\n1. 其值取决于量词符号∀,∃\\forall, \\exists∀,∃ 或者 变量分配 di=μ[ti]d_i = \\mu[t_i]di​=μ[ti​]2. 简单说就是输出变量的表达式3. 单个项不属于 WFF\n\n\n公式Formula\n1. P(t1,t2,…,tn)P(t_1, t_2, \\dots, t_n)P(t1​,t2​,…,tn​) 是公式其中 PPP 是元数为 nnn 的谓词符号，tit_iti​ 是项2. t1=t2t_1 = t_2t1​=t2​ 是公式，其中 t1,t2t_1, t_2t1​,t2​ 是项3. 如果 α\\alphaα 是公式，¬α\\neg \\alpha¬α 是公式4. 如果 α,β\\alpha, \\betaα,β 是公式，那么 α∧β,α∨β,α→β,α  ⟺  β\\alpha \\land \\beta, \\alpha \\lor \\beta, \\alpha \\to \\beta, \\alpha \\iff \\betaα∧β,α∨β,α→β,α⟺β 是公式5. 如果 α\\alphaα 是公式，那么∀x.α,∃x.α\\forall x. \\alpha, \\exist x. \\alpha∀x.α,∃x.α 是公式其中 xxx 是新定义的域元素\n1. 简单说就是输出布尔的表达式2. A=B(x)A = B(x)A=B(x) 不是公式，=== 只能用于项而不能用于公式3. 公式属于 WFF\n\n\n满足Satisfaction\n1. 如果公式在解释 I\\mathcal{I}I 中、自由变量分配 μ\\muμ 下为 True，则称 I,μ\\mathcal{I}, \\muI,μ 满足该公式2. 具体符号的定义请看 FOL 语义的定义\n\n\n\n\n 2. 语句 Sentence\n\n\n\n\n\n\n\n\n\nA WFF that does not have free variables is called sentence.\n不存在自由变量的WFF被称为语句 (Sentence)\n因此语句是一个特殊的公式(formula)。\n\n要将带有自由变量的公式转换为句子，我们需要为该公式中的每个自由变量替换值。\n一般称这个替换值为变量分配(variable assignment) μ\\muμ\n\n例如 μ[ti]\\mu[t_i]μ[ti​] 则表示项 tit_iti​ 在 μ\\muμ 的分配下转化的具体值。\n\n\nμ\\muμ 是由解释器(Interpreter)提供的。\n\n i. 语句的分类\n\n\n\n种类\n说明\n\n\n\n\n可满足的Satisfiable\n存在至少一种解释使得语句满足\n\n\n不可满足的Unsatisfiable\n不存在解释使得语句满足\n\n\n有效的Valid\n对于所有解释都使得语句满足\n\n\n非有效的Not valid\n存在至少一种解释使得语句不满足\n\n\n\n假设语句 α\\alphaα，那么：\n\n\n\nα\\alphaα\n¬α\\neg \\alpha¬α\n\n\n\n\n可满足的\n非有效的\n\n\n非有效的\n可满足的\n\n\n不可满足的\n有效的\n\n\n有效的\n不可满足的\n\n\n\n 3. 扩展 CNF\n对于 CNF 和 CNF 的子句表示法，详细请看 命题逻辑 的 二. 合取范式 CNF。\n为了适应所有 FOL，我们使用扩展的 CNF：\n\n\n\n顺序\n扩展\n\n\n\n\n111\n扩展文字转化为 CNF:1. 多元谓词、函数和其否定2. 量词形式3. 量词形式的否定\n\n\n222\n对于每个量词使用独一无二的变量名\n\n\n333\n使用德摩根将量词的否定写到里面\n\n\n444\n将量词写在外面这依赖于将每个量词使用第一无二的变量名\n\n\n555\n将每一个子句转化为 CNF 的子句表示法\n\n\n\n\nCNF 扩展的例子\n考虑该公式：A∧(B∨∀x.∀y.P(f(x),y)∨Q(x))∧¬∃x.Q(x)A \\land (B \\lor \\forall x. \\forall y. P(f(x), y) \\lor Q(x)) \\land \\neg \\exists x. Q(x)A∧(B∨∀x.∀y.P(f(x),y)∨Q(x))∧¬∃x.Q(x)\n\n\n\n步骤\n结果\n\n\n\n\n扩展文字\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x.Q(x)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\neg \\exists x. Q(x)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x.Q(x)]}\n\n\n变量名\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x2.Q(x2)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\neg \\exists x_2. Q(x_2)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[¬∃x2​.Q(x2​)]}\n\n\n删否定\n{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[∀x2.¬Q(x2)]}\\{[A], [B, \\forall x. \\forall y. P(f(x), y) \\lor Q(x)], [\\forall x_2. \\neg Q(x_2)]\\}{[A],[B,∀x.∀y.P(f(x),y)∨Q(x)],[∀x2​.¬Q(x2​)]}\n\n\n量词写外面\n∀x.∀y.∀x2.{[A],[B,P(f(x),y)∨Q(x)],[Q(x2)‾]}\\forall x. \\forall y. \\forall x_2. \\{[A], [B, P(f(x), y) \\lor Q(x)], [\\overline{Q(x_2)}]\\}∀x.∀y.∀x2​.{[A],[B,P(f(x),y)∨Q(x)],[Q(x2​)​]}\n\n\n转化为 CNF\n∀x.∀y.∀x2.{[A],[B,P(f(x),y),Q(x)],[Q(x2)‾]}\\forall x. \\forall y. \\forall x_2. \\{[A], [B, P(f(x), y), Q(x)], [\\overline{Q(x_2)}]\\}∀x.∀y.∀x2​.{[A],[B,P(f(x),y),Q(x)],[Q(x2​)​]}\n\n\n\n\n\n 三. FOL语义和逻辑解释\n\n\n\n\n\n\n\n\n\nSemantics is concerned with the meaning of sentences (but not the mapping to real world).\n语义关注的是语句的意义（而不是与现实世界的映射）\nFOL语义是有关非逻辑符号的全部信息（从 domain 转化成 domain 或者 布尔 的全部信息），从而用于评估 FOL公式 为 True 或 False。\n其中逻辑解释(Logical interpretation) J\\mathcal{J}J 定义了FOL的语义。\nJ=&lt;D,I&gt;\\mathcal{J} = &lt;\\mathcal{D, I}&gt;\nJ=&lt;D,I&gt;\n\n\n\n符号\n含义\n定义\n\n\n\n\nD\\mathcal{D}D\n论域 Domain域元素的非空集合\n1. 常量项 t∈Dt \\in \\mathcal{D}t∈D2. 变量项 ttt，其赋值 dt=μ[t]∈Dd_t = \\mu[t] \\in \\mathcal{D}dt​=μ[t]∈D3. 函数 t=f(t1,t2,… )t = f(t_1, t_2, \\dots)t=f(t1​,t2​,…) 有：a. {t1,t2,… }⊆D\\{t_1, t_2, \\dots\\} \\subseteq \\mathcal{D}{t1​,t2​,…}⊆Db. t∈Dt \\in \\mathcal{D}t∈D\n\n\nI\\mathcal{I}I\n非逻辑符号解释函数为非逻辑符号赋予映射\n111. 对于元数为 nnn 的函数符号 fff:I[f]:D×D×D×⋯×D⏟n times→D\\mathcal{I}[f]:\\underbrace{\\mathcal{D}\\times\\mathcal{D}\\times\\mathcal{D} \\times\\cdots\\times\\mathcal{D}}_{n\\ \\text{times}}\\rightarrow\\mathcal{D}I[f]:n timesD×D×D×⋯×D​​→D即在解释 I\\mathcal{I}I 下函数 fff 在每个可能的参数赋值下会返回什么域元素222. 对于元数为 nnn 的谓词符号 PPP，可以看作一个映射：I[P]:D×D×D×⋯×D⏟n times→{True, False}\\mathcal{I}[P] : \\underbrace{\\mathcal{D \\times D \\times D \\times \\cdots \\times D}}_{n \\text{ times}} \\to \\{\\text{True, False}\\}I[P]:n timesD×D×D×⋯×D​​→{True, False}即在解释 I\\mathcal{I}I 下谓词 PPP 在每个可能的参数赋值下会返回什么布尔值 333. 对于元数为 nnn 的谓词符号 PPP，也可以看作一个关系(Relation)：I⊆D×D×D×⋯×D⏟n times\\mathcal{I} \\subseteq \\underbrace{\\mathcal{D \\times D \\times D \\times \\cdots \\times D}}_{n \\text{ times}}I⊆n timesD×D×D×⋯×D​​如果存在关系元素 a∈Ia \\in \\mathcal{I}a∈I，则认为参数 aaa 可以使谓词符号 PPP 满足不存在则认为该参数组合不使得 PPP 满足\n\n\n\n简单说，D\\mathcal{D}D 就是域元素的集合，I\\mathcal{I}I 就是谓词和函数对应的真值表和输出表。图例：\n\n\n\n\n此外，除了逻辑解释(Logical interpretation) J\\mathcal{J}J 以外，自由变量分配(variable assignment) μ\\muμ 也定义了 FOL 的语义，其中 μ[ti]\\mu[t_i]μ[ti​] 则表示项 tit_iti​ 在 μ\\muμ 的分配下转化的具体值。\n那么，如果确定：\n\ndomain 元素的集合 D\\mathcal{D}D\n非逻辑符号解释 I\\mathcal{I}I\n自由变量赋值 μ\\muμ\n\n就可以判断一个FOL公式是 True 还是 False\n 1. 项的具体值讨论\n如果 ttt 是一个项，dtd_tdt​ 是其的具体值，那么：\n\n\n\nttt 的类型\ndtd_tdt​\n\n\n\n\n自由变量项\ndt=μ[t]d_t = \\mu[t]dt​=μ[t]\n\n\n常量符号项\ndt=J[t]d_t = \\mathcal{J}[t]dt​=J[t]\n\n\n函数参数\n假设 t=f(t1,t2,… )t = f(t_1, t_2, \\dots)t=f(t1​,t2​,…)d=J[f](dt1,dt2,… )d = \\mathcal{J}[f](d_{t_1}, d_{t_2}, \\dots)d=J[f](dt1​​,dt2​​,…)\n\n\n谓词参数\n同函数参数\n\n\n\n 四. 满足和逻辑蕴含\n 1. 满足操作符\n满足 (Satisfaction) 指的是在解释 J\\mathcal{J}J 和变量分配 μ\\muμ 使得 FOL公式 α\\alphaα 为真 (True)。\n i. 满足操作符定义\n假设解释 J\\mathcal{J}J 和变量分配 μ\\muμ 使得 FOL公式 α\\alphaα 满足：\n\n\n\n前提\n符号\n\n\n\n\nα\\alphaα 存在自由变量\nJ,μ⊨α\\mathcal{J}, \\mu \\models \\alphaJ,μ⊨α\n\n\nSSS 是 FOL 公式集合∀α∈S. J,μ⊨α\\forall \\alpha \\in S.\\ \\mathcal{J}, \\mu \\models \\alpha∀α∈S. J,μ⊨α\nJ,μ⊨S\\mathcal{J}, \\mu \\models SJ,μ⊨S\n\n\nα\\alphaα 是语句不存在自由变量\nJ⊨α\\mathcal{J} \\models \\alphaJ⊨α\n\n\nSSS 是 FOL 语句集合∀α∈S. J⊨α\\forall \\alpha \\in S.\\ \\mathcal{J} \\models \\alpha∀α∈S. J⊨α\nJ⊨S\\mathcal{J} \\models SJ⊨S\n\n\n不满足\n⊭\\not\\models⊨\n\n\n\n ii. 满足操作符规则\n假设：\n\n\n\n符号\n解释\n\n\n\n\nJ\\mathcal{J}J\nFOL 逻辑解释，J=&lt;D,I&gt;\\mathcal{J = &lt;D, I&gt;}J=&lt;D,I&gt;\n\n\nμ\\muμ\n变量分配\n\n\nPPP\n一个元数为 nnn 的谓词符号\n\n\ntit_iti​\n一个项\n\n\ndid_idi​\ntit_iti​ 的具体值具体请看项的具体值讨论\n\n\nα,β\\alpha, \\betaα,β\n一个公式\n\n\nxxx\n一个绑定的变量\n\n\n\n那么有规则：\n\n\n\n规则\n前提\n结果\n\n\n\n\n谓词的解释\nJ,μ⊨P(t1,t2,…,tn)\\mathcal{J}, \\mu \\models P(t_1, t_2, \\dots, t_n)J,μ⊨P(t1​,t2​,…,tn​)\n&lt;d1,d2,…,dn&gt;∈I[P]&lt;d_1, d_2, \\dots, d_n&gt; \\in \\mathcal{I}[P]&lt;d1​,d2​,…,dn​&gt;∈I[P]\n\n\n相等解释\nJ,μ⊨(t1=t2)\\mathcal{J}, \\mu \\models (t_1 = t_2)J,μ⊨(t1​=t2​)\nd1=d2d_1 = d_2d1​=d2​\n\n\n否定解释\nJ,μ⊨¬α\\mathcal{J}, \\mu \\models \\neg \\alphaJ,μ⊨¬α\nJ,μ⊭α\\mathcal{J}, \\mu \\not\\models \\alphaJ,μ⊨α\n\n\n合取解释\nJ,μ⊨(α∧β)\\mathcal{J}, \\mu \\models (\\alpha \\land \\beta)J,μ⊨(α∧β)\nJ,μ⊨α∧J,μ⊨β\\mathcal{J}, \\mu \\models \\alpha \\land \\mathcal{J}, \\mu \\models \\betaJ,μ⊨α∧J,μ⊨β\n\n\n析取解释\nJ,μ⊨(α∨β)\\mathcal{J}, \\mu \\models (\\alpha \\lor \\beta)J,μ⊨(α∨β)\nJ,μ⊨α∨J,μ⊨β\\mathcal{J}, \\mu \\models \\alpha \\lor \\mathcal{J}, \\mu \\models \\betaJ,μ⊨α∨J,μ⊨β\n\n\n存在量词解释\nJ,μ⊨∃x.α\\mathcal{J}, \\mu \\models \\exist x. \\alphaJ,μ⊨∃x.α\n∃d. μ[x]=d→J,μ⊨α\\exist d.\\ \\mu[x] = d \\to \\mathcal{J}, \\mu \\models \\alpha∃d. μ[x]=d→J,μ⊨α\n\n\n全称量词解释\nJ,μ⊨∀x.α\\mathcal{J}, \\mu \\models \\forall x. \\alphaJ,μ⊨∀x.α\n∀d. μ[x]=d→J,μ⊨α\\forall d.\\ \\mu[x] = d \\to \\mathcal{J}, \\mu \\models \\alpha∀d. μ[x]=d→J,μ⊨α\n\n\n\n 2. 逻辑蕴含\n逻辑蕴含(Logical Entailment)是 FOL 语义上的蕴含，是根据已知的知识库 KB\\text{KB}KB (公式集合) 得到一个新的值为真的公式 α\\alphaα 的过程。写作：\nKB⊨α\\text{KB} \\models \\alpha\nKB⊨α\n i. 逻辑蕴含定义\n假设 SSS 是一个 FOL 公式的集合，α∉S\\alpha \\not \\in Sα∈S 是一个公式，如果：\n∀J. ∀μ. J,μ⊨S→J,μ⊨α\\forall \\mathcal{J}.\\ \\forall \\mu .\\ \\mathcal{J}, \\mu \\models S \\to \\mathcal{J}, \\mu \\models \\alpha\n∀J. ∀μ. J,μ⊨S→J,μ⊨α\n那么：\n\n\n\n含义\n说明\n\n\n\n\n逻辑结果Logical Consequence\nα\\alphaα 是 SSS 的逻辑结果\n\n\n逻辑蕴含Logical Entail\nSSS 逻辑蕴含 α\\alphaα\n\n\n蕴含符号\nS⊨αS \\models \\alphaS⊨α\n\n\n\n同理，对于假设 SSS 是一个语句的集合，α∉S\\alpha \\not \\in Sα∈S 是一个语句，如果\n∀J. J⊨S→J⊨α\\forall \\mathcal{J}.\\ \\mathcal{J} \\models S \\to \\mathcal{J} \\models \\alpha\n∀J. J⊨S→J⊨α\n那么，SSS 逻辑蕴含 α\\alphaα，即 S⊨αS \\models \\alphaS⊨α\n ii. 蕴含和可满足性、有效性转换\n即假设 S={α1,α2,… }S = \\{\\alpha_1, \\alpha_2, \\dots\\}S={α1​,α2​,…}，那么 ⋀S=α1∧α2∧…⋀ S = \\alpha_1 \\land \\alpha_2 \\land \\dots⋀S=α1​∧α2​∧…。\n可得到 LHS  ⟺  RHSLHS \\iff RHSLHS⟺RHS：\n\n\n\nLHS\nRHS\n\n\n\n\n∅⊨α\\varnothing \\models \\alpha∅⊨α\nα\\alphaα 是有效的写作 ⊨α\\models \\alpha⊨α\n\n\nS⊨FalseS \\models \\text{False}S⊨False\nSSS 是不可满足的\n\n\nS⊨αS \\models \\alphaS⊨α\n(⋀S)∧¬α(⋀S) \\land \\neg \\alpha(⋀S)∧¬α 是不可满足的S∪{¬α}⊨FalseS \\cup \\{\\neg\\alpha\\} \\models \\text{False}S∪{¬α}⊨False\n\n\nS⊭αS \\not \\models \\alphaS⊨α\n(⋀S)∧¬α(⋀S) \\land \\neg \\alpha(⋀S)∧¬α 是可满足的S∪{¬α}⊭FalseS \\cup \\{\\neg\\alpha\\} \\not \\models \\text{False}S∪{¬α}⊨False\n\n\nS⊨αS \\models \\alphaS⊨α\nS→αS \\to \\alphaS→α 是有效的⊨(⋀S)→α\\models (⋀S) \\to \\alpha⊨(⋀S)→α\n\n\nS⊭αS \\not \\models \\alphaS⊨α\nS→αS \\to \\alphaS→α 是非有效的⊭(⋀S)→α\\not\\models (⋀S) \\to \\alpha⊨(⋀S)→α\n\n\n\n\n证明 S⊨α→(S∧¬αS \\models \\alpha \\to (S \\land \\neg \\alphaS⊨α→(S∧¬α 是不可满足的)))\n如果 S⊨αS \\models \\alphaS⊨α，即 ∀J,J⊨S→J⊨α\\forall \\mathcal{J}, \\mathcal{J} \\models S \\to \\mathcal{J} \\models \\alpha∀J,J⊨S→J⊨α\n即 ¬¬∀J,J⊨S→J⊨α\\neg \\neg \\forall \\mathcal{J}, \\mathcal{J} \\models S \\to \\mathcal{J} \\models \\alpha¬¬∀J,J⊨S→J⊨α\n即 ¬∃J,J⊨S∧J⊨¬α\\neg \\exists \\mathcal{J}, \\mathcal{J} \\models S \\land \\mathcal{J} \\models \\neg \\alpha¬∃J,J⊨S∧J⊨¬α\n即 ¬∃J,J⊨S∧¬α\\neg \\exists \\mathcal{J}, \\mathcal{J} \\models S \\land \\neg \\alpha¬∃J,J⊨S∧¬α\n也就是说 S∧¬αS \\land \\neg \\alphaS∧¬α 不可满足。\n\n\n 五. 默认假设和推理\n 1. 默认假设\nFOL 具有严谨性(Strictness)和单调性(Monotonicity)，这使得 FOL 无法添加例外。\n默认假设 (Default Assumptions) 指的是在解释和推理时为一系列例外添加一个默认的处理方法，也就是对未知的事实如何处理的规则。\n例如所有鸟会飞可以用FOL表示：\n∀x. Bird(x)→Flies(x)\\forall x.\\ Bird(x) \\to Flies(x)\n∀x. Bird(x)→Flies(x)\n假设 aaa 是一只鸟，而 aaa 最近受了伤导致 aaa 无法飞行，也就是无法满足 Bird(a)→Flies(a)Bird(a) \\to Flies(a)Bird(a)→Flies(a)，也导致这个 FOL公式 是错误的。\n对于上述情况，我们纠正了原本的描述，用新的知识来表述：任何鸟都可以飞，除非我们知道一种特殊情况：\n∀x. Bird(x)∧Normal(x)→Flies(x)\\forall x.\\ Bird(x) \\land Normal(x) \\to Flies(x)\n∀x. Bird(x)∧Normal(x)→Flies(x)\nNormal(x)Normal(x)Normal(x) 的解释可能是任意的。一般来说，我们假设没有例外，也就是 Normal(x)Normal(x)Normal(x) 始终为真，除非我们明确声明 Normal(x)Normal(x)Normal(x) 并且知道它何时为假，这种假设未知始终为真的方法也称之为 开放世界假设（Open World Assumption, OWA）。\n\n\n\n\n\n\n\n\n\n在标准的一阶逻辑（FOL）中，默认采用的是 开放世界假设（OWA）。即如果一个事实在知识库中没有被明确陈述为真，也没有被推导为假，那么我们 不能 假设它是假。\n换句话说，知识不限于知识库本身。\n\n\n\n默认假设\n示例\n\n\n\n\n开放世界假设\n1. FOL2. OWL\n\n\n闭合世界假设\n1. Prolog\n\n\n\n 2. 默认推理\n默认推理（Default Reasoning） 是一种在信息不完备的情况下进行推理的方式。它允许我们根据“通常如此”的经验规则进行推理，除非有相反证据：\n\n当事实未知时，我们暂时假设一个“默认情况”为真。\n但如果后来得到与之冲突的事实，则撤回这个假设。\n\n由于 FOL 的单调性，在给定知识库 S⊨αS \\models \\alphaS⊨α 后，即使我们引入了新的知识 βββ，依旧无法反驳 ααα。但是默认推理是非单调性的(Non-monotonic Logic)，也就是引入一些例外知识 βββ 可以让 ααα 不成立或撤销。FOL不支持非单调性，因此 FOL 不存在默认推理。\n不过有多种方法可以扩展 FOL，使其接受默认推理，包括：\n\n闭合世界推理（Closed-world reasoning）\n限制推理（Circumscription）\n默认逻辑（Default logic）\n\n i. 闭合世界推理（Closed-world reasoning）\n闭合世界推理的前提是假定知识库完整和闭合世界假设 (CWA)。\n\n\n\n术语\n说明\n\n\n\n\n假定知识库完整的\n知识库包含了足够的知识来证明所有的真实事实\n\n\n闭合世界假设Closed-world Assumption (CWA)\n如果 α\\alphaα 不能被蕴含，那么 α\\alphaα 为假\n\n\n例子\n1. 知识库：Bird(A)∧Bird(B)Bird(A) \\wedge Bird(B)Bird(A)∧Bird(B)2. 语义上 Bird(A)∧Bird(B)⊭Bird(C)Bird(A) \\wedge Bird(B) \\not\\models Bird(C)Bird(A)∧Bird(B)⊨Bird(C)3. 那么 ¬Bird(C)\\neg Bird(C)¬Bird(C) 成立\n\n\n\n闭合世界推理具有不一致性（Inconsistency）：\n\n假设我们的知识库 S={P∨Q}S = \\{P \\lor Q\\}S={P∨Q}\n但是语义上 S⊭PS \\not\\models PS⊨P 和 S⊭QS \\not\\models QS⊨Q\n根据闭合世界假设，¬P∧¬Q\\neg P \\wedge \\neg Q¬P∧¬Q，因此 S⊨¬(P∨Q)S \\models \\neg (P \\lor Q)S⊨¬(P∨Q)。这与 SSS 不一致。\n\n ii. 限制推理（Circumscription）\n限制推理是闭合世界假设的一种更精确的版本：我们定义哪些谓词应该被假定为假，除非另有说明。\n例如，定义 Abnormal(x)Abnormal(x)Abnormal(x) 默认为假，在没有进一步知识的情况下，推理者会假设 ¬Abnormal(x)\\neg Abnormal(x)¬Abnormal(x)。\n限制推理具有不一致性（Inconsistency）：尼克松菱形（Nixon Diamond）。\n iii. 默认逻辑（Default logic）\n默认逻辑是基于开放世界假设 (WWA) 的。在默认逻辑中，FOL 知识库包括：\n\n\n\n组成成分\n说明\n\n\n\n\nFOL 语句\n通常的一阶逻辑语句\n\n\n默认规则\n默认规则定义了可以使用哪些默认知识\n\n\n\n默认规则形式：\nP:J1,J2,…,Jn/CP:J_1, J_2, \\dots, J_n / C\nP:J1​,J2​,…,Jn​/C\n\n\n\n符号\n解释\n\n\n\n\nPPP\n前提Prerequisite\n\n\nJiJ_iJi​\n证明条件Justifications\n\n\nCCC\n结论Conclusion\n\n\n\n如果 PPP 成立，并且所有的 JiJ_iJi​ 都无法被证明为假，那么可以得到结论 CCC。\n例如：Bird(x):Normal(x)/Flies(x)Bird(x): Normal(x) / Flies(x)Bird(x):Normal(x)/Flies(x) 中\n\n\n\n符号\n解释\n\n\n\n\nPPP前提\nxxx 是一只鸟\n\n\nJiJ_iJi​条件\nxxx 是一只正常鸟\n\n\nCCC结论\nxxx 会飞\n\n\n\n只要不能证明 Normal(x)Normal(x)Normal(x) 为假，就能得出结论 Flies(x)Flies(x)Flies(x)。\n 六. FOL 推理系统\n理想的推理系统包含：\n\n\n\n成分\n说明\n\n\n\n\n知识库\nKB\\text{KB}KB\n\n\n公式\nα\\alphaα\n\n\n推理过程\n证明 KB⊨α\\text{KB} \\models \\alphaKB⊨α\n\n\n\n总而言之，推理就是要证明知识库是否蕴含某一个公式。\n 1. FOL 推导定义\n推导(Derivation)是基于推理系统从知识库中能够挖掘出的信息/结论。\n集合 SSS 到 公式 ccc 的推导的过程是从一个公式的集合 SSS 出发，根据推理系统不断推理出新的有限公式序列 c1,c2,…,cnc_1, c_2, \\dots, c_nc1​,c2​,…,cn​，其中 cic_ici​ 由 S∪{c1,c2,…,ci−1}S \\cup \\{c_1, c_2, \\dots, c_{i-1}\\}S∪{c1​,c2​,…,ci−1​} 推导出，最终得到公式 c=cnc = c_nc=cn​。我们写作 S⊢cS \\vdash cS⊢c。\n基础规则：如果 α∈S\\alpha \\in Sα∈S，那么 S⊢αS \\vdash \\alphaS⊢α (公式序列为空)。\n 2. FOL 推理系统的性质\n i. 合理与完备性\n\n\n\n\n\n\n\n\n\nNo FOL reasoning system is both sound and complete.\n没有一个FOL的推理系统即合理又完备的。\n但是：\n\nFOL 语言的子集存在即合理又完备的推理系统，例如命题逻辑。\n存在合理的、反驳完备性的FOL推理系统。此时通常使用将被证明蕴含的句子的反义加入到知识系统证明其不可满足性来证明其是否被蕴含。\n\n例如 Z3 的推理系统，但 Z3 狭义上不属于一种推理系统，而是一种求解器。\n\n\n\n ii. 可判断性\n由图灵证明得出 FOL 语言是不可判断的（停机问题）。\n但 FOL 是半可判断的：\n\n若公式可满足，可以在有限时间内证明其满足性。\n若公式不可满足，那么可能永不终止。\n\n 3. FOL 归结推理\n归结推理的定义请先看 命题逻辑 的 三. 命题逻辑的推理系统。\nFOL 的归结推理是基于扩展的 CNF 进行的，它是基于下面两个假设进行的：\n\n不存在平等连接词，如 ===。\n转化为 CNF 后不包含存在量词 ∃\\exists∃。\n\n满足这两点基本能保证系统是反驳完备的，即如果 SSS 不可满足，那么 S⊢⊥S \\vdash \\botS⊢⊥。\n 1. FOL 的归结类型\n\n\n\n类型\n说明\n\n\n\n\n基础归结\n1. 一个全部变量 + 一个项2. 假设变量 xxx 由全部量词 ∀\\forall∀ 绑定，ttt 是一个项3. 如果存在子句 [P(x),A(x)][P(x), A(x)][P(x),A(x)] 和 [P(t)‾,B][\\overline{P(t)}, B][P(t)​,B]，那么存在归结 [A(t),B][A(t), B][A(t),B]4. 这是因为如果 ∀x. [P(x),A(x)]\\forall x.\\ [P(x), A(x)]∀x. [P(x),A(x)] 成立，那么带入 ttt 得到 [P(t),A(t)][P(t), A(t)][P(t),A(t)]，也就是基础的归结了\n\n\n两个全部变量归结\n1. 两个全部变量2. 假设变量 x1,x2x_1, x_2x1​,x2​ 是由全部量词 ∀\\forall∀ 绑定3. 如果存在子句 [P(x1),A(x1)][P(x_1), A(x_1)][P(x1​),A(x1​)] 和 [P(x2)‾,B][\\overline{P(x_2)}, B][P(x2​)​,B]，那么存在归结 [A(x1),B][A(x_1), B][A(x1​),B] 和 [A(x2),B][A(x_2), B][A(x2​),B]4. 这是让 x1=x2x_1 = x_2x1​=x2​ 得到的\n\n\n复杂归结\n1. 假设变量 x1,x2,x3,x4x_1, x_2, x_3, x_4x1​,x2​,x3​,x4​ 是全部量词 ∀\\forall∀ 绑定的2. 假设 f,gf, gf,g 是一元函数，PPP 是二元谓词3. 如果存在子句 [P(f(x1),x2),A][P(f(x_1), x_2), A][P(f(x1​),x2​),A] 和 [P(x3,g(x4))‾,B][\\overline{P(x_3, g(x_4))}, B][P(x3​,g(x4​))​,B]，那么存在归结 [A,B][A, B][A,B]4. 这是让 x3=f(x1),x2=g(x4)x_3 = f(x_1), x_2 = g(x_4)x3​=f(x1​),x2​=g(x4​) 得到的\n\n\n\n 2. 消除假设\n该节介绍如何消除存在量词 ∃\\exists∃ 和平等连接词 ===。\n i. 消除存在量词 ∃\\exists∃：Skolemisation\nSkolemisation 是一个引入常量来替换存在变量的方法，通过证明转化后的语句的可满足性来证明语句的正确性。\n\n零元 Skolem 函数：\n\n对于每个由于存在性 ∃\\exists∃ 绑定的变量，引入一个新的 常量 或者是 零元的函数 来代替变量，并消除存在性。我们将这个函数/变量称为 Skolem 函数。\n例如，∃x.P(x)\\exists x.P(x)∃x.P(x)，引入 aaa，将句子代替为 P(a)P(a)P(a)。如果 ∃x.P(x)\\exists x.P(x)∃x.P(x)，那么就说明 P(a)P(a)P(a) 是可满足的，反之亦然，这俩是等价的。\n因为没有对 aaa 添加更多的限制，所以只需要找到一个满足转化后的句子的 aaa 的值，就证明了 ∃x.P(x)\\exists x.P(x)∃x.P(x) 成立，此时 x=1x = 1x=1。\n至于怎么找到这个可满足性就是推理系统需要处理的事了。\n\n多元 Skolem 函数：\n\n如果一个谓词不止被一种量词变量限制，那么我们需要对这个存在性所依赖的变量的每个值都使用 Skolem 函数代替。\n如果一个存在量词受到其他量词变量的约束，那么 Skolem 函数是依赖于这些量词的，应该使用多元 Skolem 函数。\n例如，∀x.∃y.P(x,y)\\forall x.\\exists y.P(x, y)∀x.∃y.P(x,y) 是对每一个 xxx 都存在一个 yyy，因此这个 yyy 是依赖于 xxx 的，因此此时 Skolem 函数是多元的，该句子应该转化为 ∀x.P(x,a(x))\\forall x.P(x, a(x))∀x.P(x,a(x))。\n而对于 ∃y.∀x.P(x,y)\\exists y.\\forall x.P(x, y)∃y.∀x.P(x,y) 的 yyy 是不依赖于 xxx 的，因此该句子转化为 ∀x.P(x,a)\\forall x.P(x, a)∀x.P(x,a)。\n ii. 消除平等连接词 ===\n相等是一种关系，因此我们可以把它当做一种谓词。我们可以视作 x=yx = yx=y 为 Equals(x,y)Equals(x, y)Equals(x,y)。\n但是FOL没有谓词的解释机制。我们只能通过添加限制来保证 Equals(x,y)Equals(x, y)Equals(x,y) 等价于 =。\n重点：我们的方法不能保证 Equals 的解释一定如我们所期望的那样，但它足以证明不满足性。\n在离散数学中，同时满足自反性、对称性和传递性的关系被称为等价关系。这种等价关系不一定是等于。\n\n添加自反性 (reflexivity)：∀x. Equals(x,x)\\forall x.\\ Equals(x, x)∀x. Equals(x,x)\n添加对称性 (symmetry)：∀x. ∀y.(Equals(x,y)⇒Equals(y,x))\\forall x.\\ \\forall y.(Equals(x, y) \\Rightarrow Equals(y, x))∀x. ∀y.(Equals(x,y)⇒Equals(y,x))\n添加传递性 (transitivity)：∀x. ∀y. ∀z.(Equals(x,y)∧Equals(y,z)⇒Equals(x,z))\\forall x.\\ \\forall y.\\ \\forall z.(Equals(x, y) \\land Equals(y, z) \\Rightarrow Equals(x, z))∀x. ∀y. ∀z.(Equals(x,y)∧Equals(y,z)⇒Equals(x,z))\n\n其次为每个函数添加等于的限制：\n\n对于任意一个单元谓词 PPP，∀x.∀y. Equals(x,y)⇒P(x) ⇔ P(y)\\forall x.\\forall y.\\ Equals(x, y) \\Rightarrow P(x)\\ \\Leftrightarrow\\ P(y)∀x.∀y. Equals(x,y)⇒P(x) ⇔ P(y)\n对于任意一个单元函数 fff，∀x.∀y. Equals(x,y)⇒Equals(f(x),f(y))\\forall x.\\forall y.\\ Equals(x, y) \\Rightarrow Equals(f(x), f(y))∀x.∀y. Equals(x,y)⇒Equals(f(x),f(y))\n对于任意一个多元谓词，使用排列组合的进行一次相等判断。\n\n该方式并不高效，它会产生大量的句子。更有效的处理是在推理系统中对等于给予特殊处理，例如新的添加推断规则，而不是在知识库中添加限制。\n 3. 可判断性\n\n\n\n\n逻辑命题\nFOL\n\n\n\n\n证明蕴含(不可满足性)\n有限的迭代内\n有限的迭代内\n\n\n反驳蕴含(可满足性)\n有限的迭代内\n可能永不停止\n\n\n\n在证明蕴含方面：\n\n\n逻辑命题中：\n\n归结推理系统是可以在有限的时间内证明的。即在逻辑命题中它是可判断的。\n\n这是因为具有有限的子句，因此其能够保证终止。\n\n\n即使是最短的不可满足情况的推导也可能需要指数(Exponential)数量的子句。即其时间复杂度是指数型的。\n\n迭代的次数（运行的时间）取决于子句的生成顺序。越早生成出空语句越早停止。\n\n\n\n\n\n在 FOL 中：\n\n归结系统可能永不停止。即在 FOL 中它是不可判定的。\n没有固定的时间复杂度，是没有定义的(Undefined)。\n\n\n\nUNSAT 问题（证明不可满足性）是一个 NP-hard 问题，是 co-NP-complete 问题（其否定是 NP-complete 问题）。\n 七. FOL 的霍恩子句\n霍恩子句定义请先看 命题逻辑 的 四. 霍恩子句。\n由于要使用归结推理，FOL的霍恩语句是不存在平等连接词 === 和存在量词 ∃\\exists∃ 的扩展 CNF 公式，且最多包含一个正子句。同命题逻辑的霍恩语句一样，其包含正霍恩语句和负霍恩语句。\n例如 正 FOL 霍恩语句 [P(x),Q1(x)‾,Q2(y)‾][P(x), \\overline{Q_1(x)}, \\overline{Q_2(y)}][P(x),Q1​(x)​,Q2​(y)​] 表示 ∀x. ∀y. P(x)∨¬Q1(x)∨¬Q2(y)\\forall x.\\ \\forall y.\\ P(x) \\lor \\neg Q_1(x) \\lor \\neg Q_2(y)∀x. ∀y. P(x)∨¬Q1​(x)∨¬Q2​(y)。\nFOL 的霍恩子句的可满足性证明是不可判定的，不存在保证终止的蕴涵检查算法。\n 1. Prolog 语言\nProlog 语言是一种声明式(Declarative)逻辑编程语言。\n\n数据类型：\n\nProlog 仅支持一种项（term）数据类型，可以是：\n\n\n\n类型\n表示\n说明\n\n\n\n\n原子Atoms\n以小写字母开头例如 a\n表示布尔型\n\n\n数字Numbers\n数字\n支持浮点数和整数\n\n\n变量Variables\n大写字母或者下划线开头例如 X, _good\n可以为任意值\n\n\n复合项Compound Term\n小写字母开头 + 括号例如 is_sunny(july, spain)\n1. is_sunny 是函数符 (functor)2. july, spain 是参数 (arguments)特殊：1. 原子是没有参数的复合项2. 列表是一种复合项，例如 [uk, [spain, 1]]3. 字符串是一种复合项，例如 &quot;Hello&quot;\n\n\n\n\n程序\n\n一个 Prolog 程序是由一组 子句（Clauses） 组成的，类型由：\n\n\n\n类型\n说明\n\n\n\n\n规则Rules\n是定义项之间的关系的正霍恩子句1. HEAD :- BODY 是 BODY→HEAD\\text{BODY} \\to \\text{HEAD}BODY→HEAD，即 [HEAD,BODY‾][\\text{HEAD}, \\overline{\\text{BODY}}][HEAD,BODY]2. HEAD 只能是一个项3. BODY 可以包含项的合取和析取a. a, b, c 表示 a∧b∧ca \\land b \\land ca∧b∧cb. a; b 表示 a∨ba \\lor ba∨bc. a, b; c 表示 (a∧b)∨c(a \\land b) \\lor c(a∧b)∨cd. 注意 BODY 是要取反的，是负文字集合\n\n\n事实Facts\n是陈述已知为真的信息的正子句1. 一个空 BODY 的规则可以为事实，例如 a. 表示 a = True2. 复合项：例如 cat(tom)，表示 tom 是一只猫3. 关联变量：例如 length(List, L) 表示用变量 L 代表列表的长度\n\n\n查询Query\n需要验证的负霍恩子句1. 使用 $?- BODY 验证\n\n\n\n当用户输入查询时，Prolog 会尝试证明 BODY 被蕴涵。如果查询包含变量，Prolog 还需要找到使查询可满足的变量赋值。\nProlog 使用 SLD 归结 来处理否定查询，尝试推导出空子句。\n例如：\n123cat(kitty).cat(X) :- parent(X, Y), cat(Y).parent(tom, kitty)\n查询 ?- cat(X).\n会得到\n123X = kitty; # kitty 是猫X = tom;   # tom 是猫false.     # 没有更多满足条件的 X 了\n\n&lt;返回符号人工智能导航\n","slug":"笔记/符号人工智能/一阶逻辑FOL","date":"2024-10-18T10:00:01.000Z","categories_index":"笔记-符号人工智能","tags_index":"Symbolic Artificial Intelligence,Logic,Formal Language,Propositional Logic,FOL,First Order Logic,Resolution,Horn Clauses,Skolemisation","author_index":"zExNocs"},{"id":"f1e0f2fa26472b39333df7e3270b73cd","title":"SAI-符号人工智能导航","content":" 一. 文章导航\n\n\n\n\n\n\n\n特别注意\n阅读其他文章前请务必将导航下面的其他章节看完。\n\n\n\n\n\n笔记\n说明\n\n\n\n\n基本概念和术语\n本文章，阅读其他文章前务必看完该章节\n\n\n一阶逻辑First Order Logic, FOL\n一种基于形式语言和命题逻辑的知识表示\n\n\n命题逻辑\n一种基于 FOL 的知识表示，是不带有论域的 FOL\n\n\n限制满足问题\n是一个 SAT 问题的扩展，同时也是一种知识的代表\n\n\n描述逻辑\n围绕对象的类别构建的知识的代表\n\n\n编程库\n有关可以寻找 FOL 和 CSP 问题的可满足性解释的 python 库介绍\n\n\n\n 二. 介绍\n\n\n\n\n\n\n\n\n\nHow can knowledge be represented symbolically and manipulated in an automated way by reasoning programs.\n符号学人工智能是如何用符号来表示知识，并通过推理程序以自动化的方式进行操控知识。\n符号人工智能又称为基于规则的人工智能，是一种通过符号表示和逻辑推理来模拟人类的人工智能技术。它依赖于明确的、可解释的规则和逻辑系统来处理问题和推理。符号AI则依赖于人类预先定义的规则和知识库。\n符号AI的主要局限是难以处理用符号表示的隐性知识（如直觉、技能）。\n 1. SAI 组成成分\n\n\n\n术语\n说明\n类型\n\n\n\n\n知识Knowledge\n1. 广义：知识是这个世界的任何信息2. 符号：符号AI的核心就是通过符号来表示这些知识符号可以是文字、数字或其他抽象符号，它们用来表示事实与规则\n1. 显性知识(explicit)：是可以明确表达、描述、编码、传递的知识2. 隐性知识(implicit)：是难以用语言或符号明确表达的知识，是通过经验、直觉、技能、实践获得的\n\n\n知识表示Representation\n1. 广义：如何用/使用哪一种语言表示知识2. 符号：用符号结构来表达世界中的事实、规则和概念\n类型：1. 谓词逻辑(Predicate Logic): 用命题、谓词、量词表示知识2. 语义网络(Semantic Network): 用节点表示概念、边表示关系3. 框架(Frame): 描述对象的属性与层级\n\n\n推理Reasoning\n1. 广义：推理是从已知的知识中提取更多的知识2. 符号：推理是利用蕴含关系或推理规则从已知事实推出未知结论的过程3. 符号AI都是形式推理的(Formal Reasoning)，是基于一定规则通过逻辑推导来完成的，因此逻辑是可追踪、可解释的4. 推理得到的知识是显性知识\n1. 推理的例子：如果知道 Fido 是一条狗，所有狗都是哺乳动物，因此Fido是哺乳动物2. 类型：a. 归结推理(Resolution Reasoning)b. 演绎推理(Deductive Reasoning)c. 归纳推理(Inductive Reasoning)d. 溯因推理(Abductive Reasoning)\n\n\n\n 2. 基础术语\n\n\n\n术语\n说明\n\n\n\n\n知识库Knowledge Base\n1. 知识库是存储系统已知知识的地方，通常是 规则、事实和对象 及其关系的集合2. 知识库里的知识是显式的(explicit)，也可能是元知识(meta)，即如何使用知识的知识3. 传统知识库需要人工构建和维护，通常是通过专家将领域知识转化为可操作的规则\n\n\n推理引擎Inference Engine\n1. 是利用推理过程从知识库自动提取新的知识的算法2. 推理引擎通常会被赋予一个假设，并尝试证明或者反驳它\n\n\n知识型系统Knowledge-Based System\n1. 是使用由用户指定或者自动学习的知识库和特定的推理引擎自动提取新的知识的系统2. 可以说 KB 是一个语句集合，在 KB-System 中假设 KB 是真的3. 知识型系统可解释性高，但缺点是需要大量领域知识\n\n\n解释器Interpreter\n1. 解释器是负责控制推理过程的执行：a. 确定使用哪条规则b. 决定前向推理（从已知到结论）还是后向推理（从目标倒推）c. 管理冲突解决（多条规则同时匹配时的选择策略）2. 解释器向用户解释推理过程（如何得出结论），这有助于系统透明性3. 解释器是推理引擎的核心组件，它控制推理步骤，但推理规则本身定义在知识库中，因此推理的逻辑独立于具体解释器实现(知识型系统不包含用户提供的具体解释器)\n\n\n语义Semantics\n1. 语义是符号与所指对象之间的关系2. 语义负责规定符号的解释规则，也就是与现实世界的映射\n\n\n蕴含Entailment\n1. 蕴含不是属于知识库的内容，而是由知识库决定的语义关系，不依赖于任何证明系统或推理规则2. 可以说 知识库 是一组已知事实（命题/公式），蕴含关系 是基于这些事实所“语义上必然成立”的结果3. 对于蕴含出事实 α\\alphaα，写作 KB⊨α\\text{KB} \\models \\alphaKB⊨α\n\n\n推导Derivation\n1. 从知识库出发使用推理规则逐渐得到一个有限证明序列，并最终得到新的公式 α\\alphaα 的过程2. 推导是一个逐渐推断的过程，推理系统会提供推断的规则3. 写作 KB⊢α\\text{KB} \\vdash \\alphaKB⊢α4. 相比于蕴含，推导是基于推理系统的纯语法概念，强调基于规则的形式证明5. KB⊢α\\text{KB} \\vdash \\alphaKB⊢α 是推理系统得到的结果，KB⊨α\\text{KB} \\models \\alphaKB⊨α 是知识系统的一个事实\n\n\n\n 3. 推理系统的属性\n\n\n\n性质\n说明\n\n\n\n\n合理性Sound\n1. 推导系统推导出的所有知识在语义上都是真命题2. 即 KB⊢α→KB⊨α\\text{KB} \\vdash \\alpha \\to \\text{KB} \\models \\alphaKB⊢α→KB⊨α3. 简单说，就是推导的都是对的4. 将推导出的语句 ccc 加入到知识库并不会改变其评估即 KB  ⟺  KB∧c\\text{KB} \\iff \\text{KB} \\land cKB⟺KB∧c\n\n\n完备性Complete\n1. 所有在语义上为真的陈述都可以通过该逻辑系统推导出2. 即 KB⊨α→KB⊢α\\text{KB} \\models \\alpha \\to \\text{KB} \\vdash \\alphaKB⊨α→KB⊢α3. 简单说，就是对的都可以推导出\n\n\n反驳完备性Refutation-Complete\n1. 所有不可满足的知识库都可以通过该推导系统推导出 False\\text{False}False2. 即如果 KB\\text{KB}KB 不可满足，那么 KB⊢False\\text{KB} \\vdash \\text{False}KB⊢False3. 可通过该性质来证明蕴含，即证明 KB∧¬α\\text{KB} \\land \\neg \\alphaKB∧¬α 不可满足\n\n\n可判断性Decidability\n存在一个算法，能够在有限时间内决定任意给定公式是否是可满足的\n\n\n\n 4. 推理系统的可判断性总结\n在证明蕴含性的推理系统中，一般可判断性有：\n\n\n\n知识系统\n可判断性\n\n\n\n\nFOL\n不可判断\n\n\n命题逻辑\nNP-Complete\n\n\nCSP有限定义域\nNP-Complete\n\n\nCSP无限定义域\n不可判断的\n\n\nFOL 的霍恩子句\n不可判断的\n\n\n命题逻辑的霍恩子句\n多项式的Polynomial\n\n\n\n 5. 推理系统的改善建议\n\n\n\n改善方法\n说明\n\n\n\n\n分解问题Decompose the problem into subproblems\n1. 将大问题拆分成多个较小的子问题，各自独立求解2. 这样做有助于减少每次求解的复杂度，并能够更好地并行化处理\n\n\n打破对称性Break the symmetry of the problem\n1. 许多问题中存在对称性（例如变量或解法等价）2. 通过识别并消除这些对称性，可以减少重复搜索，从而提高效率\n\n\n改变问题形式化表达Change the formulation\n根据问题特性，使用更高效或适合的数学模型或逻辑表达形式\n\n\n改进求解器Improve the solver\n1. 采用更强大的算法或优化技术，比如使用启发式搜索、剪枝策略或更快的求解工具2. 变量优先级\n\n\n\n变量优先级是选择对多个约束有强制性影响的变量。高优先级的变量能够通过推理传播影响其他变量，从而减少搜索空间。\n\n\n\n方法\n说明\n\n\n\n\n分析变量的传播能力\n解算器会分析每个变量的潜在影响，选择能够最大程度触发约束传播的变量\n\n\n利用启发式方法Heuristics\n1. 解算器通常使用多种启发式技巧来预测哪些变量最重要，从而快速优化搜索过程2. 启发式方法是一种“智能”猜测，可以显著提高性能，尤其是在复杂的约束问题中\n\n\n用户提供的洞察\n在某些情况下，用户可以为解算器提供关于问题结构的洞察，例如哪些变量更重要，从而帮助解算器做出更明智的优先级决策\n\n\n\n 三. 符号人工智能比较\n\n\n\n\n符号人工智能Symbolic AI, SAI\n非符号人工智能Non-symbolic AI, NSAI\n\n\n\n\n求解\n数据和规则都是显性的(explicitly)\n模型是隐式的(implicit)\n\n\n来源\n模型都是由人类提供的\n模型都是根据模式观察自动从数据提取的\n\n\n推理\n形式推理\n统计或隐式推理\n\n\n优点\n1. 可解释性强2. 适合底数据依赖场景\n1. 处理不确定性能力强2.学习能力强\n\n\n局限\n1. Rigid model，难以更新2. 输入必须明确3.有些任务难以形式化\n1.难以或不可能解释2.可能存在一些事情不可能解决?\n\n\n\n尽管NSAI在如今更流行，但是SAI依然在被广泛应用。它们之间并没有直接的可比性，都具有优缺点。\n","slug":"笔记/符号人工智能/符号人工智能导航","date":"2024-10-18T10:00:00.000Z","categories_index":"导航","tags_index":"Symbolic Artificial Intelligence,Logic,AI,Formal Reasoning","author_index":"zExNocs"},{"id":"4ac59f43cb50a2e426707404508a7b53","title":"LaTeX","content":" 一. 模板\n使用 \\documentclass[option]{template}\noption一般表示文字编码，例如 [UTF8]\ntemplate表示模板\n\n\n\n模板\n介绍\n\n\n\n\nreport\n报告\n\n\narticle\n文章\n\n\nctexart\n支持中文的文章\n\n\nctexbook\n支持中文的书\n\n\n\n使用 \\begin{document} 和 \\end{document} 块内表示文件开始和结束\n 二. 包\n使用 \\usepackage{package} 来导入包\n\n\n\n包名\n介绍\n补充\n\n\n\n\ngraphicx\n图片\n使用\\includegraphics[param]{filename}导入图片  具体见下方的 七. 图片相关\n\n\n\n 三. 标题\n\n\n\n代码\n解释\n补充\n\n\n\n\n\\titile{&lt;&gt;}\n文章标题\n\n\n\n\\author{&lt;&gt;}\n文章作者\n\n\n\n\\date{param}\n文章日期\n\\today 今天\n\n\n\n 四. 格式化命令\n\n\n\n代码\n解释\n\n\n\n\n\\textbf{&lt;&gt;}\n文字加粗\n\n\n\\textit{&lt;&gt;}\n文字斜体\n\n\n\\underline{&lt;&gt;}\n下划线\n\n\n\n 五. 排版\n\n\n\n代码\n解释\n补充\n\n\n\n\n\\part\n部\n在 book 模板中使用\n\n\n\\chapter\n章\n在 book 模板中使用\n\n\n\\section{&lt;&gt;}\n章节\n\n\n\n\\subsection{&lt;&gt;}\n子章节\n\n\n\n\\subsubsection{&lt;&gt;}\n三级章节\n\n\n\n\n 六. 环境\n环境指的是使用 \\begin{name} 和 \\end{name} 嵌套的代码。\n环境内部的文字使用同一种格式。\n\n\n\n环境\n解释\n补充\n\n\n\n\nfigure\n图片环境\n具体看图片相关\n\n\nitemize\n无序bullet列表\n使用\\item表示一个元素\n\n\nenumerate\n有序bullet列表\n使用\\item表示一个元素\n\n\ntable\n表格环境\n具体查看八. 表格相关\n\n\ntabular\n表格内容\n具体查看八. 表格相关\n\n\nequation\n公式环境\n可以简写为\\[ 和 \\]具体查看九. 数学公式相关\n\n\n\n 七. 图片相关\n使用 \\includegraphics[param]{filename} 的方式导入图片。其中param可以有：\n\n\n\n参数\n解释\n补充\n\n\n\n\nwidth=x\n将图片宽度设置为x\n可以是 x\\textwidth，表示原本大小的 x 倍\n\n\nheight=x\n将图片高度设置为 x\n一般高度和宽度选择最小的修改\n\n\n\n此外，可以使用 figure 嵌套，在内部可以使用：\n\n\n\n代码\n解释\n\n\n\n\n\\caption\n图片标题\n\n\n\\centering\n图片居中\n\n\n\n 八. 表格相关\n使用 \\begin{table} 和 \\end{table} 规定表格环境。在内部可以使用：\n\n\\center 表格居中\n\\caption{anything} 表格标题\n\n使用 \\begin{tabular}{param} 和 \\end{tabular} 环境嵌套表格内容。\n\nparam 表示表格的列的数量和格式:\n\n\n\n\n参数\n解释\n补充\n\n\n\n\nl\n左对齐。\n会添加新的一列。\n\n\nc\n居中。\n会添加新的一列。\n\n\nr\n右对齐。\n会添加新的一列。\n\n\np{width}\n设置列宽\n例如p{2cm}会添加新的一列。\n\n\n|\n表格绘制竖边框\n不会添加新的一列。\n\n\n\n\n使用 &amp; 隔开不同列之间。\n使用 \\\\ 隔开不同行之间。\n使用 \\hline 绘制水平方向的边框\n\n 九. 数学公式相关\n\n行内公式\n行内公式使用 $&lt;&gt;$ 嵌套公式\n行外公式\n使用 \\begin{equation} 和 \\end{equation} 环境嵌套。\n\n运算符号：\n\n\n\n代码\n解释\n补充\n例子\n\n\n\n\n\\over\n分数形式呈现\n\n1 \\over 2 → 121 \\over 221​\n\n\n\n","slug":"笔记/LaTeX/LaTeX","date":"2024-10-15T10:00:00.000Z","categories_index":"笔记-LaTeX","tags_index":"LaTeX","author_index":"zExNocs"},{"id":"ed2691e46b19986acffbcbe7e26041d1","title":"随笔-让Aurora支持KaTeX和Emoji","content":" ✒️背景\n最早接触markdown是在洛谷的个人博客时使用的。其中有一个功能我特别喜欢用，就是用$$符号展示出的公式，类似于这样：f(x)=eΔkTf(x)=e^{\\frac{\\Delta}{kT}}f(x)=ekTΔ​，以至于以后只要用到有关字母公式的地方就会喜欢性地打出$$。\n但是我在使用Aurora的时候，发现它并不能直接使用 $$ 公式，研究发现是Hexo默认的markdown渲染器不支持KaTeX，故有此文章来研究如何让Hexo支持。\n\n 🔖内容\nHexo的markdown渲染器有很多种：Hexo多种Markdown渲染器对比分析。\nHexo自带的markdown渲染器不支持KaTeX，也不支持emoji。而网络上常见的pandoc在部署到Aurora时会出现很多问题。这里我使用的是markdown-it-plus，它原生就支持emoji和KaTeX插件。具体步骤如下：\n\n卸载原生渲染器 npm un hexo-renderer-marked --save\n安装新渲染器 npm i hexo-renderer-markdown-it-plus --save\n添加选项：在根目录下 _config.yml 添加以下字段：\n\n12345678910markdown_it_plus:    highlight: true    html: true    xhtmlOut: true    breaks: true    langPrefix:    linkify: true    typographer:    quotes: “”‘’    pre_class: highlight\n\n添加css：在根目录下 _config.aurora.yml 找到 injects 字段，并添加css，参考如下：\n\n12345678#! ---------------------------------------------------------------#! Injections#! @docs https://aurora.tridiamond.tech/guide/site-meta.html#custom-meta#! ---------------------------------------------------------------injects:  scripts:  css:    - &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css&quot;&gt;\n\n清理并重新生成Hexo hexo clean &amp; hexo g -d\n\n 📘Reference\n\nHexo多种Markdown渲染器对比分析: https://zsyyblog.com/b73ceb85.html\nhexo-renderer-markdown-it-plus官方文档：https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\nAurora官方插件文档：https://aurora.tridiamond.tech/cn/configs/site-meta.html\n\n","slug":"随笔/随笔-让Aurora支持KaTeX和Emoji","date":"2024-05-26T16:00:00.000Z","categories_index":"随笔","tags_index":"Hexo,Aurora,KaTeX","author_index":"zExNocs"},{"id":"41e0a3409ce43bb21bec33e2e3fc9812","title":"FLAT-上下文无关语言(CFL)","content":"&lt;返回形式语言与自动机理论导航\n\n 零. 阅读须知\n如果遇到任何不懂的术语，请查阅 形式语言与自动机理论导航。\n 一. 上下文无关文法 CFG\n上下文无关文法(Context-Free Grammar, CFG) 一般由四元组 G=(N,T,P,s)G=(N, T, P, s)G=(N,T,P,s) 表示：\n\n\n\n符号\n意义\n\n\n\n\nNNN\n变元/非终结符的有穷集\n\n\nTTT\n终结符的有穷集即字母表Σ\\SigmaΣN∩T=∅N \\cap T = \\varnothingN∩T=∅\n\n\nPPP\n产生式的有穷集\n\n\nsss\n初始符号，文法开始的地方s∈Ns \\in Ns∈N\n\n\n\n上下文无关文法的产生式要求左部只有一个非终结符，即 A→αA \\to \\alphaA→α，A∈NA \\in NA∈N，α\\alphaα 可以为任意 N∪TN \\cup TN∪T 元素组成的串。\n与正则文法不同，其对右部没有限制。\n 1. 回文语言\n\n\n\n定义\n解释\n\n\n\n\nwRw^RwR\n字符串的反转\n\n\n回文字符串Palindrome\n如果字符串 w∈Σ∗w \\in \\Sigma^*w∈Σ∗是回文的1. 一般定义：w=wRw = w^Rw=wR2. 递归定义：一般情况：∣w∣=0 or 1|w| = 0 \\text{ or } 1∣w∣=0 or 1 是回文字符串递归情况：如果 www 是回文字符串，∀a∈Σ:awa\\forall a \\in \\Sigma: awa∀a∈Σ:awa 是回文字符串\n\n\n回文语言\n字母表组成的所有回文字符串的集合构成回文语言，即Lpal={w∣w∈Σ∗∧w=wR}L_{pal} = \\{w \\mid w \\in \\Sigma^* \\land w = w^R \\}Lpal​={w∣w∈Σ∗∧w=wR}1. 回文语言不是正则语言2. 回文语言是上下文无关语言\n\n\n\n根据递归定义，视别回文语言的上下文无关文法为(假设字母表为 {0,1}\\{0, 1\\}{0,1})：\nG=({A},{0,1},{A→ε∣0∣1∣0A0∣1A1},A)G = (\\{A\\}, \\{0,1\\}, \\{A \\rightarrow \\varepsilon|0|1|0A0|1A1\\}, A)\nG=({A},{0,1},{A→ε∣0∣1∣0A0∣1A1},A)\n 2. CFG 语法树/派生树\nCFG语法分析树又称为派生树(Derivation trees)，是用树的结构来表示CFG文法派生的过程。\n派生树与派生一一对应 (派生定义请看导航的基本概念中文法的派生)，组成成分为：\n\n\n\n组成成分\n说明\n\n\n\n\n根节点\n初始符号 SSS\n\n\n叶节点\n终结符\n\n\n内节点\n非终结符/变元\n\n\n子树子节点\n子树可以表示为父节点的一个产生式子节点为该产生式的右部\n\n\n产物\n将树的叶节点从左到右(无视高度)结合起来就是该派生树派生出的字符串(产物)\n\n\n\nCFG语法分析树具有确定性：每棵派生树都有唯一的最左(右)派生。\n\n派生树例子\n考虑到这个文法 G=({S,A,B},{a,b},P,S)G = (\\{S, A, B\\}, \\{a, b\\}, P, S)G=({S,A,B},{a,b},P,S)\nP={S→ABA→aS∣εB→Sb∣εP = \\begin{cases}\nS \\rightarrow AB \\\\\nA \\rightarrow aS \\mid \\varepsilon \\\\\nB \\rightarrow Sb \\mid \\varepsilon\n\\end{cases}\nP=⎩⎪⎪⎨⎪⎪⎧​S→ABA→aS∣εB→Sb∣ε​\n该文法的一种到 aaa 的派生为：\nS⇒GAB⇒GaSB⇒GaABB⇒GaBB⇒GaB⇒GaS \\underset{G}\\Rightarrow AB \\underset{G}\\Rightarrow aSB \\underset{G}\\Rightarrow aABB \\underset{G}\\Rightarrow aBB \\underset{G}\\Rightarrow aB \\underset{G}\\Rightarrow a\nSG⇒​ABG⇒​aSBG⇒​aABBG⇒​aBBG⇒​aBG⇒​a\n其派生树为：\n\n\n\n\n\n 3. 歧义 (Ambiguity)\n i. 歧义的定义\n如果 CFG GGG 使某个字符串有两种不同的派生和派生树，则称该文法 GGG 是歧义的(Ambiguity)。\n判定一个 CFG 是否是歧义的这个问题是不可判定的，即没有固定的算法来证明。\n\n歧义的例子：没有优先级的算数表达式 CFG\n考虑没有优先级的算数表达式文法：\nGexp={N,T,P,S}G_{exp} = \\{N,T,P,S\\}\nGexp​={N,T,P,S}\n\n\n\n符号\n说明\n\n\n\n\nNNN\nN={E,I,D}N = \\{E,I,D\\}N={E,I,D}E:E:E: 表达式ExpressionI:I:I: 整型IntegerD:D:D: 数字Digital\n\n\nTTT\nT={0,1,…,9,+,∗,(,)}T = \\{0,1,\\dots,9,+,*,(,)\\}T={0,1,…,9,+,∗,(,)}终结符是数字和符号\n\n\nSSS\nS=ES = ES=E初始符号是表达式\n\n\nPPP\nE→E+E∣E∗E∣(E)∣II→DI∣DD→0∣1∣2∣3∣4∣5∣6∣7∣8∣9\\begin{aligned}E &amp;\\rightarrow E + E\\\\&amp;\\mid E * E\\\\&amp;\\mid (E)\\\\&amp;\\mid I\\\\[4pt]I &amp;\\rightarrow DI \\mid D\\\\[4pt]D &amp;\\rightarrow 0 \\mid 1 \\mid 2 \\mid 3 \\mid 4 \\mid 5 \\mid 6 \\mid 7 \\mid 8 \\mid 9\\end{aligned}EID​→E+E∣E∗E∣(E)∣I→DI∣D→0∣1∣2∣3∣4∣5∣6∣7∣8∣9​\n\n\n\n那么对于 1+2∗31 + 2 * 31+2∗3 来说，有下面两种派生树：\n\n先派生加再计算乘 (乘优先级大于加):\n\n\n\n\n\n先派生乘再派生加 (加优先级大于乘):\n\n\n\n\n因此说文法 GexpG_{exp}Gexp​ 是有歧义的。\n\n\n ii. 固有歧义\n有一些 CFL 语言 LLL 所有的 CFG 文法都是歧义的，那么称这个语言 LLL 是固有歧义的。\n例如 L={aibjck∣i=j or j=k}L=\\{a^{i}b^{j}c^{k}|i=j \\text{ or } j=k\\}L={aibjck∣i=j or j=k} 中任何形式为 anbncna^nb^nc^nanbncn 串总会有两棵语法树。\n iii. 消除歧义\n消除歧义需要重新设计文法，其中一个方法是设计优先级。策略如下：\n\n\n\n步骤\n策略\n\n\n\n\n增加变元\n不同优先级使用不同的变元\n\n\n派生顺序\n优先派生低优先级，在初步构造中 高优先级 视为一个变元\n\n\n括号派生\n括号内部的优先级大于括号本身即先派生括号再派生内部表达式但括号内部的优先级要从最低开始派生\n\n\n同优先级派生\n如果是从左到右计算，同一优先级下应将右边式子作为下一优先级因为先派生的是低优先级，所以先派生的是最右边的式子\n\n\n\n\n优先级设计的例子: 有优先级的算术表达式 CFG\n假设文法：\nGexp={N,T,P,S}G_{exp} = \\{N,T,P,S\\}\nGexp​={N,T,P,S}\n\n\n\n符号\n说明\n\n\n\n\nNNN\nN={E0,E1,E2,I,D}N = \\{E_0, E_1, E_2,I,D\\}N={E0​,E1​,E2​,I,D}EnE_nEn​ 表示不同符号下优先级的表达式nnn 越大，优先级越高\n\n\nTTT\nT={0,1,…,9,+,∗,(,)}T = \\{0,1,\\dots,9,+,*,(,)\\}T={0,1,…,9,+,∗,(,)}终结符是数字和符号\n\n\nSSS\nS=E0S = E_0S=E0​初始符号是最低优先级的表达式\n\n\nPPP\nE0→E0+E1∣E1E1→E1∗E2∣E2E2→(E0)∣II→DI∣DD→0∣1∣2∣3∣4∣5∣6∣7∣8∣9\\begin{aligned}E_0 &amp;\\rightarrow E_0 + E_1 \\mid E_1 \\\\E_1 &amp;\\rightarrow E_1 * E_2 \\mid E_2 \\\\E_2 &amp;\\rightarrow (E_0) \\mid I \\\\I &amp;\\rightarrow D I \\mid D \\\\D &amp;\\rightarrow 0 \\mid 1 \\mid 2 \\mid 3 \\mid 4 \\mid 5 \\mid 6\\mid 7 \\mid 8 \\mid 9\\end{aligned}E0​E1​E2​ID​→E0​+E1​∣E1​→E1​∗E2​∣E2​→(E0​)∣I→DI∣D→0∣1∣2∣3∣4∣5∣6∣7∣8∣9​\n\n\n\n对 PPP 的解释：\n\n\n\n产生式\n解释\n\n\n\n\nE0→E0+E1∣E1E_0 \\rightarrow E_0 + E_1 \\mid E_1E0​→E0​+E1​∣E1​\nE0E_0E0​ 是加法优先级变元是 E0+E1E_0 + E_1E0​+E1​ 而不是 E1+E0E_1 + E_0E1​+E0​ 是因为先派生的是最右边的表达式，让其可以先变成乘法\n\n\nE1→E1∗E2∣E2E_1 \\rightarrow E_1 * E_2 \\mid E_2E1​→E1​∗E2​∣E2​\nE1E_1E1​ 是乘法优先级变元是 E1∗E2E_1 * E_2E1​∗E2​ 而不是 E2∗E1E_2 * E_1E2​∗E1​ 是因为先派生的是最右边的表达式，让其可以先变成乘法\n\n\nE2→(E0)∣IE_2 \\rightarrow (E_0) \\mid IE2​→(E0​)∣I\nE2E_2E2​ 是括号优先级变元括号的优先级小于括号内表达式，因此先派生括号括号内表达式是一个全新的优先级等级，要从最低优先级的开始派生\n\n\nI→DI∣DI \\rightarrow D I \\mid DI→DI∣D\nIII 是数字派生变元，从单个数字变成完整的数值其从左到右派生数字，也可以从右到左\n\n\n\n那么 1+2∗31 + 2 * 31+2∗3 只存在唯一派生树：\n\n\n\n\n\n 4. CFG 的简化\n不同的 CFG 可以表达同一个 CFL。因此也存在化简 CFG 的方法来简化。\n化简步骤如下(不懂的术语请看导航中文法的性质定义)：\n\n\n\n步骤\n化简方法\n说明\n\n\n\n\n111\n消除 ε\\varepsilonε 产生式A→εA \\to \\varepsilonA→ε\n性质：消除 ε\\varepsilonε 产生式后的文法是 L−{ε}L - \\{\\varepsilon\\}L−{ε}，即不存在空串的文法。1. 判断：判断哪些变元是可空的2. 删除：删除所有 ε\\varepsilonε 产生式3. 替换：将含有可空变元的一条产生式 A→X1X2…XnA \\to X_1X_2\\dots X_nA→X1​X2​…Xn​ 用 一组 产生式 A→Y1Y2…YnA \\to Y_1Y_2\\dots Y_nA→Y1​Y2​…Yn​代替：a. 如果 XiX_iXi​ 不是可空的，那么 Yi=XiY_i = X_iYi​=Xi​b. 如果 XiX_iXi​ 是可空的，那么 YiY_iYi​ 为 XiX_iXi​ 或 ε\\varepsilonε，使用排列组合的方式分别替换为 XiX_iXi​ 或者 ε\\varepsilonεc. YiY_iYi​ 不能全部为 ε\\varepsilonεd. 假设一个产生式可空的数量为 nnn：如果全部都是可空的，那么产生式组的个数为 2n−12^n - 12n−1如果存在不可空的，那么产生式组的个数为 2n2^n2n\n\n\n222\n消除单元产生式A→BA \\to BA→B\n1. 判断：判断哪些变元为单元对2. 删除：删除全部形为 A→BA \\to BA→B 的单元产生式3. 复制：将单元对 [A,B][A, B][A,B] 中 BBB 的产生式复制给 AAA\n\n\n333\n消除无用符号\n如果符号 XXX 不是产生的或者不是可达的，那么 XXX 就是无用符号1. 消除非产生的无用符号2. 消除非可达的无用符号先删除非产生的是因为删除后会导致一些符号变成非可达的产生是左，可达是右；先删左再删右\n\n\n\n\n消除 ε\\varepsilonε 产生式例子\n考虑下面的文法产生式：\n{S→ABA→AaA∣εB→BbB∣ε\\begin{cases}\nS &amp; \\to AB \\\\\nA &amp; \\to AaA \\mid \\varepsilon \\\\\nB &amp; \\to BbB \\mid \\varepsilon\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​SAB​→AB→AaA∣ε→BbB∣ε​\n\n\n\n步骤\n操作\n\n\n\n\n判断\n1. 可知 A,BA, BA,B 是可空的2. 因为 S→ABS \\to ABS→AB 且 A,BA, BA,B 都是可空的，因此 SSS 是可空的\n\n\n删除\n删除所有 ε\\varepsilonε 产生式，变成：{S→ABA→AaAB→BbB\\begin{cases}S &amp; \\to AB \\\\A &amp; \\to AaA \\\\B &amp; \\to BbB\\end{cases}⎩⎪⎪⎨⎪⎪⎧​SAB​→AB→AaA→BbB​\n\n\n替换\n对可空的变元替换为空进行排列组合：{S→AB∣A∣BA→AaA∣aA∣Aa∣aB→BbB∣bB∣Bb∣b\\left\\{\\begin{aligned}S &amp;\\rightarrow AB \\mid A \\mid B \\\\A &amp;\\rightarrow AaA \\mid aA \\mid Aa \\mid a \\\\B &amp;\\rightarrow BbB \\mid  bB \\mid Bb \\mid b\\end{aligned}\\right.⎩⎪⎪⎨⎪⎪⎧​SAB​→AB∣A∣B→AaA∣aA∣Aa∣a→BbB∣bB∣Bb∣b​\n\n\n\nSSS 是全部可空的，因此产生式组的个数为 22−1=32^2 - 1 = 322−1=3。（因为要减去可空的全部为空的情况）\nA,BA, BA,B 是部分可空的，因此产生式组的个数为 22=42^2 = 422=4。（会包含可空的全部为空的情况）\n\n\n\n消除单元产生式例子\n考虑下面的文法产生式：\n{S→A∣B∣0S1A→0A∣0B→1B∣1\\begin{cases}\nS &amp; \\to A \\mid B \\mid 0S1 \\\\\nA &amp; \\to 0A \\mid 0 \\\\\nB &amp; \\to 1B \\mid 1\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​SAB​→A∣B∣0S1→0A∣0→1B∣1​\n\n\n\n步骤\n操作\n\n\n\n\n判断\n由 S→AS \\to AS→A 和 S→BS \\to BS→B 可知单元对 [S,A],[S,B][S, A], [S, B][S,A],[S,B]\n\n\n删除\n删除 S→A,S→BS \\to A, S \\to BS→A,S→B{S→0S1A→0A∣0B→1B∣1\\begin{cases}S &amp; \\to 0S1 \\\\A &amp; \\to 0A \\mid 0 \\\\B &amp; \\to 1B \\mid 1\\end{cases}⎩⎪⎪⎨⎪⎪⎧​SAB​→0S1→0A∣0→1B∣1​\n\n\n复制\n将单元对 [S,A],[S,B][S, A], [S, B][S,A],[S,B] 中 A,BA, BA,B 的产生式复制给 SSS{S→0S1∣0A∣1B∣0∣1A→0A∣0B→1B∣1\\begin{cases}S &amp; \\to 0S1 \\mid 0A \\mid 1B \\mid 0 \\mid 1 \\\\A &amp; \\to 0A \\mid 0 \\\\B &amp; \\to 1B \\mid 1\\end{cases}⎩⎪⎪⎨⎪⎪⎧​SAB​→0S1∣0A∣1B∣0∣1→0A∣0→1B∣1​\n\n\n\n\n\n\n消除无用符号例子\n考虑下面的文法：\n{S→AB∣aA→b\\begin{cases}\nS \\rightarrow AB \\mid a\\\\\nA \\rightarrow b\n\\end{cases}\n{S→AB∣aA→b​\n\n\n\n步骤\n操作\n\n\n\n\n消除非产生的\n1. 已知 a,ba, ba,b 是产生的2. 根据 A→bA \\rightarrow bA→b 可以知道 AAA 是产生的3. 根据 S→aS \\rightarrow aS→a 可以知道 SSS 是产生的4. 那么 BBB 是非产生的因此删除得到：{S→aA→b\\begin{cases}S \\rightarrow a\\\\A \\rightarrow b\\end{cases}{S→aA→b​\n\n\n消除非可达的\n1. SSS 是可达的2. AAA 是不可达的因此删除得到： {S→a\\begin{cases}S \\rightarrow a\\end{cases}{S→a​\n\n\n\n\n\n 5. CFG 文法范式\n咕咕咕\n包含乔姆斯基范式(CNF, Chomsky Normal Form) 和 格雷巴赫范式(GNF, Greibach Normal Form)。\n 6. 解析器\n咕咕咕。包括 LL(1) 解析器和消除左递归。\n 二. 下推自动机 PDA\n下推自动机(Pushdown Automata) PPP 是一个识别上下文无关语言 CFL 的不确定自动机。\n下推自动机相当于NFA增加了一个栈辅助，并默认栈的储存是无限的。\n下推自动机通常有七元组表示\nP=(Q,Σ,Γ,δ,P,Z0,F)\\boldsymbol{P=(Q,\\Sigma,\\Gamma,\\delta,P,Z_{0},F)}\nP=(Q,Σ,Γ,δ,P,Z0​,F)\n\n\n\n符号\n说明\n\n\n\n\nQQQ\n有穷状态集\n\n\nΣ\\SigmaΣ\n有穷输入符号集字母表\n\n\nΓ\\GammaΓ\n有穷栈符号集\n\n\nPPP\n初始状态集合P⊆QP \\subseteq QP⊆Q\n\n\nZ0Z_0Z0​\n初始栈顶符号Z0∈ΓZ_0 \\in \\GammaZ0​∈Γ\n\n\nFFF\n接受状态集合F⊆QF \\subseteq QF⊆Q\n\n\n\n状态转移函数 δ:Q×(Σ∪{ε})×Γ→P(Q×Γ∗)\\delta:Q\\times(\\Sigma\\cup\\{\\varepsilon\\})\\times\\Gamma\\to \\mathbb{P}(Q \\times\\Gamma^{*})δ:Q×(Σ∪{ε})×Γ→P(Q×Γ∗)，一般表示为 δ(q,a,Z)={(p0,β0),(p1,β1),⋯ }\\delta \\left( q,a,Z \\right)= \\left \\{ \\left( p_{0}, \\beta_{0} \\right), \\left( p_{1}, \\beta_{1} \\right), \\cdots \\right \\}δ(q,a,Z)={(p0​,β0​),(p1​,β1​),⋯}：\n\n\n\n符号\n说明\n\n\n\n\nqqq\n当前状态\n\n\naaa\n输入符号\n\n\nZZZ\n当前栈顶\n\n\nppp\n新的状态\n\n\nβ\\betaβ\n新的栈状态，类型如下：1. 入栈：bZbZbZ，b∈Γb \\in \\Gammab∈Γ 为新的栈顶2. 出栈：ε\\varepsilonε3. 无栈操作：ZZZ\n\n\n\n 1. PDA 状态转移图表示\n对于 δ(q,a,Z)={(p0,β0),(p1,β1),⋯ }\\delta \\left( q,a,Z \\right)= \\left \\{ \\left( p_{0}, \\beta_{0} \\right), \\left( p_{1}, \\beta_{1} \\right), \\cdots \\right \\}δ(q,a,Z)={(p0​,β0​),(p1​,β1​),⋯}：\n\n\n\n组成成分\n说明\n\n\n\n\n点\n状态 q0∈Qq_0 \\in Qq0​∈Q\n\n\n边\n由 qqq 节点连向 ppp 节点的有向边\n\n\n边值\n(a,Z,β)(a, Z ,\\beta)(a,Z,β)分别是输入符号、当前栈顶、新的栈状态\n\n\n\n 2. PDA的瞬时描述 (ID)\n\n\n\n\n\n\n\n\n\nPDA的瞬时描述 (Instantaneous Description, ID) 是来描述当前PDA处于的状态、栈和输入情况。\n一般用三元组 Q×Σ∗×Γ∗:(q,w,γ)Q \\times \\Sigma^* \\times \\Gamma^* : (q, w, \\gamma)Q×Σ∗×Γ∗:(q,w,γ) 来表示。\n\n\n\n符号\n说明\n\n\n\n\nqqq\n当前的状态\n\n\nwww\n剩余输入串下一个要接收的字符为 www 的第一个字符\n\n\nγ\\gammaγ\n栈状态一般第一个字符是栈顶，最后一个字符是初始栈\n\n\n\n瞬时描述转移到下一个瞬时描述的符号是 ⊢p\\vdash_p⊢p​，多次转移符号是 ⊢p∗\\vdash_p^*⊢p∗​。\n 3. 空栈和终态\n咕咕咕\n 4. PDA 与 CFG 互相转化\nPDA 和 CFG 都能用来描述 CFL，能够互相转化。\n i. PDA 转化 CFG\n咕咕咕\n ii. CFG 转化 PDA\n咕咕咕\n 咕咕咕\n 三. 上下文无关语言的泵原理\n咕咕咕\n\n&lt;返回形式语言与自动机理论导航\n","slug":"笔记/形式语言与自动机理论/上下文无关语言","date":"2024-05-25T10:49:38.000Z","categories_index":"笔记-形式语言与自动机理论","tags_index":"Automata Theory,Formal Languages,Context-Free,CFL","author_index":"zExNocs"},{"id":"4508d8f1371ef2184c3260957e5de1aa","title":"FLAT-正则语言","content":"&lt;返回形式语言与自动机理论导航\n\n 零. 阅读须知\n\n如果遇到任何不懂的术语，请查阅 形式语言与自动机理论导航。\n语言(Language)是在特定字母表下一种字符串的集合，这种集合包含了该字母表中符合一定的规律下所有的字符串。\n可以由正则文法视别的语言是正则语言。\n一种文法只视别一种语言；一种语言可以有多种文法。\n\n例如语言 L={an ∣ n&gt;0}L = \\{a^n\\ |\\ n &gt; 0\\}L={an ∣ n&gt;0} 表示了字符串 {a,aa,aaa,… }\\{a, aa, aaa, \\dots\\}{a,aa,aaa,…} 的集合，可以用正则文法视别，因此属于正则语言。\n 一. 正则文法(Regular Grammar)\n正则文法由一个四元组 G=(N,Σ,P,s)G = (N, \\Sigma, P, s)G=(N,Σ,P,s) 来表示:\n\n\n\n符号\n意义\n\n\n\n\nNNN\n非终结符的有限集合\n\n\nΣ\\SigmaΣ\n终结符的有限集合N∩Σ=∅N \\cap \\Sigma = \\emptyN∩Σ=∅\n\n\nPPP\n产生式的有限集合\n\n\nsss\n起始的非终结符s∈Ns \\in Ns∈N\n\n\n\n正则法则对要求产生式 PPP 左部只有一个非终结符，右部为一个终结符接最多一个非终结符，或空串。\n只有以下两种形式:\n\n\n\n形式\n说明\n表示\n\n\n\n\n左线性文法Left-Linear Grammar\n从右向左搭建字符串过程是最左派生\n1. A→aA \\to aA→a2. A→BaA \\to BaA→Ba3. A→εA \\to \\varepsilonA→ε\n\n\n右线性文法Right-Linear Grammar\n从左向右搭建字符串过程是最右派生\n1. A→aA \\to aA→a2. A→aBA \\to aBA→aB3. A→εA \\to \\varepsilonA→ε\n\n\n\n 二. 正则表达式(Reular Expression)\n\n\n\n\n\n\n\n\n\n正则表达式是用来识别正则语言的一种文法。\n正则表达式有很多种，在形式语言中，只考虑下面标准正则表达式：\n假设 r,sr, sr,s 分别是语言 R,SR, SR,S 的正则表达式：\n\n\n\n正则表达式\n表示的语言\n意义\n\n\n\n\n∅\\varnothing∅\n空语言\n空集是正则语言\n\n\nε\\varepsilonε\n空字符串语言 {ε}\\{\\varepsilon\\}{ε}\n空字符串集合是正则语言\n\n\na,a∈Σa, a \\in \\Sigmaa,a∈Σ\n长度为 111 的字符串的语言{a}\\{a\\}{a}\n单长度字符串集合是正则语言\n\n\nr+sr + sr+s\nR∪SR \\cup SR∪S\n如果 R,SR, SR,S 是正则语言，那么 R∪SR \\cup SR∪S 是正则语言\n\n\nrsrsrs\nR⋅SR \\cdot SR⋅S\n如果 R,SR, SR,S 是正则语言，那么 R⋅SR \\cdot SR⋅S 是正则语言\n\n\nr∗r^*r∗\nR∗R^*R∗\n如果 RRR 是正则语言，那么 R∗R^*R∗ 是正则语言\n\n\n(r)(r)(r)\nRRR\n\n\n\n\n 1. 正则表达式的优先级\n\n\n\n优先级\n类型\n符号\n\n\n\n\n1\n括号\n(((r)$\n\n\n2\n闭包\nr∗r^*r∗\n\n\n3\n串联/乘\nrsrsrs\n\n\n4\n并联/加\nr+sr + sr+s\n\n\n\n 2. 正则表达式的代数定理\n可以转化为语言(集合)来验证\n\n\n\n运算类型\n定理\n代号表示\n\n\n\n\n元运算\n单位元 ε\\varepsilonε\nεr=rε=r\\varepsilon r = r \\varepsilon = rεr=rε=r\n\n\n元运算\n零元 ∅\\varnothing∅\n∅r=r∅=∅\\varnothing r = r \\varnothing = \\varnothing∅r=r∅=∅\n\n\n元运算\n单位元 ∅\\varnothing∅\n∅+r=r+∅=r\\varnothing + r = r + \\varnothing = r∅+r=r+∅=r\n\n\n并运算\n交换律\nr+s=s+rr + s = s + rr+s=s+r\n\n\n并运算\n结合律\nr+(s+t)=(r+s)+tr + (s + t) = (r + s) + tr+(s+t)=(r+s)+t\n\n\n并运算\n幂等律\nr+r=rr + r = rr+r=r\n\n\n串运算\n交换律不符合\nrs≠srrs \\not = srrs=sr\n\n\n串运算\n幂交换律\nrr∗=r∗r=r+rr^* = r^*r = r^+rr∗=r∗r=r+r++ε=r∗r^+ + \\varepsilon = r^*r++ε=r∗\n\n\n串运算\n幂等律\nr∗r∗=r∗r^*r^*=r^*r∗r∗=r∗\n\n\n串运算\n结合律\nr(st)=(rs)tr(st) = (rs)tr(st)=(rs)t\n\n\n串运算\n左分配律\nr(s+t)=rs+rtr(s + t) = rs + rtr(s+t)=rs+rt\n\n\n串运算\n右分配律\n(s+t)r=sr+st(s + t)r = sr + st(s+t)r=sr+st\n\n\n闭包\n\n(r∗)∗=r∗(r^*)^* = r^*(r∗)∗=r∗\n\n\n闭包\n空集幂\n∅∗=ε\\varnothing ^ * = \\varepsilon∅∗=ε\n\n\n闭包\n空字符幂\nε∗=ε\\varepsilon ^ * = \\varepsilonε∗=ε\n\n\n闭包\n\nr++ε=r∗r^+ + \\varepsilon = r^*r++ε=r∗rr∗+ε=r∗rr^* + \\varepsilon = r^*rr∗+ε=r∗\n\n\n闭包\n\nr∗(r+ε)=r∗r^*(r + \\varepsilon) = r^*r∗(r+ε)=r∗\n\n\n闭包\n\n(r+s)∗=(r∗s∗)∗(r + s)^* = (r^*s^*)^*(r+s)∗=(r∗s∗)∗(ε+r)∗=r∗(\\varepsilon + r)^* = r^*(ε+r)∗=r∗\n\n\n\n 3. 正则表示的代数推论\n\n\n\n推论符号\n说明\n\n\n\n\n(r+s)∗=s∗+(r+s)∗rs∗(r + s)^* = s^* + (r + s)^*rs^*(r+s)∗=s∗+(r+s)∗rs∗\n1. s∗s^*s∗ 表示不存在 rrr2. (r+s)∗rs∗(r + s)^*rs^*(r+s)∗rs∗ 表示 至少存在 一个 rrr\n\n\n(r+s)∗=(r+s)∗r∗=r∗(r+s)∗(r + s)^* = (r + s)^*r^* = r^*(r + s)^*(r+s)∗=(r+s)∗r∗=r∗(r+s)∗\n证明 (r+s)∗=(r+s)∗r∗(r + s)^* = (r + s)^*r^*(r+s)∗=(r+s)∗r∗：1. 后者肯定包含前者2. 前者通过下面步骤包含后者：a. L((r+s)∗r∗)⊆L((r+s)∗(r+s)∗)L((r + s) ^*r^*) \\subseteq L((r + s) ^*(r + s)^*)L((r+s)∗r∗)⊆L((r+s)∗(r+s)∗)b. L((r+s)∗(r+s)∗)=L((r+s)∗)L((r + s) ^*(r + s)^*) = L((r + s)^*)L((r+s)∗(r+s)∗)=L((r+s)∗)\n\n\n(r+s)∗r(r+s)∗=(r+s)∗rs∗=s∗r(r+s)∗(r + s)^* r (r + s)^* = (r + s)^*rs^* = s^*r(r + s)^*(r+s)∗r(r+s)∗=(r+s)∗rs∗=s∗r(r+s)∗\n都表示至少存在一个 rrr1. 前者表示任何位置的 rrr2. 中间表示最后一个 rrr3. 后者表示第一个 rrr\n\n\n\n\n证明 (r+s)∗r(r+s)∗=(r+s)∗rs∗(r + s)^* r (r + s)^* = (r + s)^* r s^*(r+s)∗r(r+s)∗=(r+s)∗rs∗\n首先证明 L((r+s)∗rs∗)⊆L((r+s)∗r(r+s)∗)L((r + s)^* r s^*) \\subseteq L((r + s)^* r (r + s)^*)L((r+s)∗rs∗)⊆L((r+s)∗r(r+s)∗)，很明显：\n\nL((r+s)∗)⊆L((r+s))∗L((r + s)^*) \\subseteq L((r + s))^*L((r+s)∗)⊆L((r+s))∗\nL(r)⊆L(r)L(r) \\subseteq L(r)L(r)⊆L(r)\nL(s∗)⊆L((r+s)∗)L(s^*) \\subseteq L((r + s)^*)L(s∗)⊆L((r+s)∗)\n\n因此有：\nL((r+s)∗rs∗)⊆L((r+s)∗r(r+s)∗)L((r + s)^* r s^*) \\subseteq L((r + s)^* r (r + s)^*)\nL((r+s)∗rs∗)⊆L((r+s)∗r(r+s)∗)\n其次证明 L((r+s)∗r(r+s)∗)⊆L((r+s)∗rs∗)L((r + s)^* r (r + s)^*) \\subseteq L((r + s)^* r s^*)L((r+s)∗r(r+s)∗)⊆L((r+s)∗rs∗)：\n\nL((r+s)∗rs∗)=L((r+s)∗(r+s)∗rs∗)L((r + s)^* r s^*) = L((r + s)^* (r + s)^* r s^*)L((r+s)∗rs∗)=L((r+s)∗(r+s)∗rs∗)\nL((r+s)∗)⊆L((r+s)∗)L((r + s)^*) \\subseteq L((r + s)^*)L((r+s)∗)⊆L((r+s)∗)\n\n此时只需要证明 L(r(r+s)∗)⊆L((r+s)∗rs∗)L(r (r + s)^*) \\subseteq L((r + s)^* r s^*)L(r(r+s)∗)⊆L((r+s)∗rs∗)。\n我们将 L(r(r+s)∗)L(r (r + s)^*)L(r(r+s)∗) 中的 (r+s)∗(r + s)^*(r+s)∗ 进行分类讨论：\n\n如果 (r+s)∗(r + s)^*(r+s)∗ 中没有 rrr，即 (r+s)∗=s∗(r + s)^* = s^*(r+s)∗=s∗。\n如果 (r+s)∗(r + s)^*(r+s)∗ 中至少有一个 rrr，那么必然存在一个其右边只有 sss 或者为 ε\\varepsilonε，即 (r+s)∗=(r+s)∗rs∗(r + s)^* = (r + s)^* r s^*(r+s)∗=(r+s)∗rs∗。\n\n那么可以得到：\n(r+s)∗=s∗+(r+s)∗rs∗(r + s)^* = s^* + (r + s)^* r s^*\n(r+s)∗=s∗+(r+s)∗rs∗\n那么：\nL(r(r+s)∗)=L(rs∗)∪L(r(r+s)∗rs∗)L(r (r + s)^*) = L(r s^*) \\cup L(r (r + s)^* r s^*)\nL(r(r+s)∗)=L(rs∗)∪L(r(r+s)∗rs∗)\n\nL(rs∗)⊆L((r+s)∗rs∗)L(r s^*) \\subseteq L((r + s)^* r s^*)L(rs∗)⊆L((r+s)∗rs∗)，此时 (r+s)∗=ε(r + s)^* = \\varepsilon(r+s)∗=ε。\nL(r(r+s)∗rs∗)⊆L((r+s)∗rs∗)L(r (r + s)^* r s^*) \\subseteq L((r + s)^* r s^*)L(r(r+s)∗rs∗)⊆L((r+s)∗rs∗)，此时 (r+s)∗=r(r+s)∗(r + s)^* = r (r + s)^*(r+s)∗=r(r+s)∗。\n\n因此：\nL(r(r+s)∗)⊆L((r+s)∗rs∗)L(r (r + s)^*) \\subseteq L((r + s)^* r s^*)\nL(r(r+s)∗)⊆L((r+s)∗rs∗)\n综上所述，\n(r+s)∗rs∗=(r+s)∗r(r+s)∗(r + s)^* r s^* = (r + s)^* r (r + s)^*\n(r+s)∗rs∗=(r+s)∗r(r+s)∗\n\n\n 4. 一些正则表达式的模板\n为了方便表示，我们使用 [^a] 表示除了 aaa 以外的其他字符。\n\n\n\n表示\n模板\n\n\n\n\n至少有一个 aaa\n(*)a[^a]*\n\n\n偶数个 aaa\n([^a]*a[^a]*a)*[^a]*\n\n\n奇数个 aaa\n[^a]*a([^a]*a[^a]*a)*[^a]*[^a]*a([^a]*+a[^a]*a)*\n\n\n\n 5. 正则表达式与语言之间的转化\n假设 rrr 为正则表达式，那么 L(r)L(r)L(r) 为该正则表达式可识别的语言。\n\n正常转化\n\n\n\n\n类型\n转化\n\n\n\n\n空集\nL(∅)=∅L(\\varnothing) = \\varnothingL(∅)=∅\n\n\n空字符串\nL(ε)={ε}L(\\varepsilon) =\\{\\varepsilon\\}L(ε)={ε}\n\n\n单字符串\nL(a)={a},a∈ΣL(a) = \\{a\\}, a \\in \\SigmaL(a)={a},a∈Σ\n\n\n并运算\nL(r+s)=L(r)∪L(s)L(r + s) = L(r) \\cup L(s)L(r+s)=L(r)∪L(s)\n\n\n串运算\nL(rs)=L(r)⋅L(s)L(rs) = L(r) \\cdot L(s)L(rs)=L(r)⋅L(s)\n\n\n幂运算\nL(r∗)=L(r)∗L(r^*) = L(r)^*L(r∗)=L(r)∗\n\n\n括号\nL((r))=L(r)L((r)) = L(r)L((r))=L(r)\n\n\n\n\n包含关系\n\n\n\n\n类型\n转化\n\n\n\n\n并运算\nL(r)⊆L(r+s)L(r) \\subseteq L(r + s)L(r)⊆L(r+s)\n\n\n并运算\n(L(r0)⊆L(s))∧(L(r1)⊆L(s))  ⟺  (L(r0)∪L(r1))⊆L(s)  ⟺  L(r0+r1)⊆L(s)\\begin{aligned}&amp;(L(r_0) \\subseteq L(s)) \\land (L(r_1) \\subseteq L(s)) \\\\ \\iff&amp; (L(r_0) \\cup L(r_1)) \\subseteq L(s)\\\\\\iff&amp; L(r_0 + r_1) \\subseteq L(s)\\end{aligned}⟺⟺​(L(r0​)⊆L(s))∧(L(r1​)⊆L(s))(L(r0​)∪L(r1​))⊆L(s)L(r0​+r1​)⊆L(s)​\n\n\n串运算\nL(r0)⊆L(r1)∧L(s0)⊆L(s1)→L(r0)L(s0)⊆L(r1)L(s1)L(r_0) \\subseteq L(r_1) \\land L(s_0) \\subseteq L(s_1) \\rightarrow L(r_0)L(s_0) \\subseteq L(r_1)L(s_1)L(r0​)⊆L(r1​)∧L(s0​)⊆L(s1​)→L(r0​)L(s0​)⊆L(r1​)L(s1​)L(r0)⊆L(r1)∧L(s0)⊆L(s1)→L(r0s0)⊆L(r1s1)L(r_0) \\subseteq L(r_1) \\land L(s_0) \\subseteq L(s_1) \\rightarrow L(r_0s_0) \\subseteq L(r_1s_1)L(r0​)⊆L(r1​)∧L(s0​)⊆L(s1​)→L(r0​s0​)⊆L(r1​s1​)\n\n\n闭包\nL(ε)⊆L(r∗)L(\\varepsilon) \\subseteq L(r^*)L(ε)⊆L(r∗)\n\n\n闭包\nL(r)⊆L(r∗)L(r) \\subseteq L(r^*)L(r)⊆L(r∗)\n\n\n闭包\nL(rr…rr∗)⊆L(r∗)L(rr\\ldots rr^*) \\subseteq L(r^*)L(rr…rr∗)⊆L(r∗)\n\n\n闭包\nL(r∗)⊆L((r+f)∗)L(r^*) \\subseteq L((r + f)^*)L(r∗)⊆L((r+f)∗)\n\n\n闭包\nL(r(r+s)∗)⊆L((r+s)∗)L(r(r + s)^*) \\subseteq L((r + s)^*)L(r(r+s)∗)⊆L((r+s)∗)∙\\bullet∙ L(r(r+s)∗)⊆L((r+s)(r+s)∗)⊆L((r+s)∗)L(r(r + s)^*) \\subseteq L((r + s)(r + s)^*) \\subseteq L((r + s)^*)L(r(r+s)∗)⊆L((r+s)(r+s)∗)⊆L((r+s)∗)\n\n\n闭包\nL((r+s)∗r)⊆L((r+s)∗)L((r + s)^*r) \\subseteq L((r + s)^*)L((r+s)∗r)⊆L((r+s)∗)\n\n\n\n 6. 如何证明两个正则表达式相等\n\n\n\n方法\n描述\n\n\n\n\n转化集合法\nr=s  ⟺  L(r)=L(s)r = s \\iff L(r) = L(s)r=s⟺L(r)=L(s)\n\n\n集合相等定理\nL(r)⊆L(s)L(r) \\subseteq L(s)L(r)⊆L(s) 并且 L(s)⊆L(r)L(s) \\subseteq L(r)L(s)⊆L(r)，那么有 L(r)=L(s)L(r) = L(s)L(r)=L(s)\n\n\n正则表达式定理\n假设正则表达式 E,FE, FE,F 中分别有变量 L0,L1,...L_0, L_1, ...L0​,L1​,...，如果将这些变量替换为具体的一个字符 a,b,ca, b, ca,b,c 能使得 L(E)=L(F)L(E) = L(F)L(E)=L(F)，那么恒有 L(E)=L(F)L(E) = L(F)L(E)=L(F)1. 例如 由于 L(aa∗)=L(a∗a)L(aa^*) = L(a^*a)L(aa∗)=L(a∗a), 那么有 L(L0L0∗)=L(L0∗L0)L(L_0L_0^*) = L(L_0^*L_0)L(L0​L0∗​)=L(L0∗​L0​)。其中 L0L_0L0​ 可以替换为任意其他的正则表达式，例如 L((ab+c)(ab+c)∗)=L((ab+c)∗(ab+c))L((ab+c)(ab+c)^*) = L((ab+c)^*(ab+c))L((ab+c)(ab+c)∗)=L((ab+c)∗(ab+c))2. 此定理不适合集合\n\n\n对称差做差法集合互斥和自动机\nE⊕F=(E∩F‾)∪(E‾∩F)E \\oplus F = (E \\cap \\overline{F}) \\cup (\\overline{E} \\cap F)E⊕F=(E∩F)∪(E∩F) 表示 E,FE, FE,F 的对称差(symmetric difference)，该集合表示所有属于 EEE 但不属于 FFF，或属于 FFF 但不属于 EEE 的字符串(类似于异或)如果 EEE 和 FFF 是正则语言，那么 E⊕FE \\oplus FE⊕F 是正则的，则必然存在 自动机(构造积自动机) 识别该语言：1. E≠FE \\ne FE=F: 非空集，自动机能够识别任意 E⊕FE \\oplus FE⊕F 中的串2. E=FE = FE=F: 空集，自动机不能识别E⊕FE \\oplus FE⊕F 任何一个串\n\n\n\n 三. 有限状态自动机 (FSA)\n有限状态自动机(Finite State Automaton, FSA)用于表示和控制具有有限个状态的系统。有限状态自动机可以用于识别正则语言。\n它分成两种，确定性有限状态自动机(DFA)和非确定性有限状态自动机(NFA)。\n 1. 确定性有限状态自动机 (DFA)\n确定性有限状态自动机(Deterministic Finite Automaton, DFA)是一种识别正则语言的确定性自动机。通常表示为五元组：\nD=(Q,Σ,δ,q0,F)D = (Q, \\Sigma, \\delta, q_0, F)\nD=(Q,Σ,δ,q0​,F)\n\n\n\n符合\n意义\n\n\n\n\nQQQ\n有穷状态集合\n\n\nΣ\\SigmaΣ\n可被识别的字母表\n\n\nδ\\deltaδ\n状态转移函数δ:Q×Σ→Q\\delta: Q \\times \\Sigma \\to Qδ:Q×Σ→Q\n\n\nq0q_0q0​\n初始状态q0∈Qq_0 \\in Qq0​∈Q\n\n\nFFF\n最终状态集合F⊆QF \\subseteq QF⊆Q\n\n\n\n用 L(D)L(D)L(D) 表示状态机 DDD 可以接受的语言。\n\n\n\n\n\n\n\n\n\n如果 ∃D:DFA,L=L(D)\\exist D: \\text{DFA}, L = L(D)∃D:DFA,L=L(D)，那么语言 LLL 是正则语言。\n i. 状态转移函数 δ\\deltaδ 表示法\n通常由函数表示法、状态转移图 或 状态转移表来表示。\n以下例子都是识别具有奇数长度二进制的DFA: D=({q0,q1},{0,1},δ,q0,{q1})D = (\\{q_0, q_1\\}, \\{0, 1\\}, \\delta, q_0, \\{q_1\\})D=({q0​,q1​},{0,1},δ,q0​,{q1​})\n a. 函数表示法\n直接由 δ(q1,a)=q2\\delta(q_1, a) = q_2δ(q1​,a)=q2​ 表示。\n\n例子：\n\n{δ(q0,0)=q1δ(q0,1)=q1δ(q1,0)=q0δ(q1,1)=q0\\begin{cases}\n\\delta(q_0, 0) = q_1 \\\\\n\\delta(q_0, 1) = q_1 \\\\\n\\delta(q_1, 0) = q_0 \\\\\n\\delta(q_1, 1) = q_0\n\\end{cases}\n⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​δ(q0​,0)=q1​δ(q0​,1)=q1​δ(q1​,0)=q0​δ(q1​,1)=q0​​\n b. 状态转移图\n\n绘制标准：\n\n每个状态 qqq 对应一个节点，用圆圈表示。\n状态转移函数 δ(q1,a)=q2\\delta(q_1, a) = q_2δ(q1​,a)=q2​ 用一条从状态节点 q1q_1q1​ 到 q2q_2q2​ 值为 aaa 的有向边表示。\n开始状态的节点被一个箭头指向。\n结束状态的节点用双圆圈表示。\n\n\n例子：\n\n\n\n\n c. 状态转移表\n\n绘制标准：\n\n每个状态 qqq 对应一行，每个字母 aaa 对应一列。\n状态转移函数 δ(q1,a)=q2\\delta(q_1, a) = q_2δ(q1​,a)=q2​ 用第 q1q_1q1​ 行 第 aaa 列填入 q2q_2q2​ 表示。\n开始状态前用箭头 →\\rightarrow→ 表示。\n接受状态前用星号 ⋆\\star⋆ 表示。\n\n\n例子：\n\n\n\n\nδ\\deltaδ\n0\n1\n\n\n\n\n→q0\\rightarrow q_0→q0​\nq1q_1q1​\nq1q_1q1​\n\n\n⋆q1\\star q_1⋆q1​\nq0q_0q0​\nq0q_0q0​\n\n\n\n ii. 扩展状态转移函数\n扩展状态转移函数被用来表示一个状态在接收一个字符串后到达的状态。\nDFA扩展状态转移函数递归定义为：\nδ^:Q×Σ∗→Q\\hat{\\delta} : Q \\times \\Sigma^* \\to Q\nδ^:Q×Σ∗→Q\nδ^(q,x)={qx=εδ^(δ(q,a),y)x=ay\\hat{\\delta}(q, x) =\n\\begin{cases}\nq &amp; x = \\varepsilon \\\\\n\\hat{\\delta}(\\delta(q, a), y) &amp; x = ay\n\\end{cases}\nδ^(q,x)={qδ^(δ(q,a),y)​x=εx=ay​\n那么可以定义DFA所接受的全部语言 L(D)L(D)L(D)：\nL(D)={w∣w∈Σ∗∧δ^(q0,w)∈F}L(D) = \\{ w \\mid w \\in \\Sigma^* \\land \\hat{\\delta}(q_0, w) \\in F \\}\nL(D)={w∣w∈Σ∗∧δ^(q0​,w)∈F}\n iii. 最小化 DFA\n对于同一个语言，可以有多种DFA识别。例如识别具有奇数长度二进制的DFA可以有下面两种形式。\n\n1.1.1. 直接讨论总长度的奇偶 D1D_1D1​：\n\n\n\n\n\n2.2.2. 分别讨论 000 和 111 的奇偶 D2D_2D2​：\n\n\n\n\n这两个DFA是等价的，即 L(D1)=L(D2)L(D_1) = L(D_2)L(D1​)=L(D2​)。但是很明显 D1D_1D1​ 比 D2D_2D2​ 更加简洁。我们可以使\n用 填表法 将 D2D_2D2​ 化简为 D1D_1D1​。\n假设一共有 (n+1)(n + 1)(n+1) 个状态，其状态的编号为 [0,n][0, n][0,n]，填表法步骤：\n\n\n\n编号\n步骤\n\n\n\n\n111\n画出 n×nn \\times nn×n 的表格，横向的排序为 [0,n−1][0, n-1][0,n−1] 从低到高，纵向的排序为 [n,1][n, 1][n,1] 从高到低。每个坐标下的两个状态视为一个状态组1. 一共有 n2n^2n2 个状态组2. 可以将重复的状态组填入 “–”，例如在状态组 (x,y)(x, y)(x,y) 中，可以将所有 x≥yx \\ge yx≥y 的状态组填入 “–”\n\n\n222\n将表格中所有 接受状态 与 非接受状态 状态组的空格中填入 ×\\times×，并视为“可区分的(Distinguishable)状态组”\n\n\n333\n将其他空白的状态组一一列出\n\n\n444\n将这些状态组分别与所有终结符(字符)带入到状态转移函数 δ\\deltaδ，获得新的状态组。将该状态组与新的状态组归类为同一类状态组\n\n\n555\n只要存在在一个状态组是可区分的状态组，那么该类状态组就都是可区分的状态组，填入 ×\\times×\n\n\n666\n将所有的空白状态组列出来检查并分类后，所有没有被确定为 “可区分的状态组”（没有被填入 ×\\times× 的状态组）都视为 “不可区分的(Indistinguishable)状态组”，即这个状态组中两个状态是等价的，可以相互转化\n\n\n777\n将所有不可区分的状态组两个或多个状态化为一个状态1. 注意是一个状态组中两个状态化为一个状态，不是同类状态组之间。同类状态组只是为了方便划分状态组是否可区分状态2. 例如 如果状态组 (q0,q1)(q_0, q_1)(q0​,q1​) 是不可区分的状态组，那么可以将 q0q_0q0​ 和 q1q_1q1​ 化为同一个状态。此外，如果还有 (q1,q2)(q_1, q_2)(q1​,q2​) 是不可区分的（此时一定也有 (q0,q2)(q_0, q_2)(q0​,q2​) 是不可区分的状态组），那么就可以将 q0,q1,q2q_0, q_1, q_2q0​,q1​,q2​ 化为同一个状态，即 q0≡q1≡q2q_0 \\equiv q_1 \\equiv q_2q0​≡q1​≡q2​3. 如果状态组 (q1,q2)(q_1, q_2)(q1​,q2​) 和 (q2,q3)(q_2, q_3)(q2​,q3​) 是同类不可区分的状态组，并不能将这四个状态视为一个状态，而是分别将 q0,q1q_0, q_1q0​,q1​ 和 q2,q3q_2, q_3q2​,q3​ 视为同一个状态，即 q0≡q1q_0 \\equiv q_1q0​≡q1​ 且 q2≡q3q_2 \\equiv q_3q2​≡q3​\n\n\n\n\n填表法简化 DFA 的例子: 化简 D2D_2D2​ 为 D1D_1D1​\nD2D_2D2​ 中，我们知道 q1q_1q1​ 和 q2q_2q2​ 是接受状态，q0q_0q0​ 和 q3q_3q3​ 是不可接受状态，那么我们根据填表法步骤 1, 2 可以画出下面的图：\n\n\n\n\n0\n1\n2\n\n\n\n\n3\n\n×\n×\n\n\n2\n×\n\n–\n\n\n1\n×\n–\n–\n\n\n\n此时只剩下状态组 (q0,q3)(q_0, q_3)(q0​,q3​) 和 (q1,q2)(q_1, q_2)(q1​,q2​) 是未知的。\n我们先转化 (q0,q3)(q_0, q_3)(q0​,q3​)：\n{(δ(q0,0),δ(q3,0))=(q1,q2)未知是否可区分(δ(q0,1),δ(q3,1))=(q2,q1)未知是否可区分\\begin{cases}\n(\\delta(q_0, 0), \\delta(q_3, 0)) = (q_1, q_2) &amp; \\text{未知是否可区分} \\\\\n(\\delta(q_0, 1), \\delta(q_3, 1)) = (q_2, q_1) &amp; \\text{未知是否可区分}\n\\end{cases}\n{(δ(q0​,0),δ(q3​,0))=(q1​,q2​)(δ(q0​,1),δ(q3​,1))=(q2​,q1​)​未知是否可区分未知是否可区分​\n那么此时我们将 (q0,q3)(q_0, q_3)(q0​,q3​) 和 (q1,q2)(q_1, q_2)(q1​,q2​) 视为同一类状态组，即：\n\n如果 (q1,q2)(q_1, q_2)(q1​,q2​) 是可区分的，那么 (q0,q3)(q_0, q_3)(q0​,q3​) 也是可区分的。\n如果到最后都不能确定 (q1,q2)(q_1, q_2)(q1​,q2​) 是否可区分，那么 (q1,q2)(q_1, q_2)(q1​,q2​) 和 (q0,q3)(q_0, q_3)(q0​,q3​) 都是不可区分的。\n\n我们再转化 (q1,q2)(q_1, q_2)(q1​,q2​)：\n{δ(q1,0),δ(q2,0))=(q0,q3)未知是否可区分δ(q1,1),δ(q2,1))=(q3,q1)未知是否可区分\\begin{cases}\n\\delta(q_1, 0), \\delta(q_2, 0)) = (q_0, q_3) &amp; \\text{未知是否可区分} \\\\\n\\delta(q_1, 1), \\delta(q_2, 1)) = (q_3, q_1) &amp; \\text{未知是否可区分}\n\\end{cases}\n{δ(q1​,0),δ(q2​,0))=(q0​,q3​)δ(q1​,1),δ(q2​,1))=(q3​,q1​)​未知是否可区分未知是否可区分​\n此时所有的状态组都检查完毕，那么剩下没有确定为可区分的状态组 (q0,q3)(q_0, q_3)(q0​,q3​) 和 (q1,q2)(q_1, q_2)(q1​,q2​) 都被视为不可区分的状态组。\n也就是说 q0q_0q0​ 和 q3q_3q3​ 是等价的，q1q_1q1​ 和 q2q_2q2​ 是等价的，即 q0≡q3,q1≡q2q_0 \\equiv q_3, q_1 \\equiv q_2q0​≡q3​,q1​≡q2​。\n此时将 q0q_0q0​ 和 q3q_3q3​ 合并为 q0q_0q0​，将 q1q_1q1​ 和 q2q_2q2​ 合并为 q1q_1q1​，可以画出最简DFA D1D_1D1​。\n\n\n iv. DFA转化正则表达式\n若语言 L=L(D)L = L(D)L=L(D) 是某 DFA 可识别的语言，那么存在正则表达式 RRR 满足 L=L(R)L = L(R)L=L(R)。\n a. 递归式法\n\n递归法的基本思想：\n\n假设 RijR_{ij}Rij​ 是对于任意两个状态 qi,qjq_i, q_jqi​,qj​ 之间转化的正则表达式。\n如果 qiq_iqi​ 和 qjq_jqj​ 不是直接相连的，那么会存在中间状态 qkq_kqk​ 有：\nRij经过k=Rij不经过k+Rik不经过k(Rkk不经过k)∗Rkj不经过kR_{ij}^{\\text{经过k}} = R_{ij}^{\\text{不经过k}} + R_{ik}^{\\text{不经过k}} (R_{kk}^{\\text{不经过k}})^* R_{kj}^{\\text{不经过k}}\nRij经过k​=Rij不经过k​+Rik不经过k​(Rkk不经过k​)∗Rkj不经过k​\n同时对于 RikR_{ik}Rik​ 和中间状态 qk−1q_{k-1}qk−1​ 也同样包含上次公式，直到递归至不存在中间状态，即两个状态之间直接相连为止，便能通过递归得到最终的正则表达式 RijR_{ij}Rij​。\n\n\n递归公式法：\n\n假设 Rij(k)R_{ij}^{(k)}Rij(k)​ 表示从状态 qiq_iqi​ 到状态 qjq_jqj​ 但中间状态下角标不超过 kkk 的全部路径的正则表达式(也就是说只用 [1,k][1, k][1,k] 区间的点作为中间节点)：\n\nRij(k)={a∣δ^(i,a)=j,a 经过的状态除两端外都不超过 k}R_{ij}^{(k)} = \\{ a \\mid \\hat{\\delta}(i, a) = j, \\text{a 经过的状态除两端外都不超过 k} \\}Rij(k)​={a∣δ^(i,a)=j,a 经过的状态除两端外都不超过 k}\nRij(0)R_{ij}^{(0)}Rij(0)​ 表示不过任何中间状态直接由 qiq_iqi​ 到 qjq_jqj​ 的正则表达式。\n\n那么对于DFA：D=({q1,q2,...,qn},Σ,δ,q1,F)D = (\\{q_1, q_2, ..., q_n\\}, \\Sigma, \\delta, q_1, F)D=({q1​,q2​,...,qn​},Σ,δ,q1​,F) 来说，其正则表达式递归公式为：\n⋃j∈FR1j(n)\\bigcup_{j \\in F} R_{1j}^{(n)}\nj∈F⋃​R1j(n)​\n其中：\nRij(k)=Rij(k−1)+Rik(k−1)(Rkk(k−1))∗Rkj(k−1)R_{ij}^{(k)} = R_{ij}^{(k-1)} + R_{ik}^{(k-1)} (R_{kk}^{(k-1)})^* R_{kj}^{(k-1)}\nRij(k)​=Rij(k−1)​+Rik(k−1)​(Rkk(k−1)​)∗Rkj(k−1)​\n初始条件：\nRij(0)={{a∣δ(qi,a)=qj}i≠j{a∣δ(qi,a)=qj}∪{ε}i=jR_{ij}^{(0)} =\n\\begin{cases}\n\\{ a \\mid \\delta(q_i, a) = q_j \\} &amp; i \\ne j \\\\\n\\{ a \\mid \\delta(q_i, a) = q_j \\} \\cup \\{\\varepsilon\\} &amp; i = j\n\\end{cases}\nRij(0)​={{a∣δ(qi​,a)=qj​}{a∣δ(qi​,a)=qj​}∪{ε}​i=ji=j​\n\n注意：\n\n当 i=ji = ji=j 的时候，表示是闭合的 qiq_iqi​ 所有的环(Cycle)的路径，也就是说递归 Rkk(k−1)R_{kk}^{(k-1)}Rkk(k−1)​ 中本身就包含了 Rkp(k−1)Rpk(k−1)R_{kp}^{(k-1)} R_{pk}^{(k-1)}Rkp(k−1)​Rpk(k−1)​ 的环形。\n(∅)∗=ε(\\varnothing)^* = \\varepsilon(∅)∗=ε，因此 ε\\varepsilonε 是正规式的一部分(ε\\varepsilonε 是正则表达式，L(ε)={ε}L(\\varepsilon) = \\{\\varepsilon\\}L(ε)={ε})。\n\n这个递归求解的过程类似于动态规划 Floyd算法，即填点法。如果很难理解可以先去看一下 Floyd 算法，这俩过程是非常类似的。\n\n\n\n\n优点\n缺点\n\n\n\n\n1. 是一套完整的算法，适合使用计算机来实现\n1. 求解过程复杂，假设 nnn 为状态个数，那么时间复杂度为 O(n3)O(n^3)O(n3)\n\n\n2. 在状态和状态转移函数不变的情况下，可以直接得出所有不同的 初始状态 和 终止状态 的正则表达式\n\n\n\n3. 可以一步一步进行化简\n\n\n\n\n\n\n递归式法例子: 求视别具有奇数长度 DFA 的正则表达式(D1D_1D1​)\nD1=({q0,q1},{0,1},δ,q0,{q1})D_1 = (\\{q_0, q_1\\}, \\{0, 1\\}, \\delta, q_0, \\{q_1\\})\nD1​=({q0​,q1​},{0,1},δ,q0​,{q1​})\n{δ(q0,0)=q1δ(q0,1)=q1δ(q1,0)=q0δ(q1,1)=q0\\begin{cases}\n\\delta(q_0, 0) = q_1 \\\\\n\\delta(q_0, 1) = q_1 \\\\\n\\delta(q_1, 0) = q_0 \\\\\n\\delta(q_1, 1) = q_0\n\\end{cases}\n⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​δ(q0​,0)=q1​δ(q0​,1)=q1​δ(q1​,0)=q0​δ(q1​,1)=q0​​\n\n\n\n与给出递归式的定义从 111 下标开始不同，这里是从 000 开始的。那么我们修改上述 Rij(−1)R_{ij}^{(-1)}Rij(−1)​ 为不经过任何中间状态从 qiq_iqi​ 转化到 qjq_jqj​ 的正则表达式。那么最终我们要求出 R01(1)R_{01}^{(1)}R01(1)​。\n\nk=−1k = -1k=−1，即不经过任何中间状态：\n\n\n\n\nRij(k)R_{ij}^{(k)}Rij(k)​\nk=−1k = -1k=−1\n\n\n\n\nR00(−1)R_{00}^{(-1)}R00(−1)​\nε+∅=ε\\varepsilon + \\varnothing = \\varepsilonε+∅=ε\n\n\nR01(−1)R_{01}^{(-1)}R01(−1)​\n0+10 + 10+1\n\n\nR10(−1)R_{10}^{(-1)}R10(−1)​\n0+10 + 10+1\n\n\nR11(−1)R_{11}^{(-1)}R11(−1)​\nε+∅=ε\\varepsilon + \\varnothing = \\varepsilonε+∅=ε\n\n\n\n为了方便观看，下面全部假设 (0+1)(0 + 1)(0+1) 为 aaa。\n\nk=0k = 0k=0，即经历中间状态 q0q_0q0​：\n\n\n\n\nRij(k)R_{ij}^{(k)}Rij(k)​\nk=0k = 0k=0\n\n\n\n\nR00(0)R_{00}^{(0)}R00(0)​\nR00(−1)+R00(−1)(R00(−1))∗R00(−1)=ε+ε⋅ε∗⋅ε=εR_{00}^{(-1)} + R_{00}^{(-1)}(R_{00}^{(-1)})^*R_{00}^{(-1)} = \\varepsilon + \\varepsilon \\cdot \\varepsilon^* \\cdot \\varepsilon = \\varepsilonR00(−1)​+R00(−1)​(R00(−1)​)∗R00(−1)​=ε+ε⋅ε∗⋅ε=ε\n\n\nR01(0)R_{01}^{(0)}R01(0)​\nR01(−1)+R00(−1)(R00(−1))∗R01(−1)=a+ε⋅ε∗⋅a=aR_{01}^{(-1)} + R_{00}^{(-1)}(R_{00}^{(-1)})^*R_{01}^{(-1)} = a + \\varepsilon \\cdot \\varepsilon^* \\cdot a = aR01(−1)​+R00(−1)​(R00(−1)​)∗R01(−1)​=a+ε⋅ε∗⋅a=a\n\n\nR10(0)R_{10}^{(0)}R10(0)​\nR10(−1)+R10(−1)(R00(−1))∗R00(−1)=a+a⋅ε∗⋅ε=aR_{10}^{(-1)} + R_{10}^{(-1)}(R_{00}^{(-1)})^*R_{00}^{(-1)} = a + a \\cdot \\varepsilon^* \\cdot \\varepsilon = aR10(−1)​+R10(−1)​(R00(−1)​)∗R00(−1)​=a+a⋅ε∗⋅ε=a\n\n\nR11(0)R_{11}^{(0)}R11(0)​\nR11(−1)+R10(−1)(R00(−1))∗R01(−1)=ε+a⋅ε∗⋅a=ε+aaR_{11}^{(-1)} + R_{10}^{(-1)}(R_{00}^{(-1)})^*R_{01}^{(-1)} = \\varepsilon + a \\cdot \\varepsilon^* \\cdot a = \\varepsilon + aaR11(−1)​+R10(−1)​(R00(−1)​)∗R01(−1)​=ε+a⋅ε∗⋅a=ε+aa\n\n\n\n\nk=1k = 1k=1，即经历中间状态 q0,q1q_0, q_1q0​,q1​：\n\n\n\n\nRij(k)R_{ij}^{(k)}Rij(k)​\nk=1k = 1k=1\n\n\n\n\nR00(1)R_{00}^{(1)}R00(1)​\nR00(0)+R01(0)(R11(0))∗R10(0)=ε+a⋅(ε+aa)∗⋅ε=ε+aa(aa)∗=(aa)∗\\begin{aligned}R_{00}^{(0)} + R_{01}^{(0)}(R_{11}^{(0)})^*R_{10}^{(0)} &amp;= \\varepsilon + a \\cdot(\\varepsilon + aa)^* \\cdot \\varepsilon \\\\&amp;= \\varepsilon + aa(aa)^* \\\\&amp;= (aa)^*\\end{aligned}R00(0)​+R01(0)​(R11(0)​)∗R10(0)​​=ε+a⋅(ε+aa)∗⋅ε=ε+aa(aa)∗=(aa)∗​  因为 ε+rr∗=r∗\\varepsilon + rr^* = r^*ε+rr∗=r∗\n\n\nR01(1)R_{01}^{(1)}R01(1)​\nR01(0)+R01(0)(R11(0))∗R11(0)=a+a⋅(ε+aa)∗⋅(ε+aa)=a+a(aa)∗+aaa(aa)∗=a(ε+aa(aa)∗)+a(aa)∗=a(aa)∗\\begin{aligned}R_{01}^{(0)} + R_{01}^{(0)}(R_{11}^{(0)})^*R_{11}^{(0)} &amp;= a + a \\cdot (\\varepsilon + aa)^* \\cdot (\\varepsilon + aa) \\\\&amp;= a + a(aa)^* + aaa(aa)^* \\\\&amp;= a(\\varepsilon + aa(aa)^*) + a(aa)^* \\\\&amp;= a(aa)^*\\end{aligned}R01(0)​+R01(0)​(R11(0)​)∗R11(0)​​=a+a⋅(ε+aa)∗⋅(ε+aa)=a+a(aa)∗+aaa(aa)∗=a(ε+aa(aa)∗)+a(aa)∗=a(aa)∗​\n\n\nR10(1)R_{10}^{(1)}R10(1)​\nR10(0)+R11(0)(R11(0))∗R10(0)=a+(ε+aa)⋅(ε+aa)∗⋅a=a+(aa)∗a+aa(aa)∗a=(ε+aa(aa)∗)a+(aa)∗a=(aa)∗a+(aa)∗a=(aa)∗a\\begin{aligned}R_{10}^{(0)} + R_{11}^{(0)}(R_{11}^{(0)})^*R_{10}{(0)} &amp;= a + (\\varepsilon + aa) \\cdot (\\varepsilon + aa)^* \\cdot a \\\\&amp;= a + (aa)^*a + aa(aa)^*a \\\\&amp;= (\\varepsilon + aa(aa)^*)a + (aa)^*a \\\\&amp;= (aa)^*a + (aa)^*a \\\\&amp;= (aa)^*a\\end{aligned}R10(0)​+R11(0)​(R11(0)​)∗R10​(0)​=a+(ε+aa)⋅(ε+aa)∗⋅a=a+(aa)∗a+aa(aa)∗a=(ε+aa(aa)∗)a+(aa)∗a=(aa)∗a+(aa)∗a=(aa)∗a​\n\n\nR11(1)R_{11}^{(1)}R11(1)​\nR11(0)+R11(0)(R11(0))∗R11(0)=(ε+aa)+(ε+aa)⋅(ε+aa)∗⋅(ε+aa)=ε+aa+(aa)∗+(aa)∗aa+aa(aa)∗+aa(aa)∗aa=(ε+aa(aa)∗)+(ε+aa(aa)∗)aa+(aa)∗=(aa)∗+(aa)∗aa+(aa)∗=(aa)∗(ε+aa)=(aa)∗\\begin{aligned}R_{11}^{(0)} + R_{11}^{(0)}(R_{11}^{(0)})^*R_{11}^{(0)} &amp;= (\\varepsilon + aa) + (\\varepsilon + aa) \\cdot (\\varepsilon + aa)^* \\cdot (\\varepsilon + aa) \\\\&amp;= \\varepsilon + aa + (aa)^* + (aa)^*aa + aa(aa)^* + aa(aa)^*aa \\\\&amp;= (\\varepsilon + aa(aa)^*) + (\\varepsilon + aa(aa)^*)aa + (aa)^* \\\\&amp;= (aa)^* + (aa)^*aa + (aa)^* \\\\&amp;= (aa)^*(\\varepsilon + aa) \\\\&amp;= (aa)^*\\end{aligned}R11(0)​+R11(0)​(R11(0)​)∗R11(0)​​=(ε+aa)+(ε+aa)⋅(ε+aa)∗⋅(ε+aa)=ε+aa+(aa)∗+(aa)∗aa+aa(aa)∗+aa(aa)∗aa=(ε+aa(aa)∗)+(ε+aa(aa)∗)aa+(aa)∗=(aa)∗+(aa)∗aa+(aa)∗=(aa)∗(ε+aa)=(aa)∗​\n\n\n\n因此我们可以得出该DFA的正则表达式为 R01(1)=a(aa)∗R_{01}^{(1)} = a(aa)^*R01(1)​=a(aa)∗。\n此外，我们还可以得出偶数长度二进制制的正则表达式为 R00(1)=(aa)∗R_{00}^{(1)} = (aa)^*R00(1)​=(aa)∗。\n实际上我们并不需要将所有的正则表达式求出，而是根据 R01(1)R_{01}^{(1)}R01(1)​ 的公式递归求解。\n\n\n b. 状态消除法\n状态消除法主要的思路是用标记了正则表达式的新路径替换被删掉的路径。\n\n步骤如下：\n\n\n\n步骤\n方法\n说明\n\n\n\n\n111\n1. 增加两个新节点表示新的初始状态和新的最终状态2. 使用 ε\\varepsilonε 各自连接到初始状态或最终状态\n1. 该步骤将DFA转化成 ε-NFA，具体在NFA中解释2. 这一步的目的是为了方便消除所有的状态3. 将原来的初始状态和最终状态修改为普通状态\n\n\n222\n使用并运算减少两个状态之间的单向边为一条\n这两条边用并运算表示\n\n\n333\n1. 使用连接和闭包运算逐步删除除了步骤一添加的点以外其他所有点2. 根据入和出的路径来添加新的边\n1. 添加边的个数为 入的个数 乘以 出的个数2. 对于指向自己的边，使用闭包3. 互相指向的两个状态最终转变成一个状态自己指向自己\n\n\n\n\n\n\n\n优点\n缺点\n\n\n\n\n1. 计算比较简单、快速\n1. 适合在纸上来进行演算，不适合计算机去实现\n\n\n\n2. 得到的不一定是最简式，需要手动化简，化简过程可能会非常复杂\n\n\n\n\n\n状态消除法例子：求识别具有奇数长度 000 二进制 DFA 的正则表达式\n下图是识别具有奇数长度 000 的二进制的DFA：\n\n\n\n根据步骤 111，添加两个新节点作为新的初始状态和结束状态：\n\n\n\n根据步骤 222，但此时并没有两个状态之间的单向边是多条的情况。\n根据步骤3，此时删除状态 q0q_0q0​：可知有两个入状态 q0q_0q0​ 的边和一个出状态 q0q_0q0​ 的边。\n\n第一条入边是 q2−(ε)→q0q_2 - (\\varepsilon) \\rightarrow q_0q2​−(ε)→q0​。\n第二条入边是 q1−(0)→q0q_1 - (0) \\rightarrow q_0q1​−(0)→q0​。\n唯一条出边是 q0−(0)→q1q_0 - (0) \\rightarrow q_1q0​−(0)→q1​。\n此外还有 q0q_0q0​ 指向自己的边 q0−(1)→q0q_0 - (1) \\rightarrow q_0q0​−(1)→q0​，这条边被视为 1∗1^*1∗。\n\n将第一条入边和出边结合，得到 q2−(1∗0)→q1q_2 - (1^*0) \\rightarrow q_1q2​−(1∗0)→q1​。\n将第二条入边和出边结合，得到 q1−(01∗0)→q1q_1 - (01^*0) \\rightarrow q_1q1​−(01∗0)→q1​。\n根据步骤2，此时 q1q_1q1​ 指向 q1q_1q1​ 的边有两条，分别是 111 和 (01∗0)(01^*0)(01∗0)，因此将这两条边用并操作结合为 1+(01∗0)1+(01^*0)1+(01∗0)，得到图为：\n\n\n\n根据步骤3，此时删除状态 q1q_1q1​：可知有一个入状态 q1q_1q1​ 和一个出状态 q1q_1q1​ 的边。\n\n入状态边是 q2−(1∗0)→q1q_2 - (1^*0) \\rightarrow q_1q2​−(1∗0)→q1​。\n出状态边是 q1−ε→q3q_1 - \\varepsilon \\rightarrow q_3q1​−ε→q3​。\n此外还有指向自己的边 q3−(1+(01∗0))→q3q_3 - (1+(01^*0)) \\rightarrow q_3q3​−(1+(01∗0))→q3​，被视为 (1+(01∗0))∗(1+(01^*0))^*(1+(01∗0))∗。\n\n将入状态边和出状态边结合得到 q2−1∗0(1+01∗0)∗→q3q_2 - 1^*0(1+01^*0)^* \\rightarrow q_3q2​−1∗0(1+01∗0)∗→q3​。\n\n\n\n所有点都删除完毕，因此可以知道该识别具有奇数长度 0 的二进制的正则表达式为 1∗0(1+01∗0)∗1^*0(1+01^*0)^*1∗0(1+01∗0)∗。\n\n\n 2. 非确定有穷自动机 NFA\n非确定的有限状态自动机(Nondeterministic Finite Automata, NFA)是一种识别正则语言的非确定性自动机。通常表示为五元组：\nN=(Q,Σ,δ,P0,F)N = (Q, \\Sigma, \\delta, P_0, F)\nN=(Q,Σ,δ,P0​,F)\n\n\n\n符合\n意义\n\n\n\n\nQQQ\n有穷状态集合\n\n\nΣ\\SigmaΣ\n可被识别的字母表\n\n\nδ\\deltaδ\n状态转移函数δ:Q×Σ→P(Q)\\delta: Q \\times \\Sigma \\to \\mathbb{P}(Q)δ:Q×Σ→P(Q)\n\n\nP0P_0P0​\n初始状态集合P0⊆QP_0 \\subseteq QP0​⊆Q\n\n\nFFF\n最终状态集合F⊆QF \\subseteq QF⊆Q\n\n\n\n i. 状态转移函数 δ\\deltaδ 表示法\n同DFA一样由函数表示法、状态转移图 或 状态转移表来表示。\n\n\n\n表示法\n说明\n\n\n\n\n函数表示法\n与DFA不同，δ\\deltaδ 等于的结果是一个状态的集合例如 δ(q0,0)={q1,q2}\\delta(q_0, 0) = \\{q_1, q_2\\}δ(q0​,0)={q1​,q2​}\n\n\n状态转移图\n与DFA一致，用有向边表示\n\n\n状态转移表\n与DFA不同，填入表格的是一个状态的集合而不是固定的状态\n\n\n\n ii. NFA 性质\n\n\n\n性质\n说明\n\n\n\n\n初始状态\n与DFA不同，NFA可以有多个初始状态\n\n\n状态转化\n同一个输入下，每个状态可以转化为多个状态也就是对于同一个输入字符串来说，可以有多个路径最终转化成多个状态\n\n\n卡住Stuck\n每个状态可以在某一个输入下转化的状态为空集 ∅\\varnothing∅此时称之为状态机卡住(stuck)，认为该语言无法被NFA识别\n\n\n最终状态\n只要存在一个路径转化的最终状态 qf∈Fq_f \\in Fqf​∈F 就称这个输入的字符串是被该NFA可接受的\n\n\n空转化\n1. NFA可以接受空输入 εεε 来进行空转化2. 即不消耗输入字符下直接转化成另一个状态3. 接受空转化的NFA称之为 ε−\\varepsilon-ε−NFA4. ε−\\varepsilon-ε−NFA 存在的目的是为了简化NFA例如可以将多个初始/接受状态使用空转移的方式来合并成同一个初始/接受状态\n\n\n识别语言的能力\nNFA没有增加DFA识别语言的能力，也就是说NFA识别的语言仍然是正则语言1. DFA转化NFA：DFA 本身就可以被视为一种 NFA2. NFA转化DFA：总是存在 DFA，使得 L(D)=L(N)L(D) = L(N)L(D)=L(N)3. NFA存在的目的是为了简化DFA\n\n\n\n iii. 扩展状态转移函数\n扩展状态转移函数用来表示一个状态在接收一个字符串后到达的状态。\nNFA的扩展状态转移函数递归定义为：\nδ^:Q×Σ∗→P(Q)\\hat{\\delta} : Q \\times \\Sigma^* \\to \\mathbb{P}(Q)\nδ^:Q×Σ∗→P(Q)\nδ^(P,x)={Px=εδ^(⋃{δ(q,a)∣q∈P},y)x=ay\\hat{\\delta}(P, x) = \n\\begin{cases} \nP &amp; x = \\varepsilon \\\\ \n\\hat{\\delta}(\\bigcup\\{\\delta(q, a)|q \\in P\\}, y) &amp; x = ay \n\\end{cases}δ^(P,x)={Pδ^(⋃{δ(q,a)∣q∈P},y)​x=εx=ay​\n那么可以定义NFA所接受的全部语言 L(D)L(D)L(D)：\nL(D)={w∣w∈Σ∗∧δ^(P0,w)∩F≠∅}L(D) = \\{w|w \\in \\Sigma^* \\land \\hat{\\delta}(P_0, w) \\cap F \\neq \\emptyset\\}\nL(D)={w∣w∈Σ∗∧δ^(P0​,w)∩F=∅}\n iv. NFA 转化为 DFA：子集构造法\n对于NFA：N=(QN,Σ,δN,P0,FN)N = (Q_N, \\Sigma, \\delta_N, P_0, F_N)N=(QN​,Σ,δN​,P0​,FN​)，可以构造出DFA：D=(QD,Σ,δD,P0,FD)D = (Q_D, \\Sigma, \\delta_D, P_0, F_D)D=(QD​,Σ,δD​,P0​,FD​)\n\n\n\n符号\n说明\n\n\n\n\nQDQ_DQD​\nQD=P(QN)Q_D = \\mathbb{P}(Q_N)QD​=P(QN​)，是状态集合的集合即DFA的一个状态是NFA状态的集合(包括空集)\n\n\nFDF_DFD​\nFD={Q∣Q⊆QN,Q∩F≠∅}F_D = \\{Q \\mid Q \\subseteq Q_N, Q \\cap F \\neq \\emptyset\\}FD​={Q∣Q⊆QN​,Q∩F=∅}1. 即DFA的一个最终状态是包含NFA至少一个最终状态的集合2. Q⊆QNQ \\subseteq Q_NQ⊆QN​ 等价于 Q∈QDQ \\in Q_DQ∈QD​，因此 QQQ 是一个集合，FDF_DFD​ 是一个状态集合的集合3. 有 FD⊆QDF_D \\subseteq Q_DFD​⊆QD​\n\n\nδD\\delta_DδD​\n∀P∈QD,δD(P,a)=⋃{δN(q,a)∣q∈P}\\forall P \\in Q_D, \\delta_D(P, a) = \\bigcup\\{\\delta_N(q, a) \\mid q \\in P\\}∀P∈QD​,δD​(P,a)=⋃{δN​(q,a)∣q∈P}即将该集合中的所有状态可以被输入 aaa 转化后的状态的集合并集\n\n\n\n步骤如下：\n\n画出DFA表格，横向标题(第1行)写出各个终结符，纵向可以无限扩展。\n在第2行的状态栏处填入NFA的初始状态集合 P0P_0P0​，并标入开始符号 →→→。\n将集合中所有初始状态分别带入当前列的终结符对应的状态转移方程中，取并集获得状态集合并填入到表格中。\n将未填入过的状态集合填入到新的状态栏处。\n重复步骤3, 4直到没有出现新的状态集合。\n将所有与NFA最终状态 FFF 取交集不为空集的状态集称为最终状态 ⋆\\star⋆。\n\n\n子集构造法例子：接受全部以01结尾的字符串的NFA转化DFA\n下图是一个接受全部以01结尾串的NFA，其五元组为：\nN=({q0,q1,q2},{0,1},δN,{q0},{q2})N = (\\{q_0, q_1, q_2\\}, \\{0, 1\\}, \\delta_N, \\{q_0\\}, \\{q_2\\})\nN=({q0​,q1​,q2​},{0,1},δN​,{q0​},{q2​})\n\n\n\n其状态转移表为：\n\n\n\nδN\\delta_NδN​\n0\n1\n\n\n\n\n→q0\\rightarrow q_0→q0​\n{q0,q1}\\{q_0, q_1\\}{q0​,q1​}\n{q0}\\{q_0\\}{q0​}\n\n\nq1q_1q1​\n∅\\varnothing∅\n{q2}\\{q_2\\}{q2​}\n\n\n⋆q2\\star q_2⋆q2​\n∅\\varnothing∅\n∅\\varnothing∅\n\n\n\n此时我们构造出对应的DFA为：\nD=(QD,{0,1},δD,{q0},FD)D = (Q_D, \\{0, 1\\}, \\delta_D, \\{q_0\\}, F_D)\nD=(QD​,{0,1},δD​,{q0​},FD​)\n我们通过步骤1, 2绘制出其状态转移表：\n\n\n\nδD\\delta_DδD​\n0\n1\n\n\n\n\n→{q0}=qA\\rightarrow \\{q_0\\} = q_A→{q0​}=qA​\n\n\n\n\n\n根据步骤3计算出其转移到的状态集合：\n\n\n\nδD\\delta_DδD​\n0\n1\n\n\n\n\n→{q0}=qA\\rightarrow \\{q_0\\} = q_A→{q0​}=qA​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n\n此时多出来一个新状态 {q0,q1}\\{q_0, q_1\\}{q0​,q1​}，我们根据步骤4将其填入到新的状态栏处，并根据步骤3计算出其转移到的专题集合：\n\n\n\nδD\\delta_DδD​\n0\n1\n\n\n\n\n→{q0}=qA\\rightarrow \\{q_0\\} = q_A→{q0​}=qA​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q2}=qC\\{q_0, q_2\\} = q_C{q0​,q2​}=qC​\n\n\n\n以此类推，根据步骤 5 最终可以得到：\n\n\n\nδD\\delta_DδD​\n0\n1\n\n\n\n\n→{q0}=qA\\rightarrow \\{q_0\\} = q_A→{q0​}=qA​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q2}=qC\\{q_0, q_2\\} = q_C{q0​,q2​}=qC​\n\n\n{q0,q2}=qC\\{q_0, q_2\\} = q_C{q0​,q2​}=qC​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n\n根据步骤 6，将与 FFF 取交集不为空集的状态集标记为 DFA 的最终状态，只有 {q0,q2}\\{q_0, q_2\\}{q0​,q2​} 符合：\n\n\n\nδD\\delta_DδD​\n0\n1\n\n\n\n\n→{q0}=qA\\rightarrow \\{q_0\\} = q_A→{q0​}=qA​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0,q2}=qC\\{q_0, q_2\\} = q_C{q0​,q2​}=qC​\n\n\n⋆{q0,q2}=qC\\star \\{q_0, q_2\\} = q_C⋆{q0​,q2​}=qC​\n{q0,q1}=qB\\{q_0, q_1\\} = q_B{q0​,q1​}=qB​\n{q0}=qA\\{q_0\\} = q_A{q0​}=qA​\n\n\n\n因此我们可以得到与该 NFA 等价的 DFA：\nD=({qA,qB,qC},{0,1},δD,qA,{qC})D = (\\{q_A, q_B, q_C\\}, \\{0, 1\\}, \\delta_D, q_A, \\{q_C\\})\nD=({qA​,qB​,qC​},{0,1},δD​,qA​,{qC​})\n\n\n\n\n\n v. 正则表达式转化 NFA\n假设正则表达式 E,FE, FE,F 转化为 NFA 后为 N(E),N(F)N(E), N(F)N(E),N(F)。\n使用下面的规则将正则表达式转化为NFA：\n\nN(∅)N(\\varnothing)N(∅)：终止状态为 ∅\\varnothing∅\n\n\n\n\n\nN(ε)N(ε)N(ε)：终止状态为初始状态。\n\n\n\n\n\nN(x)N(x)N(x)：从初始状态到终止状态有一条 xxx 的边。\n\n\n\n\n\nN(E+F)N(E+F)N(E+F)：两个自动机并行合在一起。\n\n\n\n\n\nN(EF)N(EF)N(EF)：删除所有 FFF 的起点箭头和 EEE 的接受状态，将所有连接到 EEE 接受状态的点（不是接受状态，接受状态被删除了）全部连接到 FFF 的初始状态。\n\n连向 FFF 初始状态的边值为这个点连一开始连向 EEE 接受状态的值。\n如果 EEE 中有 N(ε)N(ε)N(ε)，则不删除 FFF 的起点箭头。(因为是起点空转移到 FFF 的起点，相当于不删除 FFF 的起点箭头)。\n\n\n\n\nN(EF)N(EF)N(EF) 的例子\n假设 N(E),N(F)N(E),N(F)N(E),N(F) 为下图：\n\n\n\n那么 N(EF)N(EF)N(EF) 为：\n\n\n\n如果 N(E)N(E)N(E) 存在 N(ε)N(ε)N(ε)，如下图：\n\n\n\n那么 N(EF)N(EF)N(EF) 为：\n\n\n\n\n\n\nN(E∗)N(E^*)N(E∗)：将所有连向接受状态的点(不是接受状态)都连回初始状态，并增加一个 N(ε)N(ε)N(ε) 的点。\n\n连回初始状态的边值为这个点连向接受状态的值。\n注意很容易忘记增加 N(ε)N(ε)N(ε)。\n\n\n\n\nN(E∗)N(E^*)N(E∗) 的例子\n假设 N(E)N(E)N(E) 为下图：\n\n\n\n那么 N(E∗)N(E^*)N(E∗) 为下图：\n\n\n\n\n\n\n假设 E,FE,FE,F 使用的是 ε−ε-ε−NFA，即只有一个初始状态和结束状态，那么转化会更加简单。\n\n正则表达式转 ε−ε-ε−NFA 的规则\n假设 E,FE,FE,F 如下图所示：\n\n\n\n\nN(∅),N(ε),N(x)N(\\varnothing),N(ε),N(x)N(∅),N(ε),N(x) 同上。\nN(E+F)N(E + F)N(E+F) 为：\n\n\n\n\n\nN(EF)N(EF)N(EF) 为：\n\n\n\n\n\nN(E∗)N(E^*)N(E∗) 为：\n\n\n\n\n\n\n 四. 正则语言的泵原理 (Pumping lemma)\n泵原理(Pumping lemma)是用来判断某语言是否可以属于某一类语言的推论。泵原理往往用于证明语言不属于某一类语言。\n正则语言的泵原理(Pumping lemma for regular languages)是用来识别语言是否属于正则语言的推论。\n如果语言 LLL 是正则的，那么 ∃n∈N+\\exists n \\in \\mathbb{N}^+∃n∈N+，∀w∈L\\forall w \\in L∀w∈L，∣w∣≥n→w=xyz|w| \\geq n \\to w = xyz∣w∣≥n→w=xyz：\n\ny≠εy \\neq \\varepsilony=ε\n∣xy∣≤n|xy| \\leq n∣xy∣≤n\n∀k∈N\\forall k \\in \\mathbb{N}∀k∈N，xykn∈Lxy^k n \\in Lxykn∈L\n\n 1. 证明语言 LLL 不是正则语言\n使用泵原理证明语言 LLL 不是正则语言使用反证法，步骤如下：\n\n假设 LLL 是正则的，且 nnn 是泵值(pumping number)。\n取 w=w =w= 一个特殊情况 ∈L\\in L∈L，我们可以知道 w=xyzw = xyzw=xyz 且 ∣xy∣≤n,y≠ε|xy| \\leq n, y \\neq \\varepsilon∣xy∣≤n,y=ε。\n但是我们得出 xy0zxy^0 zxy0z 或 泵入 xy2z∉Lxy^2 z \\notin Lxy2z∈/L。\n根据泵原理，LLL 是正则的这一假设是错误的，因此 LLL 不是正则的。\n\n证明过程往往是举一个不符合泵原理的 w∈Lw \\in Lw∈L 例子，例如让项系数都等于 nnn。\n\n泵原理例子：证明语言 L={anbmcn+m∣n,m∈N}L = \\{a^n b^m c^{n+m} | n, m \\in \\mathbb{N}\\}L={anbmcn+m∣n,m∈N} 不是正则\n\n\n假设 LLL 是正则的，且 nnn 是泵值(pumping number)。\n\n\n取 w=anbnc2n∈Lw = a^n b^n c^{2n} \\in Lw=anbnc2n∈L，我们可以知道 w=xyzw = xyzw=xyz 且 ∣xy∣≤n,y≠ε|xy| \\leq n, y \\neq \\varepsilon∣xy∣≤n,y=ε。\n\n\n因为 ∣xy∣≤n,y≠ε|xy| \\leq n, y \\neq \\varepsilon∣xy∣≤n,y=ε，因此 y=ap,0&lt;p≤ny = a^p, 0 &lt; p \\leq ny=ap,0&lt;p≤n。\n\n\n我们得出 xy0z=an−pbnc2nxy^0 z = a^{n-p} b^n c^{2n}xy0z=an−pbnc2n，因为 0&lt;p≤n0 &lt; p \\leq n0&lt;p≤n，因此 n−p+n=2n−p≠2nn - p + n = 2n - p \\neq 2nn−p+n=2n−p=2n，也就是说 xy0z∉Lxy^0 z \\notin Lxy0z∈/L。\n\n\n根据泵原理，LLL 是正则的这一假设是错误的，因此 LLL 不是正则的。\n\n\n\n\n 2. 证明语言 LLL 是正则语言\n证明语言 LLL 是正则往往不使用泵原理，而是：\n\n画出 LLL 的 DFA 或 NFA。\n写出 LLL 等价的正则表达式。\n\n\n&lt;返回形式语言与自动机理论导航\n","slug":"笔记/形式语言与自动机理论/正则语言","date":"2024-05-25T10:49:37.000Z","categories_index":"笔记-形式语言与自动机理论","tags_index":"Automata Theory,Formal Languages,Regular","author_index":"zExNocs"},{"id":"c30912a6f6bb7272ee4bdbe774eedf10","title":"FLAT-形式语言与自动机理论导航","content":" 一. 文章导航\n\n\n\n\n\n\n\n特别注意\n阅读其他文章前请务必将导航下面的其他章节看完。\n\n\n\n\n\n笔记\n说明\n\n\n\n\n基本概念和术语\n本文章，阅读其他文章前务必看完该章节\n\n\n正则语言(Regular)\n正则法则、正则表达式、有限状态自动机 DFA/NFA、正则语言的泵原理\n\n\n上下文无关语言(CFL)\n上下文无关文法CFG、CFG语言派生树、歧义、CFG文法范式、解析器、下推自动机\n\n\n确定性上下文无关语言(DCFL)\n包括确定性上下文无关文法和确定的下推自动机(DPDA)\n\n\n上下文相关语言(CSL)\n包括上下文相关文法 (CSG)\n\n\n递归可枚举语言(REL)\n包括递归可枚举语法(REG)和图灵机(Turing Machines)\n\n\n\n 二. 介绍\n形式语言与自动机理论(Formal Languages and Automata Theory, FLAT) 是 计算理论（Theory of Computation）和数学的一个理论分支，基于图灵提出的数理逻辑中一阶逻辑的判定性问题而构想的图灵机。\n其主要关注于语言/计算模型中哪些问题是能计算的、哪些是不能计算的；用多少时间、要用多少存储。\n本笔记主要参考 视频-形式语言与自动机+哈工大(BV1oE4116794)。\n每种语言通常由文法、自动机、泵原理来解释。\n 三. 基本概念\n 1. 基本术语\n\n\n\n类型\n术语\n说明\n表示方法\n\n\n\n\n字符\n字符\n可打印符号和一些不可见的控制字符，是不可分割的符号/字符/字形\n实例：前置小写字母 a,ba, ba,b\n\n\n字母表字符集合\n字母表 Σ\\SigmaΣ\n字符的 非空有穷集合\n实例：希腊字母 α,β\\alpha, \\betaα,β抽象：Σ\\SigmaΣ\n\n\n字符串有序列\n字符串\n隶属于某字母表 Σ\\SigmaΣ，由其字符组成的 有穷 序列\n实例：后置小写字母 x,yx, yx,y\n\n\n字符串整型\n字符串长度 ∣x∣|x|∣x∣\n多少字符组成，递归定义：∣x∣={0x=ε∣y∣+1x=ya|x|=\\begin{cases}0 &amp; x = \\varepsilon \\\\|y| + 1 &amp; x = ya\\end{cases}∣x∣={0∣y∣+1​x=εx=ya​\n∣x∣|x|∣x∣\n\n\n字符串\n空字符串ε\\varepsilonε\n长度为 000 的串，隶属于任何字母表。空字符串不是字符，即 ε∉Σ\\varepsilon \\not\\in \\Sigmaε∈Σ\nε\\varepsilonε\n\n\n字符串\n字符串 xxx 与字符 aaa 的连接\n连接头：axaxax连接尾：xaxaxa\naxaxaxxaxaxa\n\n\n字符串\n字符串 xxx 与字符串 yyy 的连接\nxyxyxy 或者 x⋅yx \\cdot yx⋅y其中x⋅ε=ε⋅x=xx \\cdot \\varepsilon = \\varepsilon \\cdot x = xx⋅ε=ε⋅x=x\nxyxyxy  x⋅yx \\cdot yx⋅y\n\n\n字符串\n字符串的幂\nxn=xn−1⋅x , (n≥1)x^n = x^{n-1} \\cdot x\\ ,\\ (n \\ge 1)xn=xn−1⋅x , (n≥1)x0=εx^0 = \\varepsilonx0=ε\nxnx^nxn\n\n\n字符串集合\n字符串的集合\n1. 空集 ∅\\varnothing∅。注意 ∅≠{ε}\\varnothing \\ne \\{\\varepsilon\\}∅={ε}。∅\\varnothing∅ 的长度为 000，而 {ε}\\{\\varepsilon\\}{ε} 长度为 1112. 集合 AAA 和集合 BBB 之间的连接，记为 ABABAB 或者 A⋅BA \\cdot BA⋅B，公式为（排列组合）： A⋅B={w∣w=xy, x∈A∧y∈B}A \\cdot B = \\{ w \\mid w = xy, \\, x \\in A \\land y \\in B \\}A⋅B={w∣w=xy,x∈A∧y∈B}3. 集合的幂：An={{ε},n=0An−1⋅A,n≥1A^n = \\begin{cases} \\{\\varepsilon\\}, &amp; n = 0 \\\\[6pt] A^{n-1} \\cdot A, &amp; n \\ge 1 \\end{cases}An=⎩⎪⎨⎪⎧​{ε},An−1⋅A,​n=0n≥1​∅n={{ε},n=0∅,n&gt;0\\varnothing^n =\\begin{cases}\\{\\varepsilon\\}, &amp; n = 0 \\\\[6pt]\\varnothing, &amp; n &gt; 0\\end{cases}∅n=⎩⎪⎨⎪⎧​{ε},∅,​n=0n&gt;0​\n实例：大写字母 A,BA, BA,B\n\n\n字母表的幂字符串集合\n字母表的幂\nΣn\\Sigma^nΣn 表示源自字母表 Σ\\SigmaΣ 且所有长度为 nnn 的 字符串集合是字符串的集合而不是字符的集合，字符串之间可串联组合1. Σ0={ε}\\Sigma^0 = \\{\\varepsilon\\}Σ0={ε}2. Σ≠Σ1\\Sigma \\ne \\Sigma^1Σ=Σ1，前者是字符的集合，后者是长度为 1 的字符串集合3. ∅n={{ε},n=0∅,n&gt;0\\varnothing^n =\\begin{cases}\\{\\varepsilon\\}, &amp; n = 0 \\\\[6pt]\\varnothing, &amp; n &gt; 0\\end{cases}∅n=⎩⎪⎨⎪⎧​{ε},∅,​n=0n&gt;0​\nΣn\\Sigma^nΣn\n\n\n字母表的幂字符串集合\n克林闭包Kleene Closure\n字母表包括空字符串的所有可能组合的字符串Σ∗=⋃i=0∞Σi=Σ0∪Σ1∪Σ2∪⋯\\Sigma^* = \\bigcup_{i=0}^{\\infty} \\Sigma^i = \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 \\cup \\cdotsΣ∗=⋃i=0∞​Σi=Σ0∪Σ1∪Σ2∪⋯1. ∅∗=∅0={ε}\\varnothing^* = \\varnothing^0 = \\{\\varepsilon\\}∅∗=∅0={ε}\nΣ∗\\Sigma^*Σ∗\n\n\n字母表的幂字符串集合\n正闭包Positive Closure\n字母表不包括空字符串的所有可能组合的字符串Σ+=⋃i=1∞Σi=Σ1∪Σ2∪Σ3∪⋯\\Sigma^+ = \\bigcup_{i=1}^{\\infty} \\Sigma^i = \\Sigma^1 \\cup \\Sigma^2 \\cup \\Sigma^3 \\cup \\cdotsΣ+=⋃i=1∞​Σi=Σ1∪Σ2∪Σ3∪⋯1. Σ∗=Σ+∪{ε}\\Sigma^* = \\Sigma^+ \\cup \\{\\varepsilon\\}Σ∗=Σ+∪{ε}2. ∅+=∅\\varnothing^+ = \\varnothing∅+=∅\nΣ+\\Sigma^+Σ+\n\n\n集合\n子集的集合\n定义 P(Q)\\mathbb{P}(Q)P(Q) 是 QQQ 的所有子集的集合（集合的集合），包括空集1. P(Q)={S∣S⊆Q}\\mathbb{P}(Q) = \\{ S \\mid S \\subseteq Q \\}P(Q)={S∣S⊆Q}2. ∣P(Q)∣=2∣Q∣|\\mathbb{P}(Q)| = 2^{|Q|}∣P(Q)∣=2∣Q∣\nP(Q)\\mathbb{P}(Q)P(Q)\n\n\n语言字符串集合\n语言Language\n若 Σ\\SigmaΣ 为字母表，LLL 是字符串的集合，且 L⊆Σ∗L \\subseteq \\Sigma^*L⊆Σ∗，那么就称语言 LLL 是字母表 Σ\\SigmaΣ 上的语言1. 语言是一种字符串的集合2. ∅,{ε}\\varnothing, \\{\\varepsilon\\}∅,{ε} 是任何字母表上的语言\n实例：大写字母抽象：LLL\n\n\n文法规则集合\n文法Grammar\n1. 是一种语言结构的规则集合2. 可以是自然语言也可以是形式语言(例如正则表达式)3. 文法规定了哪些字符串是有效的(符合文法规则的)，哪些是无效的4. 每个文法确定唯一的语言，一种语言不一定只有一种文法详细定义请看下面的文法术语表\nGGG\n\n\n自动机\n自动机Automata\n1. 可以被称为机器(machine)2. 用于模拟具有有限个状态的机器或系统的行为，可以根据输入来改变当前状态，并且能够自动地按照预定的规则进行操作3. 自动机往往用于识别语言详细定义请看下面的自动机介绍\n实例：大写字母抽象：AAA\n\n\n\n 2. 文法术语\n文法通常由 终结符、非终结符、产生式 集合组成，定义了如何使用起始符号(Start Symbol)和产生式集合来构建合法的语言\n\n\n\n术语\n说明\n表示方法\n\n\n\n\n终结符Terminal Symbol\n1. 是语言的最基本符号2. 是构成语言的最小单元3. 无法使用文法规则更改成其他符号4. 字符相比终结符更宏大：例如本笔记中所有的字都属于&quot;字符&quot;，用来描述非终结符的符号也属于&quot;字符&quot;但在形式语言理论的描述中，我们的字符通常指的是终结符5. 在形式语言理论的描述中，字母表也可以被视为终结符的集合，字符串也可以被视为终结符的序列\n可以是任何字符或符号一般用小写字母 a,ba, ba,b 表示\n\n\n非终结符Nonterminal Symbol\n也可以叫 变元 和 文法范畴，是一种文法变量，是一种描述语言结构的符号可以通过文法规则进行组合和转换，生成新的结构\n一般用大写字母 A,BA, BA,B 表示\n\n\n产生式Production\n描述如何将一个 非终结符 替换为 终结符 和/或 其他非终结符 的序列1. 由被称为 头 或者 左部 的终结符、产生式符号 →\\to→ (读作定义为)、被称为 体 或 右部 的终结符 和/或 非终结符的序列 组成例如 A→aBA \\to aBA→aB，读作 AAA 定义为 aBaBaB2. 一个左部可以拥有多个右部，不同右部之间用 ∣|∣ 隔开例如 A→aB∣a∣αA \\to aB|a|\\alphaA→aB∣a∣α 3. 非终结符 AAA 的全体产生式称为 AAA 产生式\n→\\to→\n\n\n文法的派生Derivation\n从 文法变元 到 字符串 的分析过程称之为 推导 或 派生 (Derivation)派生是自顶向下的，由产生式的 头向体 的方向分析1. 只改变符号中最左边变元的派生过程被称为 最左派生2. 只改变符号中最右边变元的派生过程被称为 最右派生3. 任何派生都有其最左派生和最右派生即 A⇒∗Gw  ⟺  A⇒∗lmw  ⟺  A⇒∗rmwA \\underset{G}{\\overset{*}{\\Rightarrow}} w \\iff A \\underset{lm}{\\overset{*}{\\Rightarrow}} w \\iff A \\underset{rm}{\\overset{*}{\\Rightarrow}} wAG⇒∗​w⟺Alm⇒∗​w⟺Arm⇒∗​w也就是说，非终结符替换的顺序并不影响最终生成的终结符串\n一般一次派生: ⇒G\\underset{G}{\\Rightarrow}G⇒​一般多次派生: ⇒∗G\\underset{G}{\\overset{*}{\\Rightarrow}}G⇒∗​最左一次派生: ⇒lm\\underset{lm}{\\Rightarrow}lm⇒​最左多次派生: ⇒∗lm\\underset{lm}{\\overset{*}{\\Rightarrow}}lm⇒∗​最右一次派生: ⇒rm\\underset{rm}{\\Rightarrow}rm⇒​最右多次派生: ⇒∗rm\\underset{rm}{\\overset{*}{\\Rightarrow}}rm⇒∗​\n\n\n文法的规约Specification\n从 字符串 到 非终结符/变元 的分析过程称之为 递归推理 或 规约 (Specification)规约是自底向上的，由产生式的 体向头 的方向分析例如给出一个字符串来推理出文法产生该语言的过程\n\n\n\n\n\n往往使用 L(G)L(G)L(G) 表示该文法可以视别的语言。\n\n 3. 文法性质术语\n\n\n\n术语\n解释\n\n\n\n\nε\\varepsilonε 产生式\n形如 A→εA \\to \\varepsilonA→ε 的产生式被称为 ε\\varepsilonε 产生式\n\n\n符号可空性\n1. 终结符：所有终结符都是非可空的2. 变元：如果变元 A⇒∗GεA\\underset{G}{\\overset{*}{\\Rightarrow}}\\varepsilonAG⇒∗​ε，那么 AAA 是可空的i. 存在 A→εA \\to \\varepsilonA→ε, 那么 AAA 是可空的ii. 存在 B→αB \\to \\alphaB→α 且 α\\alphaα 所有变元都是可空的，那么 BBB 是可空的\n\n\n变元单元对\n如果有 A⇒∗GBA\\underset{G}{\\overset{*}{\\Rightarrow}}BAG⇒∗​B，那么称 [A,B][A, B][A,B] 是单元对1. 存在 A→BA \\to BA→B，那么 [A,B][A, B][A,B] 是单元对2. 若[A,B],[B,C][A, B], [B, C][A,B],[B,C] 是单元对，那么 [A,C][A, C][A,C] 是单元对注意：单元对是有序的\n\n\n符号可达性\n假设 SSS 是初始符号1. 终结符/变元：若 S⇒∗GαXβS \\underset{G}{\\overset{*}{\\Rightarrow}} \\alpha X \\betaSG⇒∗​αXβ，那么 XXX 是可达的i. 初始符号 SSS 是可达的ii. A→αA \\to \\alphaA→α，如果 AAA 是可达的，那么 α\\alphaα 所有符号都是可达的\n\n\n符号产生性\n假设 TTT 是终结符集合1. 终结符：终结符∈T\\in T∈T都是产生的2. 变元：若 αXβ⇒∗Gw,w∈T∗\\alpha X \\beta \\underset{G}{\\overset{*}{\\Rightarrow}} w, w \\in T^*αXβG⇒∗​w,w∈T∗，那么 XXX 是产生的i. 存在 A→αA \\to \\alphaA→α，如果 α\\alphaα 符号都是产生的，那么 AAA 是产生的\n\n\n符号无用性\n如果符号既是产生的也是可达的，那么符号就是有用的否则就是无用符号\n\n\n\n 4. 自动机介绍\n\n自动机的组成成分：\n\n\n\n\n组成成分\n符号\n说明\n\n\n\n\n状态集合\nQQQ\n自动机所处的状态\n\n\n字母表\nΣ\\SigmaΣ\n可以被自动机视别的字符集合\n\n\n状态转移函数\nδ\\deltaδ\n根据自动机状态和接收的字符将自动机转移到另一个状态的函数\n\n\n初始状态\n单个状态：sss多个状态：SSS\n自动机的初始状态或初始状态集合\n\n\n接受状态集合\nF⊆QF \\subseteq QF⊆Q\n自动机结束时可接受的状态\n\n\n其他组成成分\n\n由自动机的类型定义\n\n\n\n\n自动机类型：\n\n\n\n\n类型\n定义\n\n\n\n\n确定性自动机\n每个状态接收任何输入转移的状态唯一，在识别语言的过程每个阶段状态具有唯一性对于任意 q∈Q,a∈Σq \\in Q, a \\in \\Sigmaq∈Q,a∈Σ，δ(q,a)\\delta(q, a)δ(q,a) 有且只有一个结果，是确定且固定的通常 δ:Q×Σ→Q\\delta: Q \\times \\Sigma \\to Qδ:Q×Σ→Q\n\n\n非确定性自动机\n每个状态接收输入转移的状态不唯一对于任意 q∈Q,a∈Σq \\in Q, a \\in \\Sigmaq∈Q,a∈Σ，δ(q,a)\\delta(q, a)δ(q,a) 有多个结果通常 δ:Q×Σ→P(Q)\\delta: Q \\times \\Sigma \\to \\mathbb{P}(Q)δ:Q×Σ→P(Q)非确定性自动机允许使用空输入 ε\\varepsilonε 来进行状态转移\n\n\n\n\n往往使用 L(A)L(A)L(A) 表示该自动机可以视别的语言集合。\n\n 四. 语言分类 ———— 乔姆斯基层次结构\n乔姆斯基层次结构(Chomsky hierarchy) 是形式语言理论、计算机科学和语言学领域的一种包含形式文法的层次结构。\n下面表格层次从小到大排序。其中：\n\naaa 表示终结符。\nA,BA, BA,B 表示非终结符。\nα,β,γ\\alpha, \\beta, \\gammaα,β,γ 表示终结符和/或非终结符序列。\nwww 表示终结符序列(字符串)\nwRw^RwR 表示字符串的反转，即若 w=a1a2…an−1anw = a_1a_2 \\dots a_{n-1}a_nw=a1​a2​…an−1​an​，那么 wR=anan−1…a2a1w^R=a_na_{n-1} \\dots a_2a_1wR=an​an−1​…a2​a1​。\n\n\n\n\n类型\n语言\n自动机\n文法产生式约束\n实例\n\n\n\n\n类型3\n正则语言(Regular)\n有限状态自动机\nA→a,A→aBA \\to a, A \\to aBA→a,A→aB (右正则)A→a,A→BaA \\to a, A \\to BaA→a,A→Ba (左正则)\nL={an ∣ n&gt;0}L = \\{a^n\\ |\\ n &gt; 0\\}L={an ∣ n&gt;0}\n\n\n类型2.5\n确定性上下文无关(DCFL)\n确定性下推自动机\nA→αA \\to \\alphaA→α\nL={wcwR}L = \\{wcw^R\\}L={wcwR}\n\n\n类型2\n上下文无关(CFL)\n非确定性下推自动机\nA→αA \\to \\alphaA→α\nL={wwR}L = \\{ww^R\\}L={wwR}\n\n\n类型1\n上下文相关(CSL)\n线性有界非确定性图灵机\nαAβ→αγβ\\alpha A \\beta \\to \\alpha \\gamma \\betaαAβ→αγβ\nL={anbncn ∣ n&gt;0}L = \\{a^nb^nc^n\\ |\\ n &gt; 0\\}L={anbncn ∣ n&gt;0}\n\n\n类型0\n递归可枚举(REL)\n图灵机\nγ→α\\gamma \\to \\alphaγ→α\nL={w ∣ 终止图灵机}L = \\{w\\ |\\ \\text{终止图灵机}\\}L={w ∣ 终止图灵机}\n\n\n\n\n\n\n 五. 其他\n 1. 停机问题 (Halting Problem)\n咕咕咕\n","slug":"笔记/形式语言与自动机理论/形式语言与自动机理论导航","date":"2024-05-25T10:49:36.000Z","categories_index":"导航","tags_index":"Automata Theory,Formal Languages","author_index":"zExNocs"},{"id":"65f160195ccd659ac96d8ee64667ccce","title":"算法-最短路","content":"&lt;返回算法与数据结构导航\n\n 一. 最短路问题\n前提：最短路算法是指在一个图 G=(V,E)G = (V, E)G=(V,E) 中，VVV 分为 源点SSS、中间点 CCC 和终点 TTT，每个边都就有一个花销 W(E)W(E)W(E)。\n问题：从源点 SSS 到终点 TTT 的花销最少为多少。\n在下述描述中：\n\nnnn 为点的个数，mmm 为边的个数\nw(i,j)w(i, j)w(i,j) 表示点 iii 到点 jjj 的花销\n\n自边等于 000，即 w(i,i)=0w(i, i) = 0w(i,i)=0\n不连通等于 inf⁡\\infinf\n\n\n\n 1. 最短路算法的评判标准\n\n\n\n标准\n说明\n\n\n\n\n负边\n是否支持边的权值是负数的\n\n\n有向边\n是否支持有向边\n\n\n环\n是否支持环负环无意义\n\n\n多源\n是否可以多个源点分为单源最短路/多源最短路\n\n\n记录\n是否可以记录路径\n\n\n\n 二. 最短路算法\n 1. Floyd-Warshall (FW)\nFW算法是一个动态规划的算法，通过逐步加入点来构造子答案从而获取整体最优解的方法。\n i. FW 的实现方法\n\n\n\n步骤\n说明\n\n\n\n\n定义\n定义 d(i,j,k)d(i, j, k)d(i,j,k) 表示点 i,ji, ji,j 之间使用 {1,…,k}\\{1, \\dots, k\\}{1,…,k} 作为允许使用的潜在中间点的最短路花销例如 d(2,5,3)d(2, 5, 3)d(2,5,3) 表示仅使用 {1,2,3}\\{1, 2, 3\\}{1,2,3} 其中的点作为中间点时点 222 到点 555 的最小花销注意这些中间点可以使用也可以不使用，但不能使用其他的点\n\n\n初始化\n对于所有两个点之间 d(i,j,0)=w(i,j)d(i, j, 0) = w(i, j)d(i,j,0)=w(i,j)\n\n\n状态转移方程\n考虑当 k=k+1k = k + 1k=k+1 时，即对于每两对点之间在前 kkk 个点已经加入好，有d(i,j,k+1)=min⁡(d(i,j,k),d(i,k+1,k)+d(k+1,j,k))d(i, j, k+1) = \\min(d(i, j, k), d(i, k+1,k) + d(k+1, j, k))d(i,j,k+1)=min(d(i,j,k),d(i,k+1,k)+d(k+1,j,k))类似于一个松弛操作，即要么不加入这个点，要么从这个点走过来\n\n\n\n此外，可以对数组的 kkk 维度进行滚动数组。\n代码：\n1234for (int k = 0; k &lt; n; k++)  for (int i = 0; i &lt; n; i++)    for (int j = 0; j &lt; n; j++)      dist[i][j] = Math.min(dist[i][j], dist[i][k] + dist[k][j]);\n ii. FW的标准\n\n\n\n标准\n是否支持\n\n\n\n\n负边\n✔️\n\n\n有向边\n✔️\n\n\n环\n✔️\n\n\n多源\n✔️支持所有点之间的最短路\n\n\n记录\n✔️\n\n\n时间复杂度\nO(n3)O(n^3)O(n3)与边个数无关\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/算法/最短路算法","date":"2024-05-24T12:00:12.000Z","categories_index":"笔记-算法","tags_index":"Graph Theory,Algorithm,Shortest Path,Dijkstra,Floyd-Warshall","author_index":"zExNocs"},{"id":"5c601e076c464576a9ec4bb975f50ec5","title":"算法-简单动态规划","content":"&lt;返回算法与数据结构导航\n\n 一. 动态规划介绍\n\n\n\n\n\n\n\n\n\nDynamic Programming (DP) is a general method that can be suitable when the optimal solutions satisfy a “decomposition property”.\n动态规划(DP)是一种适用于最优解满足“分解性质”情况的通用方法。\n 1. DP的步骤\n\n将最优解分解为子解，相当于将问题分解为子问题，并且子解对于子问题是最优的。\n最优解可以通过更小的子问题的最优解来构建。\n\n 2. DP的思路\n\n对于某一个解，如果我想要得到这个解，我该知道哪些解才能得出这个解，并依次获取和尝试合并这些可能解的组合，并将这些解记录。\n又或者说，我现在已知某一个解，我是否可以让这个解和其他输入/解组合获取一个新的解。其中这个“得到”和“获取”的过程是一个状态转移的过程，这个过程是一个状态转移方程/贝尔曼方程(Bellman Equation)。\n\n 3. DP与其他算法类型比较\n\n\n\n类型\n相比\n\n\n\n\n分而治之\nDP中的子问题可以重叠，即不同的路径可能会遇见相同的子问题。此时需要避免重复计算\n\n\n暴力搜索\n暴力搜索是将所有可能的答案依次列出来并测试，答案之间可能没有太大的关系。而动态规划是根据状态转移来尝试获取哪些解\n\n\n\n 二. 动态规划的例子\n 1. 子集和问题\n问题：给出一个整型集合 SSS，和一个目标值 KKK，找出一个子集 SsubS_{sub}Ssub​，其元素的和等于 KKK。\n假设输入 S[i],0≤i≤(n−1)S[i], 0 \\leq i \\leq(n - 1)S[i],0≤i≤(n−1) 是集合的第 iii 个元素 (从 000 开始)\n步骤：\n\n定义 dp[i][m]=1,m∈[0,K]dp[i][m] = 1, m \\in [0, K]dp[i][m]=1,m∈[0,K] 表示使用前 iii 个元素组成的子集中 SsubS_{sub}Ssub​ 元素和可以等于 mmm。\n如果我们知道 dp[i−1][m]=1dp[i - 1][m] = 1dp[i−1][m]=1，那么可以根据它和当前元素 S[i]S[i]S[i] 得出 dp[i][m+S[i]]=1dp[i][m + S[i]] = 1dp[i][m+S[i]]=1\n对 m∈[0,K−S[i]]m \\in [0, K - S[i]]m∈[0,K−S[i]] 遍历依次检查是否为 111，得到 dp[i−1][m]=1dp[i - 1][m] = 1dp[i−1][m]=1\n由此可以得到状态转移方程：dp[i][m]=dp[i][m] ∣ dp[i−1][m−S[i]]dp[i][m] = dp[i][m]\\ |\\ dp[i - 1][m - S[i]]dp[i][m]=dp[i][m] ∣ dp[i−1][m−S[i]]\n\n其时间复杂度为 O(kn)O(kn)O(kn)。\n此外我们可以使用滚动数组将其变成一维dp，此时对容量 mmm 的遍历是从后往前的（如果从前往后会导致 iii 元素被重复计算，此时属于完全背包问题）。\n 其他\n\n\n\n笔记\n说明\n\n\n\n\n最短路算法\n二. 1. Floyd-Warshall 算法\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/算法/简单动态规划","date":"2024-05-24T12:00:11.000Z","categories_index":"笔记-算法","tags_index":"Algorithm,Dynamic Programming,DP","author_index":"zExNocs"},{"id":"b71aeb72416b4ad218769c37ddf1fec8","title":"算法-贪心算法","content":"&lt;返回算法与数据结构导航\n\n 一. 介绍\n贪心(Greedy)算法是一种常见的启发式算法，其主要思想是做出短期内看起来最好的决定，而不考虑未来的策略。\n一些贪心算法可以得到最优解，例如解决最小生成树(Minimal Spanning Tree, MST)的算法。而大部分贪心算法无法给出最优解，但是可以给出接近最优的解。\n 二. 贪心算法解决的问题\n 1. 最小生成树问题(Minimal Spanning Tre, MST)\n问题输入：联通的、有边权值的无向图(connected, undirected, weighted graph)。\n问题输出：一棵树，仅使用图中存在的边连接图中所有顶点，并且边权重的和是最小的。\n a. Prim 算法\nPrim 算法是一种贪心算法，其核心思想是从一个顶点开始，逐步将与当前树相连的边中权值最小的边加入树中，直到包含所有顶点为止，可视为一种“加点法”。\n实现思路：\n\n选择任意顶点 MMM\n选择对外可以连接到的所有的点中最小的那个边，并将边加入到 MST中，将点加入到内部的点中\n是否全部连接，如果没有则返回步骤 222。通过已连接的边个数判断，即 边的个数 e=n−1e = n - 1e=n−1\n\n算法实现：\n\n初始化数组 value[n] = inf，数组大小为点的个数，表示表示内部点对未连接的点边权的最小值；初始化连接边的个数m = 0\n随机选择一个点 MMM\n使value[M] = 0，并根据 MMM 连接的所有边 (M,V) 更新 value 数组\n找到 value[v] != 0 中最小的点 VVV，使 value[V] = 0，m++\n跟 333 一样根据 VVV 连接的所有边 (V,U) 更新 value 数组。\n判断是否所有的点已经连通，即 (m - 1) == n，如果没有则返回 444\n\n其中步骤 333 的伪代码为：\n12345value[M] = 0              # 因为已经被连接，所以更新为0forall e in edge(M)  v = e.v                 # 获取边连接的另一个点  if(e.value &lt; value[v])  # 根据边权值更新对外连接点的大小    value[v] = e.value \n b. Kruskal 算法\nKruskal 算法是一种贪心算法，其核心思想是将图中的所有边按权值从小到大排序，然后依次选择边，如果添加的边不会形成环，则将其加入最小生成树中，直至生成树包含 V−1V-1V−1 条边（其中 VVV 是顶点数），可视为一种“加边法”。\n其中判断是否会形成环可以使用 并查集(Union-find Disjoint Sets) 来判断。\n实现思路：\n\n将边按权值从小到大排序\n选择当前最小的边，判断边的两个点是否已经联通(并查集判断是否是同一个根节点)\n如果没有联通，则将这个边加入答案，并将这两个点加入同一个并查集中\n如果联通，则跳过这个边\n是否已经包含 V−1V-1V−1 条边，如果没有则返回 222\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/算法/贪心算法","date":"2024-05-24T12:00:10.000Z","categories_index":"笔记-算法","tags_index":"Algorithm,Greedy,Minimal Spanning Tre,Prim Algorithm,Kruskal Algorithm","author_index":"zExNocs"},{"id":"76a9d45b04968897c2ef95f634fdfd9b","title":"算法-排序算法","content":"&lt;返回算法与数据结构导航\n\n 一. 排序算法的基础理论\n排序算法简单来说就是将无序的数组变成有序的数组。\n 1. 排序算法的性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\n如果两个元素键值相等，排序算法会保留这两个元素的相对位置\n\n\n自适应性Adaptive\n如果数组已经接近已排序，那么算法的效率会提高\n\n\n接入模式Access Patterns\n1. 顺序接入(Sequential Access): 数据的读取和写入可以按照其在存储器中存放的顺序进行2. 随机接入(Random Access): 访问数据元素的位置是跳跃的、不连续的\n\n\n额外空间\n是否需要额外空间\n\n\n\n 2. 基于比较的排序算法\n如果一个排序算法仅包含关于 成对比较 元素的信息，那么就称这个排序是基于比较的(comparison-based)。\n并不是所有的排序算法都是基于比较的，例如桶排序(bucket sort)是使用实际的值来进行排序的，其时间复杂度是 O(n)O(n)O(n)，但是其实现依赖于其值的范围。是一种使用空间换取时间的方法。\n 3. 排序算法的时间复杂度理论极限\n对于 nnn 个数的数组来说，它一共拥有 n!n!n! 种排序方法。我们使用基于比较的算法来对数组进行排序是通过两两比较来减半它排序方法的可能性。也就是说，基于比较的排序算法本质其实是逐步将 n!n!n! 减半成 111。\n这意味着我们需要去做 log⁡2(n!)\\log_2(n!)log2​(n!) 次比较。其中 O(log⁡(n!))=O(nlog⁡n)O(\\log(n!)) = O(n \\log n)O(log(n!))=O(nlogn)。也就是说基于比较的算法不能比 O(nlog⁡n)O(n \\log n)O(nlogn) 更优。\n 二. 排序算法的实现\n 1. 冒泡排序(Bubble Sort)\n冒泡排序的基本思想是让让大的元素逐渐往后移动。\n因为算法中最大元素像水泡一样逐渐向上冒，因此被称为冒泡排序。\n其算法结构：\n\n外部循环(Outer loop)：扫描整个数组。\n内部循环(Inner loop)：对于数组每个元素与右边邻域对比，如果右边邻域更小则立即交换。\n\n i. 时间复杂度分析\n考虑到最差的情况，也就是每次循环都会进行交换。外部循环次数为 (n−1)(n - 1)(n−1)。假设当前外部循环 index=iindex = iindex=i，那么内部循环次数为 (n−i−1)(n - i - 1)(n−i−1)，因此总循环次数为 n(n−1)2\\frac{n(n - 1)}{2}2n(n−1)​。\n假设比较和交换原始操作数为 ttt 为常数，循环以外的原始操作数为 kkk 为常数，那么总原始操作数为 n(n−1)2+t(n−1)+k\\frac{n(n - 1)}{2} + t(n - 1) + k2n(n−1)​+t(n−1)+k。\n根据删除规则，我们可以知道它的时间复杂度是 O(n2)O(n^2)O(n2)。\n此外，也可以使用递归关系来证明冒泡排序的时间复杂度：\n\n使用递归关系证明冒泡排序的时间复杂度\n首先冒泡排序并不是天然递归的，而是一个双重循环。\n但是我们能使用递归思想来将冒泡排序改成递归：如果要将长度为 nnn 的数组进行排序，首先将这个数组中的最大数值通过冒泡操作交换到当前数组最右边的位置并固定，随后再将剩下 n−1n - 1n−1 的数组进行排序（递归）。\n这样就写出其运行时间的递推公式：\nT(n)=dn+T(n−1)T(n) = dn + T(n - 1)\nT(n)=dn+T(n−1)\nT(1)=1T(1) = 1\nT(1)=1\n\ndndndn 表示通过冒泡操作交换所需要的时间。\nT(n−1)T(n - 1)T(n−1) 表示剩余数组排序所需要的时间。\n\n我们可以根据等差数列求出 T(n)T(n)T(n) 的通项公式为：\nT(n)=1+(n(n+1)2−1)dT(n) = 1 + \\left( \\frac{n(n + 1)}{2} - 1 \\right)d\nT(n)=1+(2n(n+1)​−1)d\n那么很明显 T(n)T(n)T(n) 是 Θ(n2)\\Theta(n^2)Θ(n2)。\n\n\n ii. 性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\nTRUE如果相同元素不进行交换\n\n\n自适应性Adaptive\nTRUE通过判断内循环是否交换过来拥有自适应性\n\n\n接入模式Access Patterns\n顺序接入\n\n\n额外空间\nNO\n\n\n链表适用\n适合单向链表\n\n\n\n 2. 选择排序(Selection Sort)\n选择排序的基本思想是将最大的元素直接插入到最右侧。\n实现思路是保持数组后面的元素不变作为已排序的元素，前面的元素作为未排序的元素，选择未排序的元素组中最大的元素插入到已排序元素组的头部。\n其算法结构是：\n\n外部循环：扫描整个数组。\n内部循环：\n\n扫描整个 未排序 部分的数组并记录住最大元素的位置。\n将最大的元素插入到已排序部分的数组的最左边；或者与未排序部分的数组最右边的元素交换并将其加入到已排序部分数组。\n\n\n\n i. 与冒泡排序的比较\n与冒泡排序不同，冒泡排序是比较当前元素和其邻域，而插入排序是比较当前元素和当前被记录的元素。\n为什么要延迟交换而不是立即交换：\n\n如果交换操作可能会比较昂贵，并不像数组一样是 O(1)O(1)O(1)，那么就需要尽可能减少交换次数。\n如果数组非常大，那么需要尽可能地减少交换次数来提高效率。\n\n ii. 时间复杂度分析\n相比于冒泡排序，它们具有相同数量的迭代和比较，仅仅是有更少数量的交换。\n因此它的时间复杂度也是 O(n2)O(n^2)O(n2)。\n iii. 性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\nFALSE\n\n\n自适应性Adaptive\nFALSE\n\n\n接入模式Access Patterns\n顺序接入\n\n\n额外空间\nNO\n\n\n链表适用\n适合单向链表\n\n\n\n 3. 插入排序(Insertion Sort)\n插入排序的基本思想是选择当前元素，将其插入到合适的位置。\n实现思路是保持数组前面元素的排序不变作为已排序的元素，后面的元素作为未排序的元素。选择当前未排序第一个元素，将其插入到已排序元素中从右到左第一个等于或小于该元素的元素右边（通过不断地交换实现）。\n其算法结构：\n\n外部循环：扫描整个数组。\n内部循环：获取并记录当前 未排序 元素的最左边元素，从右到左扫描 已经排序 的元素，如果被扫描的元素比记录的元素大，那么就交换；直到被扫描的元素比记录的元素小。\n\n i. 与冒泡排序的比较\n类似于冒泡排序，插入排序也是通过不断地交换将元素到达对应的位置。\n不同的是，冒泡排序是将未排序的元素中最大值达到最右侧（位置固定，但是选择的元素会变）；插入排序是将选择的元素插入到对应的位置（选择的选择不变，但位置会变）。\n ii. 时间复杂度分析\n\n在最坏的情况下，它的外部原始操作数是 O(n)O(n)O(n)，内部原始操作数是 O(n)O(n)O(n)，因此它的总时间复杂度是O(n∗n)=O(n2)O(n * n) = O(n^2)O(n∗n)=O(n2)。\n在最佳的情况下，它内部循环操作数是 O(1)O(1)O(1)，那么它的总时间复杂程度是 O(n∗1)=O(n)O(n*1)=O(n)O(n∗1)=O(n)。\n\n iii. 性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\nTRUE如果相同的元素不进行交换\n\n\n自适应性Adaptive\nTRUE\n\n\n接入模式Access Patterns\n顺序接入\n\n\n额外空间\nFALSE\n\n\n链表适用\n不适合单向链表；适合双向链表\n\n\n\n 4. 归并排序(Merge Sort)\n归并排序的主要思想是分而治之(divide-and-conquer)，它是先划分再排序。\n简单说就是递归分解，回归组合(进行归并 (merge))。\n i. 时间复杂度分析\n\n\n\n操作\n说明\n时间复杂度\n\n\n\n\n分解Divide\n将待排序的数组 SSS 分解为两个部分 S1S_1S1​, S2S_2S2​\n对于数组来说，分解时间复杂度为 O(1)O(1)O(1)\n\n\n递归Recur\n分别分解 S1S_1S1​ 和 S2S_2S2​ 直到剩下单个元素或者空元素为止单个元素或空元素被视为已经排序好的数组\n递归的时间复杂度是 O(log⁡(n))O(\\log(n))O(log(n))也就是归并二叉树的高度\n\n\n组合Conquer\n将已排序的 S1S_1S1​ 和 S2S_2S2​ 进行合并(merge)：1. 比较两个数组当前第一个数(最小的数)2. 选择最小的一个放入新的数组中3. 直到一个数组为空后将另一个数组的剩余元素依次放入新的数组合并后的数组是已经排序好的数组\n假设放入的操作时间复杂度为O(m)O(m)O(m)那么合并的时间复杂度是O(m×n)O(m \\times n)O(m×n)对于数组来说 m=1m = 1m=1是归并二叉树的每一层排序加起来为 O(n)O(n)O(n)\n\n\n\n整体而言，对于数组来说归并排序的时间复杂度是稳定的 O(nlog⁡(n))O(n\\log(n))O(nlog(n))\n其时间复杂度也可以使用 Master定理 证明，详细见 Big-Oh 和 Master 定理 中 三.2. 的例子。\n归并排序递归的过程是一个二叉树的结构。\n\n ii. 性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\nTRUE归并遇见相等数据时优先选择左边数组\n\n\n自适应性Adaptive\nFALSE非常稳定的 O(nlog⁡(n))O(n\\log(n))O(nlog(n))\n\n\n接入模式Access Patterns\n顺序接入\n\n\n额外空间\nTRUE也可以不使用，但是较为复杂一般不考虑\n\n\n链表适用\n需要快速对中间数据进行访问，不适合链表\n\n\n\n 5. 快速排序 (Quick Sort)\n快排主要思想是分而治之，它是先排序后划分。\n i. 时间复杂度分析\n\n\n\n操作\n说明\n时间复杂度\n\n\n\n\n分解Divide\nPartition操作：随机选择一个元素 xxx 称之为枢 pivot。将数组 SSS 分为：1. LLL: 值小于 xxx 的元素集合2. GEGEGE：值大于等于 xxx 的元素集合\n假设 交换 或者 删除再插入 的时间复杂度是 O(1)O(1)O(1) 分解的时间复杂度是 O(n)O(n)O(n)\n\n\n递归Recur\n对 LLL 和 GEGEGE 分别进行分解，直到剩下一个元素或空元素\n最差情况：枢总是最值，递归所有子集的时间复杂度为 O(n)O(n)O(n)最佳情况：枢总是中间值，递归所有子集的时间复杂度为 O(log⁡(n))O(\\log(n))O(log(n))\n\n\n组合Conquer\n将 LLL 和 GEGEGE 左右连接起来\nO(1)O(1)O(1)\n\n\n\n其时间复杂度总体：\n\n最差的情况下为 O(n2)O(n^2)O(n2)\n最佳的情况下为 O(nlog⁡(n))O(n\\log(n))O(nlog(n))\n平均情况下在一半的时间中快速排序选择的枢是中间值，那么时间复杂度是 O(nlogn)O(nlogn)O(nlogn)\n\n也可以认为平均情况下，选择的枢值总是让两个子集分解成 13\\frac{1}{3}31​ 和 23\\frac{2}{3}32​ 区域，因此二叉树高度为 32log⁡(n)\\frac{3}{2}\\log(n)23​log(n)\n具体证明可以自行查看维基百科\n\n\n\n快速排序递归调用的过程是一个二叉树结构。\n ii. 快排的分解 Partition操作 实现\n有两个方法：\n\n使用额外的空间进行分解\n\n创建两个数组，分别表示 LLL 和 GEGEGE\n选择一个枢(pivot)\n从左到右遍历数组，将小于枢的数加入到 LLL，将大于等于枢的数加入到 GEGEGE\n\n\n使用双指针的方法进行分解，不需要额外空间\n\n选择一个枢(pivot)\n定义两个指针 jjj 和 kkk，分别初始化指向数组的开头和结尾\n使用 jjj 向右扫描，直到找到第一个 ≥≥≥ 枢的元素 或者 j==kj==kj==k 停止\n使用 kkk 向左扫描，直到找到第一个 &lt;&lt;&lt; 枢的元素 或者 j==kj==kj==k 停止\n交换 j,kj, kj,k 的元素\n如果 j&lt;kj&lt;kj&lt;k，则返回操作 333\n此时 j==kj==kj==k，并且此时 j,kj,kj,k 的位置元素等于枢，也是 GEGEGE 位置的左边界线\n\n\n\n iii. 枢选择\n如果选择枢的方式是固定而不是随机的(例如总是选择第一个值作为枢)，并且出现了 LLL 子集是空的情况(此时选择的枢是最小值)，那么此时会导致算法出现死循环。因为每次对 GEGEGE 子集进行排序时，总是会选择最左边的值(也是最小值)作为枢，从而导致 LLL 子集是空的情况。\n\n\n\n\n\n\n\n\n\n快速排序要避免固定选择枢和出现一方子集是空集的情况，否则可能会导致死循环。\n解决方法：\n\n使用随机的方式选择枢\n为三个部分：分别是 L,pivot,E+GL, {pivot}, E + GL,pivot,E+G\n三点取值：选择最左边的数、中间的数和最右边的数中的中位数作为枢\n\n iv. 性质\n\n\n\n性质\n说明\n\n\n\n\n稳定性Stability\nFALSE\n\n\n自适应性Adaptive\nFALSE\n\n\n接入模式Access Patterns\n随机接入\n\n\n额外空间\nFALSE\n\n\n链表适用\n适用双向链表\n\n\n\n 三. 快速比较\n\n\n\n性质\n冒泡排序\n选择排序\n插入排序\n归并排序\n快速排序\n\n\n\n\n稳定性Stability\n✔️\n✖️\n✔️\n✔️\n✖️\n\n\n自适应性Adaptive\n✔️\n✖️\n✔️\n✖️\n✖️\n\n\n接入模式Access Patterns\n顺序\n顺序\n顺序\n顺序\n随机\n\n\n额外空间\n✖️\n✖️\n✖️\n✔️\n✖️\n\n\n链表适用\n✔️\n✔️\n双: ✔️\n✖️\n双: ✔️\n\n\n时间复杂度\nO(n2)O(n^2)O(n2)\nO(n2)O(n^2)O(n2)\nO(n2)O(n^2)O(n2)\nO(nlog⁡(n))O(n\\log(n))O(nlog(n))\nO(nlog⁡(n))O(n\\log(n))O(nlog(n))\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/算法/排序","date":"2024-05-24T12:00:09.000Z","categories_index":"笔记-算法","tags_index":"Algorithm,Sort","author_index":"zExNocs"},{"id":"2de2788f125fd3108d2350b3554e8f62","title":"算法基础理论和定义","content":"&lt;返回算法与数据结构导航\n\n 一. 基础理论\n算法的设计思路通常分为下面四种：\n\n\n\n思路\n说明\n\n\n\n\n暴力搜索(Brute Force)\n生成所有潜在解决方案并测试哪些是实际解决方案。时间复杂通常非常高，是属于多项式级其以上的时间复杂度\n\n\n分而治之(Divide and Conquer)\n递归地将问题分解成更小的部分并逐步解决它们，然后将它们重新组合在一起。是一种比较高效的设计思路\n\n\n启发式(Heuristics)\n是一个使用经验法则(rule of thumb)设计的算法。启发式算法比简单的方法能够做出更好的决策，但仍然不一定是最佳的\n\n\n动态规划(DP)\n是一种适用于最优解满足“分解性质”情况的通用方法DP中的子问题可以重叠，即不同的路径可能会遇见相同的子问题\n\n\n\n 1. 启发式算法\n启发式算法是一个使用经验法则(rule of thumb)设计的算法。启发式算法比简单的方法能够做出更好的决策，但仍然不一定是最佳的。\n通常有两种：\n\n\n\n种类\n目的\n例子\n\n\n\n\n总是最佳\n通常是为了加快程序运行速度\n1. A* 搜索算法2. 快速排序随机选择枢(pivot)\n\n\n不总是最佳\n给出以其他方式无法获得的良好答案一般用于解决一些NP-hard问题，例如 TSP问题、图染色问题\n遗传算法、模拟退火\n\n\n\n i. 启发式算法的常见问题\n// todo\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/算法/算法基础理论和定义","date":"2024-05-24T12:00:08.000Z","categories_index":"笔记-算法","tags_index":"Algorithm,Heuristics","author_index":"zExNocs"},{"id":"d887fe0c72dd8bd0586b67fecb09745c","title":"数据结构-二叉搜索树 BST","content":"&lt;返回算法与数据结构导航\n\n 一. 二叉搜索树的定义和 ADT\n二叉搜索树是一个储存(key,value)值到节点的二叉树，并满足下面的性质：\n\n\n\n性质\n\n\n\n\n对于任意一个内部节点 vvv，拥有左子节点 lll 和 右子节点 rrr，满足 key(l) &lt;= key(v) &lt;= key(r)\n\n\n对于任意一个节点 vvv，其左边子树节点的所有值都比 vvv 小，右边子树节点的所有值都比 vvv 大\n\n\n二叉搜索树的中序遍历(inorder traversal)返回的数组一定是根据key升序的\n\n\n\n 1. 二叉搜索树的主要操作和初步实现方法\n设二叉搜索树的高为 hhh。对于平衡二叉树来说，h=log⁡(n)h = \\log(n)h=log(n)。\n\n\n\n操作\n函数\n说明\n时间复杂度\n\n\n\n\n查找\nsearch(Key): Value\n与当前节点储存的key比较：如果等于就返回该值如果比 keykeykey 小则查找左节点如果比 keykeykey 大则查找右节点如果为空(或叶节点)，则返回 null\nO(h)O(h)O(h)\n\n\n插入\ninsert(Key, Value)\n类似于查找的方式找到合适的位置进行插入，作为新的叶节点\nO(h)O(h)O(h)\n\n\n删除\nremove(Key): Value\n类似于查找的方式找到相应的节点，分为下面四个情况：1. 没有找到该节点，此时返回 null2. 该节点是叶节点，直接删除并返回该节点并返回该节点的值3. 节点具有一个子节点，将该子节点替换到被删除节点的位置4. 节点具有两个子节点：i. 此时根据树的中序遍历找到当前 key 的下一个 key 节点 www (称之为中序后继)ii. 使用 www 替代该节点iii. 同删除步骤尝试删除 www，直到不符合被删除的节点具有两个子节点为止\nO(h)O(h)O(h)\n\n\n\n 2. 平衡二叉树(Balanced Trees)\n平衡二叉树的任何节点的左子树和右子树高度最多相差 111。\n平衡二叉树的高度 h=log⁡(n)h = \\log(n)h=log(n)。\n 3. 二叉树的旋转\n一般使用旋转的方法让二叉搜索树逐步变成平衡二叉树。\n i. 一次旋转\n一次旋转适合三个高度节点之间呈现类似于 \\ 或者 / 的直线型，也就是中间高度的节点是中间值的情况。\n此时可以将 \\ 或者 / 旋转成 ^。\n旋转的时间复杂度为 O(1)O(1)O(1)\n根据子节点的位置又分为左旋转和右旋转。\n a. 左旋转\n左旋转是旋转当前节点 VVV 和其右节点 RRR，因此要求右节点 RRR 不为空。\n其主要有两个步骤：\n\n\n\n步骤\n伪代码\n\n\n\n\n1. 将右节点 RRR 的左节点变成节点 VVV 的新右节点 (给予左节点)\nV.right = R.left\n\n\n2. 将右节点的左节点变成该节点 (交换节点位置)\nR.left = V\n\n\n3. 将 VVV 的父节点相应位置修改为 RRR；或者修改 root\nparent(V).left/right = Rroot = R\n\n\n\n b. 右旋转\n右旋转是旋转当前节点 VVV 和其左节点 LLL，因此要求左节点 LLL 不为空。\n\n\n\n步骤\n伪代码\n\n\n\n\n1. 将左节点 LLL 的右节点变成节点 VVV 的新左节点 (给予右节点)\nV.left = L.right\n\n\n2. 将左节点的右节点变成该节点 (交换节点位置)\nL.right = V\n\n\n3. 将 VVV 的父节点相应位置修改为 LLL；或者修改 root\nparent(V).left/right = Lroot = L\n\n\n\n\n ii. 二次旋转\n诸如图中的 &gt; 和 &lt; 形无法直接进行一次旋转。例如，将 &gt; 进行左旋转会变成 &lt;。\n此时需要将 &gt; 和 &lt; 形旋转成 \\ 或者 / 形，就同上述一次旋转了。\n\n 二. 二叉搜索树 CDT\n 1. AVL 树\n// todo\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/二叉搜索树","date":"2024-05-24T12:00:06.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Tree,Binary Tree,Binary Search Tree,BST,AVL Tree","author_index":"zExNocs"},{"id":"e0bbb840c83f55fe9b80bf1823d58d81","title":"数据结构-映射 Map","content":"&lt;返回算法与数据结构导航\n\n 一. 映射的 ADT\n\n\n\n\n\n\n\n\n\nA map models a collection of (key, value) entries that is searchable by the key.\n映射是可通过 key 进行搜索的 (key, value) 条目的 collection。\n在映射中，具有相同 key 值的元素是不被允许的。也就是说，一个 key 值只对应一个 (key, value) 条目(Entry)。\n 1. 映射的子类\n映射的子类主要是条目 Entry，其具有以下的成员：\n\n\n\n成员名\n变量\n\n\n\n\n键\nKey k\n\n\n值\nValue v\n\n\n\n键和值通常是由两个模板(Template)实现。\n 2. 映射的主要操作：\n\n\n\n名称\n函数\n说明\n\n\n\n\n获取\nget(Key): Value\n根据 key 找到对应的 value; 不存在则返回 null\n\n\n插入\nput(Key, Value): Value\n插入 (key, value) 并返回原来的 value ; 不存在则返回 null\n\n\n删除\nremove(Key): Value\n删除 (key, value) 并返回原来的 value; 不存在则返回 null\n\n\n状态\nsize(): int\n返回条目个数\n\n\n状态\nisEmpty(): boolean\n返回 Collection 是否为空\n\n\n迭代器\nkeySet(): Iterator\n返回 key 集合的迭代器\n\n\n迭代器\nvalueSet(): Iterator\n返回 value 集合的迭代器\n\n\n迭代器\nentrySet(): Iterator\n返回 (key, value) 集合的迭代器\n\n\n\n 二. 映射的 CDT\n 1. 基于简单链表的映射\n使用链表模拟的映射。具有以下时间复杂度：\n\n\n\n名称\n函数\n说明\n时间复杂度\n\n\n\n\n获取\nget(Key): Value\n顺序遍历链表寻找 key\nO(n)O(n)O(n)\n\n\n插入\nput(Key, Value): Value\n需要先遍历链表寻找是否有重复 key如果没有进行插入如果有则进行替换\nO(n)O(n)O(n)\n\n\n删除\nremove(Key): Value\n顺序遍历链表寻找 key\nO(n)O(n)O(n)\n\n\n\n因为链表具有较差的访问能力，因此无论是排序的链表还是未排序的链表（链表无法使用二分查找法）时间复杂度操作都是 O(n)O(n)O(n)。\n 2. 基于哈希表(Hash Table)的映射 (HashMap)\n基本思想：将每个 key 转化成 index 放入一个较大的数组中。\n转化的过程是将 key 编码成一个整型，再将整个整型压缩到一个范围内作为数组的 index。\n i. 哈希表的术语\n\n\n\n术语\n解释\n\n\n\n\n哈希码 h1h_1h1​Hash Code\n是将键值转一个整型的函数，即keys → integers。也可以说是将 key编码一些可能的方法：1. 将 key 的内存地址作为哈希码2. 将 key 的 bit 值转化成整型作为哈希码。一般用于内存不大于整型的数据类型，例如byte, short, int, float3. 将 key 的 bit 值划分成相同长度的部分，对这些部分求和(忽略溢出)。适用于内存大于整型的数据类型，例如 double, long4. 多项式累积方法主要是字符串：将字符串看作一个多项式，每个字符的 ASCII 作为系数与作为变量的固定的基数进行累乘：H=s0⋅bk−1+s1⋅bk−2+⋯+sk−1⋅b0H = s_0 \\cdot b^{k - 1} + s_1 \\cdot b^{k - 2} + \\dots + s_{k-1}\\cdot b^0H=s0​⋅bk−1+s1​⋅bk−2+⋯+sk−1​⋅b0Hi=Hi−1∗b+siH_i = H_{i - 1} * b + s_iHi​=Hi−1​∗b+si​\n\n\n压缩函数 h2h_2h2​Compression Function\n是将整型压缩到一定范围的函数，即integers → [0, N-1]。一些可能的方法：1. 除法(Division): h2(x)=x mod Nh_2(x) = x\\ \\text{mod}\\ Nh2​(x)=x mod N NNN 通常是一个素数2. 乘加除法(Multiply, Add and Divide (MAD)): h2(x)=(ax+b) mod Nh_2(x) = (ax + b)\\ \\text{mod}\\ Nh2​(x)=(ax+b) mod N a,ba, ba,b 是非负整数a mod N≠0a\\ \\text{mod}\\ N \\not = 0a mod N=0, 否则 xxx 无论多少总能映射到 bbb \n\n\n哈希函数 hhhHash Function\n公式：h(x)=h2(h1(x))h(x) = h_2(h_1(x))h(x)=h2​(h1​(x)) 定义：将一个对象映射成一个固定范围[0,N−1][0, N-1][0,N−1]的整形的函数目的：使用明显随机的方式来将 keys 分散分散的目的是为了减少冲突(Collision)随机的目的是为了减少模式(Pattern)，从而减少冲突\n\n\n哈希值Hash Value\n由哈希函数得到的值被称为哈希值\n\n\n冲突Collision\n当不同的元素获取到相同的索引时，会发生冲突\n\n\n\n那么使用哈希表来实现Map主要的思路是：\n\n寻找哈希函数：将 (k,v)(k, v)(k,v) 储存在 i=h(k)i = h(k)i=h(k) 索引的数组中。\n处理冲突。\n\n ii. 处理哈希冲突的方法\n a. 分离链(Separate Chaining)\n主要的思想是让相同 index 的元素以链表的形式连接起来。其中链表中每个节点还有一个单独的 key 值用于寻找具体的元素。\n链表中应该具有以下操作，假设存在 mmm 个冲突元素：\n\n\n\n操作\n函数\n分析和时间复杂度\n\n\n\n\n查询\nget(k): Element\n顺序查询：O(m)O(m)O(m)\n\n\n插入\nput(k, v): Element\n需要是否有相同相同的 key 元素：O(m)O(m)O(m)\n\n\n删除\nremove(k): Element\n顺序查询：O(m)O(m)O(m)\n\n\n\n对于基于分离连的哈希表，其实现下来则是：\n\n\n\n名称\n函数\n说明\n\n\n\n\n获取\nget(Key): Value\nreturn A[h(k)].get(k)\n\n\n插入\nput(Key, Value): Value\nreturn A[h(k)].put(k)注意如果新添加要让 size++\n\n\n删除\nremove(Key): Value\nreturn A[h(k)].remove(k)注意删除成功要让 size--\n\n\n\n对于每个操作，最佳访问时间是 O(1)，最差访问时间依然是 O(n)，即全部都有冲突。\n但是平均下来，其时间复杂度应该是 O(n/m)，其中 m 是哈希表数组的容量。一般用负载因子(load factor) α=n/N 来表示哈希表的性能。\n在合理的负载因子下，其各个操作的期望值基本上都是 O(1)O(1)O(1)。具体证明可自行查阅。\n b. 平衡二叉树\n与分离链类似，其主要的思想是让相同 index 的元素以平衡二叉树的形式连接起来。\n与直接使用一个平衡二叉树(TreeMap)模拟映射的比较请看下面排序映射。\n c. 开放地址(Open addressing)\n主要思想是让冲突的新元素放入到下一个可用的数组中。\n一些可能的方法：s\n c1. 线性探索(Linear probing)\n使用一个常数 ccc 来进行冲突元素的新元素寻址。即：\nhi(k)=(h(k)+ic) mod m, i=0,1,2,…,m−1h_i(k) = (h(k) + ic)\\ \\text{mod}\\ m,\\ i = 0, 1, 2, \\dots,m - 1\nhi​(k)=(h(k)+ic) mod m, i=0,1,2,…,m−1\n其中：\n\niii: 探查次数(偏移量)\nhi(k)h_i(k)hi​(k): 第 iii 个具有相同 h(k)h(k)h(k) 的条目。\nmmm: 哈希表的大小\n\n其主要性质：\n\n可能会导致未来新元素使用更长的时间来寻址。\n如果数组满了可能会导致死循环，因此要规定最多循环次数。\n\n操作主要实现方法：\n\n\n\n名称\n函数\n说明\n\n\n\n\n获取\nget(Key): Value\n1. 计算 index = h(k)2. 若该位置为空则查询失败3. 若该位置的key 与 k 相同，则查询成功4. 否则计算 index = (index + 1) % m 返回步骤 222\n\n\n插入\nput(Key, Value): Value\n1. 计算 index = h(k)2. 若该位置为空，则直接插入3. 若冲突，则计算 index = (index + 1) % m4. 若达到最大次数，则抛出异常。否则返回步骤 222\n\n\n删除\nremove(Key): Value\n如果直接删除，会导致后面冲突的数组查询失败。可能的方法：1. 不断往右找到最后一个与该位置哈希值相同的值，代替到该位置2. Lazy deletion 延迟删除: 将该位置标记为 已删除。查询到该位置则跳过，插入到该位置则直接替换。\n\n\n\n\n成功查找的平均探查次数约为：\n\n12(1+11−α)\\frac{1}{2}(1 + \\frac{1}{1 - \\alpha})\n21​(1+1−α1​)\n\n失败查找的平均探查次数约为：\n\n12(1+1(1−α)2)\\frac{1}{2}(1 + \\frac{1}{(1 - \\alpha)^2})\n21​(1+(1−α)21​)\n\n其中 α=nm\\alpha = \\frac{n}{m}α=mn​ 为装载因子 (load factor)。\n\nnnn 是要插入的个数，mmm 是数组大小。\n\n\n\n当 α\\alphaα 接近 111 时性能相当于线性扫描，性能非常低下。通常限制 α≤0.7\\alpha \\le 0.7α≤0.7。\n c2. 双哈希(Double Hashing)\n使用一个额外的哈希函数 d(k)d(k)d(k) 来辅助寻找新元素，计算新的哈希值：\nhdj=(h(k)+jd(k)) mod N,j∈[0,N−1]h_{dj} = (h(k) + jd(k))\\ \\text{mod}\\ N, j \\in [0, N - 1]\nhdj​=(h(k)+jd(k)) mod N,j∈[0,N−1]\n\n选择 index=hdj\\text{index} = h_{dj}index=hdj​ 中第一个空元素作为新的哈希值\nNNN 必须是质数，以探索所有的可能数组包。\n\n一些可能的 d(k)d(k)d(k):\n\nd(k)=q−(k mod q)d(k) = q - (k\\ \\text{mod}\\ q)d(k)=q−(k mod q), q&lt;Nq &lt; Nq&lt;N 且 qqq 为质数\n\n对于线性探索(Linear probing)来说，d(k)=cd(k) = cd(k)=c 始终是一个常数。\n 三. 有序映射 ADT\n有序映射(Ordered Map)指的是键会按照全序关系进行排列，使得支持最近邻查询：\n\n键值小于等于 kkk 的最大项\n键值大于等于 kkk 的最小项\n键值处于某个范围的所有项\n\n 四. 排序映射 CDT\n 1. 基于已排序数组的映射 (SortedArrayMap)\n// todo\n 2. 直接基于二叉平衡树的映射 (TreeMap)\n// todo\n与使用平衡二叉树的哈希表比较：\n\n\n\n项目\nHashMap - 基于树处理冲突\nTreeMap\n\n\n\n\n建树\n使用平衡二叉树森林来处理哈希冲突\n全局使用一个平衡二叉树\n\n\n时间复杂度\n平均 O(1)O(1)O(1)最坏的情况下为 O(log⁡(n))O(\\log(n))O(log(n))\n始终为 O(log⁡(n))O(\\log(n))O(log(n))\n\n\n抗恶意/退化\n最坏会退化到 O(log⁡(n))O(\\log(n))O(log(n))\n不受散列影响，始终为 O(log⁡(n))O(\\log(n))O(log(n))\n\n\n顺序 &amp; 迭代\n是无排序的\n是有序的，支持比较器升序迭代\n\n\n范围操作\n不擅长如果需要查询一个key范围内的需要额外索引\n支持 first, last, floor, ceiling, subMap, headMap, tailMap 操作时间复杂度都是O(log⁡(n))O(\\log(n))O(log(n))\n\n\n链要求\n需要高质量 hashCode/equals散列分布影响性能\n需要 Comparable/Comparator 的全序\n\n\n内存开销\n有数组（装载因子控制）+ 节点少数大冲突桶会有树节点（带额外指针）整体常更省对象数量，且缓存友好\n仅树节点，但每节点指针多、旋转维护缓存局部性差\n\n\n扩容成本\n存在再散列/搬迁（摊还执行）\n无扩容，插入时局部旋转\n\n\n小规模性能\n常更快（常数项小、命中好）\n常数项略大\n\n\n迭代稳定性\n不是排序意义上的稳定结构变化可能改变遍历次序\n确定的有序遍历\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/映射map","date":"2024-05-24T12:00:05.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Map,Hash Table","author_index":"zExNocs"},{"id":"7faf5045ebd000ff3f47b2cb6dc02586","title":"数据结构-优先队列和堆","content":"&lt;返回算法与数据结构导航\n\n 一. 优先队列 ADT\n优先队列是一个抽象数据结构(ADT)。优先队列是储存一组具有 (key, value) 的数据结构，并能够有效地返回和操作其中具有最小/最大 key 的元素。\n一般我们默认优先队列是最小优先队列(Min-Priority Queue)，也就是返回/操作拥有最小key的元素。\n其主要的操作包括：\n\n\n\n操作\n函数\n介绍\n\n\n\n\n插入\ninsert(k, v)\n插入一组(k, v)元素\n\n\n删除\npop(): Element\n删除并返回优先级最高的元素\n\n\n查询\ntop(): Element\n返回优先级最高的元素\n\n\n状态\nsize(): int\n返回元素个数\n\n\n状态\nisEmpty(): boolean\n返回队列是否为空\n\n\n\n 二. 优先队列的 CDT\n 1. 基于二叉堆(Binary Heap)的优先队列\n本笔记中默认的二叉堆是 小根堆。\n\n\n\n\n\n\n\n\n\nA binary heap is a complete binary tree storing key-value pairs at its nodes.\n二叉堆是将 (key, value) 对储存在节点的完全二叉树。\n i. 二叉堆的性质\n\nHeap-Order：对于每一个除了根以外的节点，都有 key(v) &gt;= key(parent(v))。\n\n即子节点的值不会比父节点更小。\n堆顶，即二叉堆的根节点是所有节点中的最小值。\n\n\nComplete Binary Tree：是一个完全二叉树。因此如果一共有 nnn 个节点，那么树高为 h=log⁡(n)h = \\log(n)h=log(n)。\n\n ii. 二叉堆的插入实现\n步骤如下：\n\n根据完全二叉树性质（最深处从左到右）插入一个新的叶节点 ZZZ。\n将新的叶节点 ZZZ 储存新的数据对 (key, value)。\n恢复堆(unheap)操作：将插入点 ZZZ 从下到上进行 冒泡。在小根堆中：\n\n如果父节点的 key 值比 ZZZ 大，那么就交换两个节点的位置或元素。\n依次反复直到父节点的 key 值比 ZZZ 小或者 ZZZ 达到根节点。\n\n\n\n注意：\n\n操作 111 中，如果使用类或者结构体的方式实现二叉树，那么寻找位置的时间复杂度可能会达到 O(n)O(n)O(n)。\n如果使用树形数组 (二.3.i.)的方式实现，那么只需要在数组的末尾插入新的节点即可。此时时间复杂度为 O(1)O(1)O(1)。\n操作 333 中，因为完全二叉树的高度为 h=log⁡(n)h = \\log(n)h=log(n)，因此 unheap 操作的时间复杂度是 O(log⁡(n))O(\\log(n))O(log(n))。\n\n iii. 二叉堆的删除实现\n步骤如下：\n\n使用最后一个叶节点 w 代替根节点。\n删除原根节点\n恢复堆属性(Downheap操作)：将新根点 w 从上往下 冒泡。在小根堆中：\n\n选择 w 两个子节点中 最小 的子节点。\n如果该子节点的 key 值比 w 节点小，那么就交换两个节点的位置或元素。\n重复操作直到所有的子节点的 key 值比 w 大或者以及达到叶节点。\n\n\n\n 2. 其他优先队列\n除了二叉堆以外，还有二项式堆(Binomial Heap)和斐波那契堆(Fibonacci Heap)。\n以后如果有机会再补充。\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/优先队列和堆","date":"2024-05-24T12:00:04.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Priority Queue,Heap","author_index":"zExNocs"},{"id":"bc36d13ad9232edecc9b42fb7ab0884b","title":"数据结构-基础树结构","content":"&lt;返回算法与数据结构导航\n\n 一. 树的 ADT 和定义\n\n\n\n\n\n\n\n\n\nIn computer science, a tree is an abstract model of a hierarchical structure. A tree consists of nodes with a parent-child relation.\n在计算机科学中，树是层次结构的抽象模型。树由具有父子关系的节点组成。\n树是由n（n&gt;0）个有限节点组成一个具有层次关系的集合。\n 1. 树的特点\n\n每个节点都只有有限个子节点或无子节点\n没有父节点的节点称为根节点\n每一个非根节点有且只有一个父节点\n除了根节点外，每个子节点可以分为多个不相交的子树\n树里面没有环路(cycle)\n\n 2. 树的术语\n\n\n\n术语\n种类和名称\n介绍\n\n\n\n\n树的节点\nNode node\n树的基本单位。树是由多个节点的组成的每个节点包含元素、父节点和子节点\n\n\n父节点\nNode parent\n若一个节点含有子节点，则这个节点称为其子节点的父节点\n\n\n子节点\nNode[] children\n一个节点含有的子树的根节点称为该节点的子节点\n\n\n兄弟节点\nNode[]\n具有相同父节点的节点互称为兄弟节点；\n\n\n节点的度\ndegree: Node → int\n该节点的子树总个数也可以说子节点的总个数\n\n\n树的度\ndegree: Tree → int\n最大的节点度称为树的度\n\n\n根节点\nNode root\n树中不具有父节点的节点一棵树只有一个根节点\n\n\n叶节点\nNode leafNode external\n不具有子节点的节点也可以说度为000 的节点一棵树可以由多个叶节点二叉树的叶节点可以为空节点 null\n\n\n内节点\nNode internal\n具有至少一个子节点的节点也可以说度不为 000 的节点\n\n\n祖先节点\nancestors: Node → Node[]\n递归定义：1. 该节点的父节点 2. 该节点的父节点的祖先节点这些节点的集合是该节点的祖先节点\n\n\n祖孙节点\ndescendant: Node → Node[]\n递归定义：1. 该节点的子节点 2. 该节点的子节点的祖孙节点这些节点的集合是该节点的祖孙节点\n\n\n节点的深度\ndepth: Node → int\n该节点的祖先节点的个数根节点的深度为 000\n\n\n节点的高度\nheight: Node → int\n该节点到达树叶的最长路径长度叶节点的高度为 000\n\n\n树的高度\nheight: Tree → int\n最大的叶节点深度或者说从根节点到叶节点最长的路径(不包括根节点)\n\n\n\n 3. 树的种类\n\n\n\n种类名\n介绍\n\n\n\n\n有序/无序\n无序树/自由树：树中任意节点的子节点之间没有顺序关系有序树/搜索树/查找树：树中任意节点的子节点之间有顺序关系\n\n\n平衡/不平衡\n绝对平衡树: 任意节点的子树之间两两高度差绝对值不超过 111平衡树: 所有叶节点在同一层不平衡树：所有叶节点不一定在同一层\n\n\n分叉情况\n1. 等差树：每个内节点的子节点个数都相同2. 不等叉树：每个节点的键值个数不一定相同、子节点个数也不一定相同\n\n\n\n 4. 树的 ADT 组成成分\n\n树的子类主要包含节点 Node，其构成为：\n\n\n\n\n成员名称\n成员类型\n\n\n\n\n元素\nElement e\n\n\n父节点\nNode parent\n\n\n子节点\nNode[] children\n\n\n\n\n树的 ADT 成员主要包含：\n\n\n\n\n成员名称\n成员类型\n\n\n\n\n根节点\nNode root\n\n\n树的大小\nint size\n\n\n\n\n树的操作：\n\n\n\n\n类型\n函数\n介绍\n\n\n\n\n根节点\nroot(): Node\n返回当前树的根节点\n\n\n父节点\nparent(Node): Node\n返回该树种某个节点的父节点\n\n\n子节点\nchildren(Node): Node[]\n返回节点的子节点或者迭代器\n\n\n查询\nisInternal(Node): boolean\n返回节点是否是内节点\n\n\n查询\nisExternal(Node): boolean\n返回节点是否是叶节点\n\n\n树状态\nsize(): int\n返回树的节点个数\n\n\n树状态\nisEmpty(): boolean\n返回树是否为空\n\n\n\n 5. 树的遍历(Traversal)方法\n\n\n\n遍历类型\n介绍\n伪代码\n\n\n\n\n前序遍历Preorder\n先访问父节点，再从左到右遍历其子节点。\n1. Algorithm preOrder(v)2. ···visit(v)3. ···for each child w of v4. ······preorder(w)\n\n\n后序遍历Postorder\n先遍历子节点，再访问父节点\n1. Algorithm postOrder(v)2. ···for each child w of v3. ······postorder(w)4. ···visit(v)\n\n\n中序遍历Inorder\n属于二叉树的专属。先遍历左节点，再访问父节点，再遍历右节点\n1. Algorithm inOrder(v)2. ···inOrder(v.left)3. ···visit(v)4. ···inOrder(v.right)\n\n\n\n 二. 二叉树\n二叉树是一种抽象数据结构(ADT)。\n\n一般定义：\n\n\n\n\n\n\n\n\n\n\na tree whose each internal node has at most two children, and the children of a node are an ordered pair, though one might be “missing”.\n一棵每个内部节点最多有两个子节点，并且节点的子节点是有序对的树。\n\n递归定义：\n\n\n\n\n\n\n\n\n\n\nA tree consisting of a single node, or a tree whose root has an ordered pair of “children”, each of which is missing (a null) or is the root of a binary tree.\n由单个节点组成的树，或者其根具有一对有序“子节点”的树，每个子节点都缺失（为空）或为二叉树子树的根。\n\n特点：\n\n\n每个节点最多有两个子节点\n子节点之间是有序的，即左子节点和右子节点，可以为空节点。\n\n 1. 二叉树的大致种类\n\n\n\n种类\n介绍\n\n\n\n\n完满二叉树Full Binary Tree\n如果二叉树的所有内部节点都具有两个子节点，那么称这个二叉树是完满二叉树。\n\n\n完美二叉树Perfect Binary Tree\n如果一个满二叉树中所有的子节点都在同一个深度，那么称这个二叉树是完美二叉树。\n\n\n完全二叉树Complete Binary Tree\n除了叶节点所处的深度以外，其他深度是一个完美二叉树，并且叶节点是靠右排序的二叉树是完全二叉树。\n\n\n\n 2. 二叉树的效率分析\n求树的高度：如果树的大小为 nnn，那么：\n对于完美二叉树来说，时间复杂度是 Θ(log(n))Θ(log(n))Θ(log(n))。\n对于非完美二叉树来说，考虑到一条链，时间复杂度是 Ω(log(n))Ω(log(n))Ω(log(n)) 和 O(n)O(n)O(n)。\n 3. 二叉树的简单 CDT\n i. 基于数组的二叉树 (Array-Based Representation of Binary Tree)\n它也可以被称为 树形数组(tree-as-array)。它是一种使用数组作为CDT实现二叉树ADT的方式。\n一般使用 rank(Node): int 来表示节点的数组索引。\n\n具有以下性质：\n\n\n\n\n性质\n解释\n\n\n\n\nrank(root) = 1\n根节点的索引是 111\n\n\nrank(parent(node)) = rank(node) &gt;&gt; 1\n每个节点的父节点是该节点的索引整除以 222\n\n\nrank(left_child(node)) = rank(node) &lt;&lt; 1\n每个节点的左节点是该节点的索引 ×2\\times 2×2\n\n\nrank(right_child(node)) = rank(node) &lt;&lt; 1 + 1\n每个节点的右节点是该节点的索引 ×2+1\\times 2 + 1×2+1\n\n\n\n\n优点：\n\n\n\n\n优点\n\n\n\n\n能够节省空间。因为不用储存相关的指针，而是使用计算代替\n\n\n储存能够更紧凑，具有更好的内存局部性&quot;better memory locality&quot;\n\n\n很好地解决缓存和内存层次结构的问题——当访问数组元素时，其他条目可以被拉入缓存，因此访问速度更快\n\n\n\n ii. 平衡二叉树\n具体请看这一篇笔记：二叉搜索树\n 三. 树结构的导航\n\n\n\n笔记\n树类型\n\n\n\n\n二叉搜索树\n二叉树/平衡二叉树\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/基础树结构","date":"2024-05-24T12:00:03.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Tree,Binary Tree","author_index":"zExNocs"},{"id":"64d157aa708cf8cbeb55e3947e3ed7d6","title":"数据结构-向量 Vector","content":"&lt;返回算法与数据结构导航\n\n 一. 向量的抽象 ADT 和介绍\n\n\n\n\n\n\n\n\n\n向量的主要目的是创建一个比数组(Array)更泛用的模型。\n主要特征:\n\n与数组固定大小不同，向量一个自动调节大小的数据结构。\n一个元素在向量中的索引(index)被认为是前面元素的个数(number of elements prceding it)。\n\n为了不完全依赖于数组，因此我们不使用“索引(index)”概念，而使用“前面元素的个数”概念。\n例如对于一个向量 AAA 来说，A[2]A[2]A[2] 表示有 222 个元素在它的前面，分别是 A[0]A[0]A[0], A[1]A[1]A[1]。\n这个概念也可以称为 排名(rank)\n\n\n\n向量的操作通常包含：\n\n\n\n操作类型\n函数\n说明\n要求\n\n\n\n\n查询\nelemAtRank(int rank): Element\n返回位置为 rank 的元素\nO(1)O(1)O(1)随机读入\n\n\n替换\nreplaceAtRank(int rank, Element e): Element\n替换掉 rank 的元素为 e 并返回原来的元素\nO(1)O(1)O(1)\n\n\n插入\ninsertAtRank(int rank, Element e)\n在 rank 位置插入新的元素 e\nO(n)O(n)O(n)\n\n\n删除\nremoveAtRank(int rank): Element\n删除并返回 rank 位置的元素\nO(n)O(n)O(n)\n\n\n状态\nsize(): int\n返回向量大小\nO(1)O(1)O(1)\n\n\n状态\nisEmpty(): boolean\n返回向量是否为空\nO(1)O(1)O(1)\n\n\n\n 二. 向量的 CDT\n 1. 基于数组的向量 (Array-based Vector)\n是使用一个大小为 NNN 的数组 VVV 作为向量的CDT，并使用整型变量 nnn 记录向量的大小。\n\n成员变量包含：\n\n\n\n\n成员类型\n成员名称\n作用\n\n\n\n\nArray&lt;Element&gt; V\n数组成员\n用于储存数据\n\n\nint max_size\n最大大小\n记录数组的最大大小 NNN\n\n\nint size\n大小\n记录向量的大小 nnn\n\n\n\n\n操作和实现说明以及时间复杂度：\n\n\n\n\n操作类型\n函数\n实现\n时间复杂度\n\n\n\n\n查询\nelemAtRank(int rank): Element\n直接返回 V[rank]\nO(1)O(1)O(1)\n\n\n替换\nreplaceAtRank(int rank, Element e): Element\n直接 V[rank] = e\nO(1)O(1)O(1)\n\n\n插入\ninsertAtRank(int rank, Element e)\n需要对后续的元素进行右平移\nO(n)O(n)O(n)\n\n\n删除\nremoveAtRank(int rank): Element\n需要对原来的元素进行左平移\nO(n)O(n)O(n)\n\n\n状态\nsize(): int\n直接返回 size\nO(1)O(1)O(1)\n\n\n状态\nisEmpty(): boolean\n判断 size == 0\nO(1)O(1)O(1)\n\n\n添加\npush(Element e)\nV[size] = esize = size + 1\n不需要扩大: O(1)O(1)O(1)需要扩大: 看下面分析\n\n\n删除\npop()\nsize = size - 1\nO(1)O(1)O(1)\n\n\n\n i. 扩大数组(Resize Array)和时间复杂度分析\n在 insertAtRank(r, o) 和 push(o) 操作中，如果数组已经满了，那么需要替换数组为更大的数组。此时的时间复杂度平均下来需要计算其平摊时间。\n替换数组需要复制原来的数据到新的数据中。假设当前数组的大小为 mmm，每次替换所使用的时间为 s2s_2s2​，那么这个过程需要的时间 T(m)=s2nT(m)=s_2nT(m)=s2​n，即这个过程的时间复杂度是 O(m)O(m)O(m)。\n假设执行 push 次数为 nnn，数组扩大的次数为 kkk，总运行时间为 T(n)T(n)T(n)，push一次的时间为 s1s_1s1​，替换一次的时间为 s2s_2s2​。可以总结出两个扩大数组的方法和其时间复杂度：\n\n\n\n过程\n增量策略 Incremental Strategy\n翻倍策略 Doubling Strategy\n\n\n\n\n基本思想\n使用固定的常数 ccc 来进行扩大数组\n每次扩展都双倍数组的大小\n\n\n替换次数 kkk\nk=floor(nc)k = \\text{floor}(\\frac{n}{c})k=floor(cn​)\nk=floor(log⁡(n))k = \\text{floor}(\\log(n))k=floor(log(n))\n\n\n执行总运行时间T(n)T(n)T(n)\nT(n)=s1n+s2(c+2c+⋯+kc)=s1n+s2c⋅k(k+1)2=s22cn2+(s1+s22)n\\begin{aligned}T(n) &amp;= s_1 n + s_2 (c + 2c + \\cdots + kc) \\\\&amp;= s_1 n + s_2 c \\cdot \\frac{k(k+1)}{2}\\\\&amp;= \\frac{s_2}{2c} n^2 + \\left(s_1 + \\frac{s_2}{2}\\right)n\\end{aligned}T(n)​=s1​n+s2​(c+2c+⋯+kc)=s1​n+s2​c⋅2k(k+1)​=2cs2​​n2+(s1​+2s2​​)n​\nT(n)=s1n+s2(1+2+4+⋯+2k−1)=(s1+s2)n−s2\\begin{aligned}T(n) &amp;= s_1 n + s_2 (1 + 2 + 4 + \\cdots + 2^{k-1}) \\\\&amp;= (s_1 + s_2)n - s_2\\end{aligned}T(n)​=s1​n+s2​(1+2+4+⋯+2k−1)=(s1​+s2​)n−s2​​\n\n\nT(n)T(n)T(n) 的 Big-Oh\n由上述可知T(n)T(n)T(n) 是 O(n2)O(n^2)O(n2)\n由上述可知 T(n)T(n)T(n) 是 O(n)O(n)O(n)\n\n\n平摊后复杂度\nO(T(n)n)=O(n)O(\\frac{T(n)}{n}) = O(n)O(nT(n)​)=O(n)\nO(T(n)n)=O(1)O(\\frac{T(n)}{n}) = O(1)O(nT(n)​)=O(1)\n\n\n\n 三. 向量的作用\n 1. 使用向量模拟栈 Stack\n栈(Stack)是一个先入后出(first in last out, FILO)的数据结构，其操作主要是：\n\n\n\n操作类型\n函数\n说明\n要求\n\n\n\n\n栈顶\nElement top()\n相当于elemAtRank(size())\nO(1)O(1)O(1)\n\n\n入栈\nvoid push(Element e)\n相当于insertAtRank(size(), Element e)\nO(1)O(1)O(1)\n\n\n出栈\nElement pop()\n相当于removeAtRank(size() - 1)\nO(1)O(1)O(1)\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/向量","date":"2024-05-24T12:00:02.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Vector","author_index":"zExNocs"},{"id":"d7cbad849ec2c47f86c0a9c7d6e40eff","title":"数据结构-链表","content":"&lt;返回算法与数据结构导航\n\n 一. 链表的抽象 ADT 和性质\n\n\n\n\n\n\n\n\n\n链表是一种线性数据结构，由一系列节点组成，每个节点包含数据和指向下一个节点的指针。\n其操作通常包含：\n\n\n\n操作类型\n函数\n要求\n\n\n\n\n头部添加\npushHhead(Element):Element\nO(1)O(1)O(1)\n\n\n尾部添加\npushBack(Element):Element\n\n\n\n插入前面\ninsertBefore(Node, node, Element):Element\n\n\n\n插入后面\ninsertAfter(Node node, Element):Element\nO(1)O(1)O(1)\n\n\n位置插入\ninsert(int index, Element):Element\n\n\n\n删除头部\nremoveHead():Element\nO(1)O(1)O(1)\n\n\n删除尾部\nremoveTail():Element\n\n\n\n删除节点\nremove(Node):Element\n\n\n\n位置删除\nremove(int index):Elementremove(Element):Element\n\n\n\n查找\nfind(int index):Elementcontains(Element):boolean\n\n\n\n\n 二. 链表的实现 CDT\n 1. 单向链表(Singly Linked List)\n\n\n\n\n\n\n\n\n\nA singly linked list is a concrete data structure consisting of a sequence of nodes. Each node stores an element and a pointer/reference to the next node.\n单链表是一个由一系列节点组成的具体数据结构。每个节点存储一个元素以及指向下一个节点的指针/引用。\n单向链表的节点 Node 的成员：\n\n\n\n成员类型\n成员名称\n作用\n\n\n\n\nElement\n元素\n储存的数据\n\n\nNode* next\n下一个节点\n指向下一个节点的指针\n\n\n\n单向链表 Linked List 的成员：\n\n\n\n成员类型\n通常名称\n作用\n\n\n\n\nNode* head\n链表头\n链表的头节点，用于查找的开始\n\n\nNode* tail\n(可选)链表尾\n用于更快速地尾部插入\n\n\n\n单向链表的函数/方法/操作：\n\n\n\n操作类型\n函数\n时间复杂度\n说明\n\n\n\n\n头部添加\npush_head(Element):Element\nO(1)O(1)O(1)\nelement.setNext(head);head = element\n\n\n尾部添加\npush_back(Element):Element\n记录尾部: O(1)O(1)O(1)未记录: O(n)O(n)O(n)\n未记录尾部需要遍历一遍链表注意修改尾部指针tail\n\n\n插入前面\ninsertBefore(Node, node, Element):Element\nO(n)O(n)O(n)\n要遍历查找 pre\n\n\n插入后面\ninsertAfter(Node node, Element):Element\nO(1)O(1)O(1)\n注意尾部插入时修改尾部指针\n\n\n位置插入\ninsert(int index, Element):Element\n寻找：O(n)O(n)O(n)插入：O(1)O(1)O(1)\n需要遍历查找元素实现需要记录一个pre注意头部和尾部的修改\n\n\n删除头部\nremoveHead():Element\nO(1)O(1)O(1)\nhead = head.next前提不为空\n\n\n删除尾部\nremoveTail():Element\n无论记录与否都是 O(n)O(n)O(n)\n因为需要查找 tail 的上一个节点。注意修改尾部记录\n\n\n删除节点\nremove(Node):Element\n寻找：O(n)O(n)O(n)删除：O(1)O(1)O(1)\n需要查找该节点的 pre注意头部和尾部的删除\n\n\n位置删除\nremove(int index):Elementremove(Element):Element\n寻找：O(n)O(n)O(n)删除：O(1)O(1)O(1)\n需要遍历查找该元素/位置实现需要记录pre注意头部和尾部的删除\n\n\n查找\nfind(int index):Elementcontains(Element):boolean\nO(n)O(n)O(n)\n顺序查找\n\n\n元素交换\nswapElement(Node, Node)\nO(1)O(1)O(1)\n常用\n\n\n节点交换\nswapNode(Node, Node)\nO(n)O(n)O(n)\n不常用因为要找这两个的点的 pre\n\n\n\n 2. 双向链表 (Doubly Linked List)\n\n\n\n\n\n\n\n\n\nA doubly linked list provides a natural extension of a singly linked list.Each node stores an element and a pointer/reference to the next node and a pointer/reference to the previous node.\n双向链表是单向链表的扩展。每个节点存储一个元素、一个指向下一个节点的指针/引用以及一个指向上一个节点的指针/引用。\n双向链表的节点 Node 的成员：\n\n\n\n成员类型\n成员名称\n作用\n\n\n\n\nElement\n元素\n储存的数据\n\n\nNode* next\n下一个节点\n指向下一个节点的指针\n\n\nNode* pre\n上一个节点\n指向上一个节点的指针\n\n\n\n双向链表 Linked List 的成员：\n\n\n\n成员类型\n通常名称\n作用\n\n\n\n\nNode* head\n链表头\n链表的头节点，用于查找的开始\n\n\nNode* tail\n(可选)链表尾\n用于更快速地尾部插入\n\n\n\n双向链表的函数/方法/操作：\n\n\n\n操作类型\n函数\n时间复杂度\n说明\n\n\n\n\n头部添加\npush_head(Element):Element\nO(1)O(1)O(1)\nelement.setNext(head);head.setPre(element)head = element\n\n\n尾部添加\npush_back(Element):Element\n记录尾部: O(1)O(1)O(1)未记录: O(n)O(n)O(n)\n未记录尾部需要遍历一遍链表注意修改尾部指针tail\n\n\n插入前面\ninsertBefore(Node, node, Element):Element\nO(1)O(1)O(1)\n注意插入头部时修改头部指针\n\n\n插入后面\ninsertAfter(Node node, Element):Element\nO(1)O(1)O(1)\n注意尾部插入时修改尾部指针\n\n\n位置插入\ninsert(int index, Element):Element\n寻找：O(n)O(n)O(n)插入：O(1)O(1)O(1)\n需要遍历查找元素注意头部和尾部的修改\n\n\n删除头部\nremoveHead():Element\nO(1)O(1)O(1)\nhead = head.nexthead.pre = null前提不为空\n\n\n删除尾部\nremoveTail():Element\n记录尾部: O(1)O(1)O(1)未记录: O(n)O(n)O(n)\n与 单向链表 不同\n\n\n节点删除\nremove(Node):Element\nO(1)O(1)O(1)\n与 单向链表 不同\n\n\n位置删除\nremove(int index):Elementremove(Element):Element\n寻找：O(n)O(n)O(n)删除：O(1)O(1)O(1)\n需要遍历查找该元素/位置注意头部和尾部的删除\n\n\n查找\nfind(int index):Elementcontains(Element):boolean\nO(n)O(n)O(n)\n顺序查找\n\n\n元素交换\nswapElement(Node, Node)\nO(1)O(1)O(1)\n不常用因为可能会有节点的绑定\n\n\n节点交换\nswapNode(Node, Node)\nO(1)O(1)O(1)\n常用与单向链表不同\n\n\n\n此外，有两种双向链表的设计方向：\n\n让head和tail指向实实在在的节点。如果链表为空，则head = NULL, tail = NULL。\n分配head和tail为新的节点，节点的元素为空。如果链表为空，则head.next == tail。\n\n 3. 循环链表\n头与尾相连的链表，可以随机选择一个节点进行查找。这里不再赘述。\n 三. 链表的比较\n 1. 单向和双向链表的比较\n\n\n\n\n单向链表\n双向链表\n\n\n\n\n已知节点前插入\n必须找到前驱: O(n)O(n)O(n)\nO(1)O(1)O(1)\n\n\n删除已知节点\n比如找到前驱: O(n)O(n)O(n)\nO(1)O(1)O(1)\n\n\n遍历\n只能朝着一个方向遍历\n可以双向遍历\n\n\n交换\n通常使用元素交换\n通常使用节点交换\n\n\n内存开销\n稍小\n稍大\n\n\n维护成本\n简单\n复杂\n\n\n\n除此之外，需要查询到特定位置的操作，基本上都是 O(n)O(n)O(n)。因为链表是顺序读入的。\n 2. 与数组的比较\n比较于数组(Array)和向量(Vector)：\n\n\n\n\n链表\n数组\n\n\n\n\n查找\n无论是否已知index，链表的查找时间复杂度都为 O(n)O(n)O(n)，是顺序读入。\n已知index的情况下数组的查找时间复杂度是 O(1)O(1)O(1)，是随机读入。\n\n\n插入/删除\n抛开查找，链表的插入/删除的时间复杂度可以达到 O(1)O(1)O(1)\n因为需要位移，所以数组的时间复杂度是 O(n)O(n)O(n)\n\n\n\n链表数据结构通常具有较快的插入和删除能力，但是具有较差的查询能力。\n因为链表是顺序读入，所以链表 无法使用 二分查找法。\n 3. 其他的数据结构\n一般具有较快的插入、删除和查询能力的数据结构都比较复杂，例如：\n\n跳表 (Skip List)：其三个操作的时间复杂度都是 O(logn)O(logn)O(logn)。\n哈希表(Hash Table)：其三个操作的平均时间复杂度都是 O(1)O(1)O(1)，最差情况下时间复杂度是 O(n)O(n)O(n)。\n平衡树(Balanced Trees)：其三个操作的时间复杂度都是 O(logn)O(logn)O(logn)。\n\n红黑树(Red-Black Tree)\nAVL树(AVL Tree)\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/链表","date":"2024-05-24T12:00:01.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures,Linked List","author_index":"zExNocs"},{"id":"f89361051f476df18c2ace63f2bea23b","title":"数据结构基础理论和定义","content":"&lt;返回算法与数据结构导航\n\n 一. 面向对象编程\n\n\n\n\n\n\n\n\n\n面向对象程序设计（Object Oriented Programming，OOP）是一种计算机编程架构。 OOP的一条基本原则是计算机程序由单个能够起到子程序作用的单元或对象组合而成。\n面向对象编程(Object-oriented)的原因：\n\n区分规范(specification) 和 实施细节(implementation details)\n使用相同的ADT来探索不同的CDTs。\n无需更改ADT的代码来快速更改和提升CDTs。\n\n 二. 抽象数据类型ADT 和具体数据类型 CDT\n 1. 抽象数据类型 ADT\n\n\n\n\n\n\n\n\n\n抽象数据类型(Abstract Data Types, ADTs) 是数据结构的抽象，规定了储存什么数据、如何操作数据。\nADT是通过选择不同的CDT来实现。\n ADT 的组成成分\n i. 储存的数据对象类型。\n\n\n\n\n\n\n\n\n\nADT定义了储存什么样的数据。\n\n\n\n\n\n\n提示\n通常会使用模板(Template) 来让同一种抽象数据结构适配多个数据对象类型。但同时也会要求这些数据对象实现一些接口，例如可比较接口(Comparable)。\n\n\n ii. 对数据的操作。\n\n\n\n\n\n\n\n\n\nADT定义了数据结构的功能。通常会使用Big-Oh来限制操作效率的上限。\n\n\n\n\n\n\n提示\n通常会使用抽象函数来定义这些功能。\n\n\n最基础的数据操作包括：\n\n\n\n操作类型\n说明\n补充\n\n\n\n\n添加(add/push)、插入(insert)\n向结构中添加新元素\n\n\n\n查找(find)、访问(query)\n根据条件检索元素\n是否支持随机访问(random access)\n\n\n删除(delete/remove/pop)\n移除某个元素\n\n\n\n\n除此之外，其他常用的数据操作：\n\n\n\n操作类型\n说明\n\n\n\n\n构造(construct/create)\n创造或初始化数据结构\n\n\n销毁(deconstruct/destory/clear)\n释放数据结构占用的资源\n\n\n更新(update/set)\n修改现有的元素值\n\n\n判空(isEmpty)\n判断数据结构是否为空\n\n\n遍历(traverse)\n遍历器访问\n\n\n容量(size/length)\n当前元素数量\n\n\n\n iii. 与操作相关的错误条件。\n\n\n\n\n\n\n\n\n\nADT定义了可能导致操作失败的错误条件。\n\n\n\n\n\n\n提示\n通常使用抽象函数抛出什么样的异常来规定错误条件。\n\n\n例如越界访问错误、重复元素冲突错误。\n 2. 具体数据类型 CDT\n具体数据类型(Concrete Data Types, CDTs) 是数据结构的实际。\n\nCDT是数据隐藏的和封装的（面向对象）。\nCDT的选择影响运行时间和空间使用。\n\n 三. 遍历\n遍历(Traversals) 指访问(visit)一个数据结构的所有元素:\n\n每一个元素只访问一次。\n访问的顺序是系统的、有序的、有意义的。\n\n例如树的前序遍历、后序遍历。\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/数据结构/数据结构基础理论和定义","date":"2024-05-24T11:00:00.000Z","categories_index":"笔记-数据结构","tags_index":"Data Structures","author_index":"zExNocs"},{"id":"c9bba4489c24a7e8b85aa4cba4f4edb1","title":"算法和数据结构-Big-Oh 和 Master 定理","content":"&lt;返回算法与数据结构导航\n\n 一. 🔬算法效率的评估\n如何评估一个算法的效率？最直接的方式就是在程序输入后何时才能获得输出值。其中一种比较直观的方式是根据 程序的运行时间 来评估测量算法效率。\n在同一个程序中，程序的运行时间往往会随着输入大小(input size)而增加。即使固定输入大小，实际运行时间通常也会有所不同，这取决于输入的详细信息。例如在最短路算法中，即使是相同数量的点和边，不同的连接方法也会导致运行时间不同* (关于SPFA，它死了)* 。\n由于你的时间非常值钱，因此我们需要一些方法来对算法的效率进行客观地评估，这些方法主要可以分类为两种：实验统计和理论分析。\n 方法一：Experiment 实验统计\n\n\n\n\n\n\n\n\n\n\n实验统计是使用观察和控制变量的方法来对一种现象进行系统的测试和验证。\n具体步骤如下:\n\n写一个程序实施该算法\n使用不同的输入大小和输入信息运行程序\n记录实际运行时间\n绘制并使用统计学分析（如回归分析）\n\n在固定输入大小、不同输入信息中，统计中获得最佳运行时间、最差运行时间、平均运行时间，我们通常会关注 最差的情况，主要原因是平均时间往往很难以去分析 (例如为什么在判断一个公司工资待遇往往不是使用平均值，更多的是最低工资)。\n 缺点 &amp; 局限性\n\n必须用程序实现该算法，可能会很耗时。\n需要提供大量输入集或者选择合适的输入集来找到最差的情况，不然会导致结果的偏差。\n效率的评估受到硬件/软件/语言环境的影响。\n\n\n 方法二：Theory 理论分析\n\n\n\n\n\n\n\n\n\n理论分析是基于已有的知识和数据，运用逻辑和数学的方法来对一种现象进行解释和预测。\n跟实验统计一样，我通常指关注 最差的情况。\n 特点\n\n具有一定的抽象性。\n能够独立于硬件/软件/语言环境来评估算法的效率。\n能够考虑所有可能的输入。\n\n 缺点 &amp; 局限性\n\n实施过程可能会比较困难，需要一定的知识基础。\n在现实实施的时候可能有一些特殊情况导致与理论结果相差较大，使用理论解释这种情况可能会比较困难。\n\n 评估标准\n在实验统计中，我们往往使用 程序的运行时间 来作为评估的标准，但是在理论知识中我们无法使用这个来作为评估标准。因为程序的运行时间往往受到环境的影响，因此理论难以测量出运行时间，所以我们使用另一种方法来作为理论分析中使用的评估标准：原始运算数量。\n 原始运算的定义\n\n\n\n\n\n\n\n\n\n原始运算(primitive operations)是算法执行的基本运算。\n在真实的计算机中，实际的运算应该是逻辑门的操作，但是很明显这个是很琐碎的，与算法的运算相差太远。因此我们需要去定义哪些运算属于原始运算以便于记数。一般来说，我们会将 汇编代码、算数运算 视作一个原始运算。\n注意，原始运算的定义不是固定的，下面有更具体的说明。在这个笔记中，我们将这些操作视作一个原始运算：\n\n\n\n描述\n伪代码样例\n\n\n\n\n变量赋值\na ← 0\n\n\n数组索引\na[10]\n\n\n变量比较\na == 10\n\n\n算数运算\na + 1\n\n\n函数调用\nfunction()\n\n\n函数返回\nreturn 0\n\n\n\n注意：\n\n在本笔记中我们忽视了汇编中有关jump的指令，即本笔记中默认jump指令原始运算为0。\n数组索引需要用到袁术运算是因为它需要在内存中进行索引。\n函数调用属于原始运算是因为它需要在内存中进行索引。\n在CPU中，浮点运算(如除法)实际上是一个非常复杂的算法。但在汇编语言中，它只是一个指令，因此我们也将其视作一个原始运算。\n\n对于其他的运算，都可以拆分为这些原始运算：\n\n\n\n描述\n分析\n操作数\n伪代码样例\n\n\n\n\nfor循环，循环次数为nnn\n要经历1次初始赋值；n次判断；n次叠加，每次叠加是两个原始运算(加法和赋值)\n1+n+2n=3n+11+n+2n=3n+11+n+2n=3n+1\nfor i ← 1 to n do\n\n\nfor循环，循环次数为(n−1)(n-1)(n−1)\n要经历1次初始赋值；(n-1)次判断，每次判断是两个原始运算(减法和比较)；(n-1)次叠加，每次叠加是两个原始运算(加法和赋值)\n1+2(n−1)+2(n−1)=4n−31+2(n-1)+2(n-1)=4n-31+2(n−1)+2(n−1)=4n−3\nfor i ← 1 to (n-1) do\n\n\nwhile循环，循环次数为nnn\n每次循环只需要判断即可\nnnn\nwhile i &gt; n\n\n\nwhile循环，循环次数为(n−1)(n-1)(n−1)\n每次循环都需要进行判断和减法\n2(n−1)2(n-1)2(n−1)\nwhile i &gt; (n-1)\n\n\nif then判断，then内部原始运算数为k\n一个判断。由于我们是统计最差次数，因此我们需要将if里原始运算进行累加(默认触发)\n1+k1+k1+k\nif … then …\n\n\n\n\n一个计算原始运算算法的例子 arrayMax(A, n)\n一个返回数组最大值arrayMax(A, n)的伪代码：\n123456Algorithm arrayMax(A, n)    # A为数组，n为数组大小，即数组从0开始，(n-1)结束。  currentMax ← A[0]         # 原始运算数为2，分别是数组索引和赋值  for i ← 1 to (n - 1) do   # (n-1)次数的循环运算，原始运算数为(4n-3)    if A[i] &gt; currentMax    # 每次循环原始运算数为2，分别是数组索引和判断，共(n-1)次循环，原始运算数为2(n-1)      currentMax ← A[i]     # then内部运算。每次循环原始运算数为2，分别是数组索引和赋值，原始运算数为2(n-1)  return currentMax         # 原始运算为1，函数返回\n综上所述，这个算法的原始运算总数为 2+(4n−3)+2(n−1)+2(n−1)+1=8n−42 + (4n - 3) + 2(n - 1) + 2(n - 1) + 1 = 8n - 42+(4n−3)+2(n−1)+2(n−1)+1=8n−4\n\n\n原始运算的个数并不是固定的，例如在计算操作 c←A[i]c \\leftarrow A[i]c←A[i] 中，你也可以认为是444个原始运算：\n\n获取AAA数组的指针储存在寄存器中。\n获取iii储存在寄存器中。\n计算A+iA + iA+i作为A[i]A[i]A[i]的指针储存在寄存器中。\n复制变量cc的数值写在A+iA + iA+i指针的内存中。\n\n当然在这个笔记中，你也可以认为只有222个原始运算：\n\n根据 iii 索引获取 A[i]A[i]A[i] 数组位置 (数组索引)。\n将变量cc的数值赋值给 A[i]A[i]A[i] (变量赋值)。\n\n但是无论是44还是22，这个操作永远不可能会是2n2n，即它的增长率永远不可能会超过常数级(增长率的定义在下面)。\n\n\n\n\n\n\n\n\n\n原始运算的个数只与算法的效率有关，与正确性无关。\n 使用原始运算估算运行时间\n增长率(Growth Rate)指的是函数的因变量随着自变量的增加而增长的速率。\n\n\n\n\n\n\n\n\n\n对于算法来说，假设它的最差情况的运行时间为T(n)T(n)，那么T(n)T(n)的增长率是该算法的固有属性，是不受硬件/软件环境影响的。\n我们可以使用原始运算来估算运行时间，假设：\n\n原始运算的个数为P(n)P(n)P(n)。\n最快的原始运算所需要的时间为aaa，是一个常数。\n最慢的原始运算所需要的时间为bbb，是一个常数。\n\n可以得出：aP(n)≤T(n)≤bP(n)aP(n) \\leq T(n) \\leq bP(n)aP(n)≤T(n)≤bP(n)\n由于a和b都是常数，那么我们认为T(n)和P(n)具有相同的增长率。很明显T(n)和P(n)的导数肯定是不同的，因此增长率并不等同于导数。\n但是增长率有一种只可意会不可言传的感觉：这种增长率具体形式是什么样的？如何定义哪两个函数具有相同的增长率？那么就需要用到我们的Big-Oh表示法了。\n 二. 🏠Big-Oh表示法和其家族\n 基本知识\n我们需要一种函数分类(Classification of Functions)来通过缩放的行为将函数分组在一起，同一组的函数具有这样的相似性：\n\n删除不必要的细节。\n相对快速、简单。\n处理运行时可能会发生的“奇怪的”函数(例如分段函数)。\n在数学上拥有明确的定义。\n\n其中一种最佳的方法是使用Big-Oh表示法和其家族(Big-Oh notation and family)：\nOOO: Big-Oh\nΩ\\OmegaΩ: Big-Omega\nΘ\\ThetaΘ: Big-Theta\nooo: little-oh\nω\\omegaω: little-omega\n本笔记只集中于前四个的定义和Big-Oh的相关理论。\n\n 1. Big-Oh：O(n)\n 定义\n假设有两个正函数f(n)f(n)和g(n)g(n)，如果我们称 f(n) is O(g(n))f(n) \\ is \\ O(g(n))f(n) is O(g(n))，当且仅当\n\n\n\n\n\n\n\n\n\n∃c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣≤cg(n)\\exist c &gt; 0, \\exist n_0 &gt; 0, \\forall n \\geq n_0: |f(n)| \\textcolor{red}{\\leq} cg(n)∃c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣≤cg(n)\n注意：\n\n量词顺序是 ∃ ∃ ∀\\exist \\ \\exist \\ \\forall∃ ∃ ∀\nccc 和 n0n_0n0​ 必须是常数，不能随着nn变化。不然这是没有意义的。\n注意符号 &gt;,≥,≤&gt;,\\geq,\\leq&gt;,≥,≤ 的区分。是比较严格的，例如，n0n_0n0​ 不能等于 000, nnn 可以等于 n0n_0n0​。\n\n此外，Big-Oh 可以被定义为：\nlim sup⁡n→∞f(n)g(n)&lt;∞\\limsup_{n \\to \\infty} \\frac{f(n)}{g(n)} &lt; \\infty\nn→∞limsup​g(n)f(n)​&lt;∞\n\n\n\n\n\n\n\n\n\nBig-Oh只规定了 f(n)f(n)f(n) 的增长率的上限(upper bound on the growth rate of the function)。\n 特点\n\nBig-Oh不关注算法或者是“算法最差的运行时间”，而是只关注于函数。也就是说它并不是对算法进行分类，而是对函数进行分类。\n一般f(n)f(n)f(n)表示运行时间，nnn表示输入的个数，所以Big-Oh中描述的函数一般为 f:N+→R+f: \\mathbb{N^+} \\to \\mathbb{R^+}f:N+→R+ ，g(n)g(n)g(n)也类似。\nBig-Oh只规定了f(n)f(n)f(n)的增长率的上限，也就是说，当nnn足够大时，f(n)f(n)f(n)的增长速率不大于g(n)g(n)g(n)。\nBig-Oh中 g(n)g(n)g(n) 的选择并不是一定要选择“最佳”或者“最有用的”函数。例如，对于 f(n)=1f(n) = 1f(n)=1 可以是 O(1)O(1)O(1) ，但也可以是 O(n)O(n)O(n) 。因此g(n)g(n)g(n)的增长率越小越能反应出 f(x)f(x)f(x) 的增长率。\n\n 性质\nBig-Oh作为一个二元关系(binary relation)，拥有以下性质：\n\nBig-Oh具有自反性(Reflexive, e.g. xRxx R xxRx)，即 f(n)f(n)f(n) 是 O(f(n))O(f(n))O(f(n))。\nBig-Oh不具有对称性(Symmetric, e.g. xRy  ⟺  yRxxRy \\iff yRxxRy⟺yRx), 例如 f(n)=1f(n)=1f(n)=1 是 O(n)O(n)O(n), 但 f(n)=nf(n) = nf(n)=n 不是 O(1)O(1)O(1)\nBig-Oh具有传递性(Transitive, e.g. xRy &amp; yRz→xRzxRy\\ \\&amp;\\ yRz \\to xRzxRy &amp; yRz→xRz)。即如果 ∀n≥n1,f(n)≤c1g(n)\\forall n \\geq n_1, f(n) \\leq c_1g(n)∀n≥n1​,f(n)≤c1​g(n), 且 ∀n≥n2,g(n)≤c2h(n)\\forall n \\geq n_2, g(n) \\leq c_2h(n)∀n≥n2​,g(n)≤c2​h(n)，那么总有 ∀n≥n3,f(n)≤c1c2h(n),n3=max⁡(n1,n2)\\forall n \\geq n_3, f(n) \\leq c_1c_2h(n), n_3 = \\max(n_1, n_2)∀n≥n3​,f(n)≤c1​c2​h(n),n3​=max(n1​,n2​)\n\n综上所述，Big-Oh具有自反性和传递性，因此Big-Oh更像是⊂,∈,≤\\subset, \\in, \\leq⊂,∈,≤, 而不是===。因此有一种表示方法是将Big-Oh视作集合，使用n∈O(n)n \\in O(n)n∈O(n)。此时也有会 O(lower_order)⊂O(heigher_order)O(lower\\_ order) \\subset O(heigher\\_ order)O(lower_order)⊂O(heigher_order)。\n 推论 &amp; 方法\n\n推论1：存在三个函数 f(n),g(n),p(n)f(n), g(n), p(n)f(n),g(n),p(n) 和正数k,b∈N+k, b\\in \\mathbb{N^+}k,b∈N+\n，如果f(n)f(n)f(n)是O(g(n))O(g(n))O(g(n))，且f(n)=k p(n)+bf(n)=k\\ p(n) + bf(n)=k p(n)+b，那么有p(n)p(n)p(n)是O(g(n))O(g(n))O(g(n))。\n\n\n证明推论1\n假设有c0&gt;0,n0&gt;0c_0 &gt; 0, n_0 &gt; 0c0​&gt;0,n0​&gt;0，对于n1≥n0n_1 \\geq n_0n1​≥n0​，有：\nf(n1)≤c0 g(n1)f(n_1) \\leq c_0 \\ g(n_1)f(n1​)≤c0​ g(n1​)，那么有：\nk p(n1)+b≤c0 g(n1)k\\ p(n_1) + b \\leq c_0 \\ g(n_1)k p(n1​)+b≤c0​ g(n1​)，整理得：\np(n1)≤c0kg(n1)−bkp(n_1) \\leq \\frac{c_0}{k}g(n_1) - \\frac{b}{k}p(n1​)≤kc0​​g(n1​)−kb​\n当n足够大时候，假设此时n1≥n2n_1 \\geq n_2n1​≥n2​，有 cg(n1)≥2bcg(n_1) \\geq 2bcg(n1​)≥2b\n从而有 p(n1)≤c02kg(n1)p(n_1) \\leq \\frac{c_0}{2k}g(n_1)p(n1​)≤2kc0​​g(n1​)\n设 c1=c2k&gt;0c_1=\\frac{c}{2k}&gt;0c1​=2kc​&gt;0，我们得到：\np(n1)≤c1 g(n1)p(n_1) \\leq c_1\\ g(n_1)p(n1​)≤c1​ g(n1​)，即\n存在 c1c_1c1​，n2n_2n2​ 使得 ∀n&gt;n2,p(n)≤c1 g(n)\\forall n &gt; n_2, p(n) \\leq c_1\\ g(n)∀n&gt;n2​,p(n)≤c1​ g(n)\n因此p(n)p(n)p(n)是O(g(n))O(g(n))O(g(n))。\n\n\n\n推论2 （乘法）：如果f1(n)f_1(n)f1​(n) 是 O(g1(n))O(g_1(n))O(g1​(n))， f2(n)f_2(n)f2​(n) 是 O(g2(n))O(g_2(n))O(g2​(n))，那么 f1(n)f2(n)f_1(n)f_2(n)f1​(n)f2​(n) 是 O(g1(n)g2(n))O(g_1(n)g_2(n))O(g1​(n)g2​(n))。\n\n\n证明推论2\n∵ f1(n)f_1(n)f1​(n) 是 O(g1(n))O(g_1(n))O(g1​(n))\n∴ f1(n)≤c1g1(n) for all n≥n1f_1(n) \\leq c_1g_1(n)\\ \\text{for all}\\ n \\geq n_1f1​(n)≤c1​g1​(n) for all n≥n1​\n∵ f2(n)f_2(n)f2​(n) 是 O(g2(n))O(g_2(n))O(g2​(n))\n∴ f2(n)≤c2g2(n) for all n≥n2f_2(n) \\leq c_2g_2(n)\\ \\text{for all}\\ n \\geq n_2f2​(n)≤c2​g2​(n) for all n≥n2​\n∵ f1(n),f2(n),g1(n),g2(n)≥0f_1(n), f_2(n), g_1(n), g_2(n) \\geq 0f1​(n),f2​(n),g1​(n),g2​(n)≥0\n∴ f1(n)f2(n)≤c1c2g1(n)g2(n).∀n≥n0,n0=max⁡(n1,n2)f_1(n)f_2(n) \\leq c_1c_2g_1(n)g_2(n). \\forall n \\geq n_0, n_0 = \\max(n_1, n_2)f1​(n)f2​(n)≤c1​c2​g1​(n)g2​(n).∀n≥n0​,n0​=max(n1​,n2​)\n∴ f1(n)f2(n)f_1(n)f_2(n)f1​(n)f2​(n) 是 O(g1(n)g2(n))O(g_1(n)g_2(n))O(g1​(n)g2​(n))\n\n\n推论3 （加法）：如果 f(n)=1+h(n)f(n) = 1 + h(n)f(n)=1+h(n)，且当n→∞n \\to \\inftyn→∞ 时 h(n)→0h(n) \\to 0h(n)→0，那么 f(n)f(n)f(n) 是 O(1)O(1)O(1)。\n\n证明推论3\n∵ 当n→∞n \\to \\inftyn→∞ 时 h(n)→0h(n) \\to 0\nh(n)→0\n∴ ∃n0&gt;0, ∀n≥n0,h(n)≤1\\exist n_0 &gt; 0,\\ \\forall n \\geq n_0, h(n) \\leq 1∃n0​&gt;0, ∀n≥n0​,h(n)≤1\n∴ ∃n0&gt;0, ∀n≥n0,f(n)≤2\\exist n_0 &gt; 0,\\ \\forall n \\geq n_0, f(n) \\leq 2∃n0​&gt;0, ∀n≥n0​,f(n)≤2\n∴ f(n)f(n)f(n) 是 O(1)O(1)O(1) 取 c=2,n0=n1c = 2, n_0 = n_1c=2,n0​=n1​ 且 h(n1)≤1h(n_1) \\leq 1h(n1​)≤1\n\n\n一些常用的 h(n)h(n)h(n):\n\nn2/2nn^2/2^nn2/2n\nn2000/2n100n^{2000}/2^{\\frac{n}{100}}n2000/2100n​\n(log⁡(n))100/n0.1(\\log(n))^{100}/n^{0.1}(log(n))100/n0.1\n\n综合推论2和3，可知如果 f(n)=g(n)(1+h(n))f(n) = g(n)(1 + h(n))f(n)=g(n)(1+h(n))，且当n→∞n \\to \\inftyn→∞ 时 h(n)→0h(n) \\to 0h(n)→0，那么 f(n)f(n)f(n) 是 O(g(n))O(g(n))O(g(n))。\n因此我们总结了获取Big-Oh的通用方法 —— 删除规则(Drop Rules)：\n\n删除低阶(lower-order)项 (根据推论2, 3)。阶排名可以看下面常用表示表。\n删除常数(constant)项系数 (根据推论1,总能找到 kkk 使得系数变成 111)。\n\n 例子\n\n证明 arrayMax(A, n) 是 O(n)O(n)O(n) 的例子 (定义法)\n由上述计算原始运算次数的例子可知，arrayMax(A, n) 的原始运算次数为 f(n)=8n−4f(n) = 8n - 4f(n)=8n−4。\n设 g(n)=ng(n) = ng(n)=n，因此需要求证 ∃c&gt;0,∃n0&gt;0,∀n≥n0,f(n)≤cg(n)\\exists c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0, f(n) \\le c g(n)∃c&gt;0,∃n0​&gt;0,∀n≥n0​,f(n)≤cg(n)，整理可得：\n{n≤4c−8c&gt;8n≥4c−8c&lt;8−4≤0c=8\\begin{cases}\nn \\le \\frac{4}{c - 8} &amp; c &gt; 8 \\\\\nn \\ge \\frac{4}{c - 8} &amp; c &lt; 8 \\\\\n-4 \\le 0 &amp; c = 8\n\\end{cases}\n⎩⎪⎪⎨⎪⎪⎧​n≤c−84​n≥c−84​−4≤0​c&gt;8c&lt;8c=8​\n由于我们规定是 ∀n≥n0\\forall n \\ge n_0∀n≥n0​，因此我们只能取 n≥8n \\ge 8n≥8。\n当我们取 n=8n = 8n=8 时，很明显任意 n0&gt;0n_0 &gt; 0n0​&gt;0 都可以证明成立。此时我们可以取 n0=1n_0 = 1n0​=1。\n当我们取 n&gt;8n &gt; 8n&gt;8 时，很明显任意 n0&gt;4c−8n_0 &gt; \\frac{4}{c - 8}n0​&gt;c−84​ 都可以证明成立。此时我们可以取 n0=4c−8n_0 = \\frac{4}{c - 8}n0​=c−84​。\n实际上，上述情况只需要求出一组 (c,n0)(c, n_0)(c,n0​) 即可，因此我们可以直接取 c=8,n0=1c = 8, n_0 = 1c=8,n0​=1。不过这里给出了一种选取 (c,n0)(c, n_0)(c,n0​) 的具体方法。\n因此 arrayMax(A, n) 的时间复杂度是 O(n)O(n)O(n)。\n\n\n\n对于分段函数Big-Oh的证明 (定义法)\n如何计算下面函数的 Big-Oh：\nf(n)={nif n is even1if n is oddf(n) =\n\\begin{cases}\nn &amp; \\text{if } n \\text{ is even} \\\\\n1 &amp; \\text{if } n \\text{ is odd}\n\\end{cases}\nf(n)={n1​if n is evenif n is odd​\n因为 Big-Oh 是规定的函数增长率的上限，因此我们应该取增长率最大的函数，即 f(n)=nf(n) = nf(n)=n。\n此时当 c=1,n0=1c = 1, n_0 = 1c=1,n0​=1 可以证明出 f(n)f(n)f(n) 是 O(n)O(n)O(n)，而无法证明出 f(n)f(n)f(n) 是 O(1)O(1)O(1)。\n\n\n\n求 f(n)=n2+nf(n) = n^2 + nf(n)=n2+n 的 Big-Oh (定理2，3)\nf(n)=n2+n=n2(1+1n)f(n) = n^2 + n = n^2(1 + \\frac{1}{n})f(n)=n2+n=n2(1+n1​)\n因为自反性，n2n^2n2 是 O(n2)O(n^2)O(n2)。\n因为当 n→∞n \\rightarrow \\inftyn→∞ 时 1n→0\\frac{1}{n} \\rightarrow 0n1​→0，根据推理 3 可知 1+1n1 + \\frac{1}{n}1+n1​ 是 O(1)O(1)O(1)。\n因此根据推理 2，f(n)f(n)f(n) 是 O(n2∗1)=O(n2)O(n^2 * 1) = O(n^2)O(n2∗1)=O(n2)。\n\n\n\n求 f(n)=5n4+3n3f(n) = 5n^4 + 3n^3f(n)=5n4+3n3 的Big-Oh (删除规则)\n\n删除低阶 3n3^n3n，因此 f(n)f(n)f(n) 是 O(5n4)O(5n^4)O(5n4)\n删除常数 5，因此 f(n)f(n)f(n) 是 O(n4)O(n^4)O(n4)\n\n\n\n Big-Oh 公约\n\n使用最小且正确的增长率函数表示Big-Oh。例如说 2n2n2n 是 O(n)O(n)O(n) 而不是 O(n2)O(n^2)O(n2)，尽管后者也是正确的。\n使用最简的函数表示Big-Oh。例如说 2n2n2n 是 O(n)O(n)O(n) 而不是 O(2n)O(2n)O(2n)。\n\n遵循这个公约可以更好地去分析算法以及给出最大的信息。\n 其他\n对于 nO(1)n^{O(1)}nO(1) 来说，相当于是 {nf(n)∣f(n) is O(1)}\\{n^{f(n)} \\mid f(n) \\text{ is } O(1)\\}{nf(n)∣f(n) is O(1)}。\n也就是说 {n1,n2,n3,...}⊂nO(1)\\{n^1, n^2, n^3, ...\\} \\subset n^{O(1)}{n1,n2,n3,...}⊂nO(1)，{n12,n13,n14,...}⊂nO(1)\\{n^{\\frac{1}{2}}, n^{\\frac{1}{3}}, n^{\\frac{1}{4}}, ...\\} \\subset n^{O(1)}{n21​,n31​,n41​,...}⊂nO(1)。\n\n\n\n\n\n\n\n\n\nnO(1)n^{O(1)}nO(1) is any function that is no worse than (Big-Oh of) some power law.\nnO(1)n^{O(1)}nO(1) 表示任何不超过指数级的函数。\n 2. Big-Omega: Ω(n)\\Omega (n)Ω(n)\n 定义\n假设有两个正函数 f(n)f(n)f(n) 和 g(n)g(n)g(n)，如果我们称 f(n)f(n)f(n) is Ω(g(n))\\Omega(g(n))Ω(g(n))，当且仅当\n\n\n\n\n\n\n\n\n\n∃c&gt;0,∃n0&gt;0,∀n≥n0:f(n)≥cg(n)\\exists c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : f(n) \\textcolor{red}{\\ge} c g(n)\n∃c&gt;0,∃n0​&gt;0,∀n≥n0​:f(n)≥cg(n)\n注意：\n\n量词顺序是 ∃∃∀\\exists \\exists \\forall∃∃∀。\n与 Big-Oh 不同，最后的符号是 ≥\\ge≥ 而不是 ≤\\le≤。\n\n此外，Big-Omega 可以被定义为：\nlim inf⁡n→∞f(n)g(n)&gt;0\\liminf_{n \\to \\infty} \\frac{f(n)}{g(n)} &gt; 0\nn→∞liminf​g(n)f(n)​&gt;0\n 特点\n\nBig-Omega 规定了 f(n)f(n)f(n) 的 增长率的下限，也就是说，当 nnn 足够大时，f(n)f(n)f(n) 的增长速率不小于 g(n)g(n)g(n)。\nBig-Omega g(n)g(n)g(n) 的选择并不是一定要选择“最佳”或者“最有用的”函数。例如，对于 f(n)=n3−nf(n) = n^3 - nf(n)=n3−n 可以是 Ω(n3)\\Omega(n^3)Ω(n3)，但也可以是 Ω(n2)\\Omega(n^2)Ω(n2)。因此 g(n)g(n)g(n) 的增长率越大越能说明 f(n)f(n)f(n) 的增长率。\n一般可以用来描述算法的最佳情况。\n\n 性质\n类似于 Big-Oh，Big-Omega 作为一个二元关系拥有下面的性质：\n\nBig-Omega 具有自反性 (Reflexive, e.g. xRxxRxxRx)。\nBig-Omega 不具有对称性 (Symmetric, e.g. xRy  ⇏  yRxxRy \\;\\not\\Rightarrow\\; yRxxRy⇒yRx)。\nBig-Omega 具有传递性 (Transitive, e.g. xRyxRyxRy 且 yRz→xRzyRz \\rightarrow xRzyRz→xRz)。\n\nBig-Omega 更像是 ≥\\ge≥。\n 推论 &amp; 方法\n\n推论 1： f(n)f(n)f(n) is O(g(n))  ⟺  g(n)O(g(n)) \\iff g(n)O(g(n))⟺g(n) is Ω(f(n))\\Omega(f(n))Ω(f(n))\n推论 2（乘法）： 如果 f1(n)f_1(n)f1​(n) 是 Ω(g1(n))\\Omega(g_1(n))Ω(g1​(n))，f2(n)f_2(n)f2​(n) 是 Ω(g2(n))\\Omega(g_2(n))Ω(g2​(n))，那么 f1(n)f2(n)f_1(n)f_2(n)f1​(n)f2​(n) 是 Ω(g1(n)g2(n))\\Omega(g_1(n)g_2(n))Ω(g1​(n)g2​(n))。\n\n删除规则仍然适用于 Big-Omega，但是注意删除法则跟 Big-Oh 一样 是 删除低阶函数 而不是删除高阶函数。\n例如 f(n)=n3−nf(n) = n^3 - nf(n)=n3−n 中，应该删除的也是 nnn。找到 n3n^3n3 后我们就可以找比 n3n^3n3 阶低的函数来代替。\n 3. Big-Theta: Θ(n)\\Theta (n)Θ(n)\n 定义\n假设有两个正函数 f(n)f(n)f(n) 和 g(n)g(n)g(n)，如果我们称 f(n)f(n)f(n) is Θ(g(n))\\Theta(g(n))Θ(g(n))，当且仅当\n\n\n\n\n\n\n\n\n\n∃c′&gt;0,∃c′′&gt;0,∃n0&gt;0,∀n≥n0:c′g(n)≤f(n)≤c′′g(n)\\exists c&#x27; &gt; 0, \\exists c&#x27;&#x27; &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : c&#x27; g(n) \\le f(n) \\le c&#x27;&#x27; g(n)\n∃c′&gt;0,∃c′′&gt;0,∃n0​&gt;0,∀n≥n0​:c′g(n)≤f(n)≤c′′g(n)\n此外，Big-Theta 可以被定义为：\n\n\n\n\n\n\n\n\n\nf(n) is Θ(g(n))  ⟺  f(n) is O(g(n)) and f(n) is Ω(g(n))f(n) \\text{ is } \\Theta(g(n)) \\iff f(n) \\text{ is } O(g(n)) \\text{ and } f(n) \\text{ is } \\Omega(g(n))\nf(n) is Θ(g(n))⟺f(n) is O(g(n)) and f(n) is Ω(g(n))\nf(n) is Θ(g(n))  ⟺  f(n) is O(g(n)) and g(n) is O(f(n))f(n) \\text{ is } \\Theta(g(n)) \\iff f(n) \\text{ is } O(g(n)) \\text{ and } g(n) \\text{ is } O(f(n))\nf(n) is Θ(g(n))⟺f(n) is O(g(n)) and g(n) is O(f(n))\n 性质\nBig-Theta 作为一个二元关系拥有下面的性质：\n\nBig-Theta 具有自反性 (Reflexive, e.g. xRxxRxxRx)。\nBig-Theta 具有 对称性 (Symmetric, e.g. xRy  ⟺  yRxxRy \\iff yRxxRy⟺yRx)：如果 f(n)f(n)f(n) 是 Θ(g(n))\\Theta(g(n))Θ(g(n))，那么 g(n)g(n)g(n) 是 Θ(f(n))\\Theta(f(n))Θ(f(n))。\n可以根据 Big-Theta 的第二定义和 Big-Omega 的推论 1 得出。\nBig-Theta 具有传递性 (Transitive, e.g. xRyxRyxRy 且 yRz→xRzyRz \\rightarrow xRzyRz→xRz)。\n\nBig-Theta 更像是 ≈\\approx≈。\n 4. little-oh: o(n)\n 定义\n假设有两个正函数 f(n)f(n)f(n) 和 g(n)g(n)g(n)，如果我们称 f(n)f(n)f(n) is o(g(n))o(g(n))o(g(n))，当且仅当\n\n\n\n\n\n\n\n\n\n∀c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣&lt;cg(n)\\textcolor{red}{\\forall} c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : |f(n)| \\textcolor{red}{&lt;} c g(n)\n∀c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣&lt;cg(n)\n注意：\n\n量词顺序是 ∀∃∀\\forall \\exists \\forall∀∃∀。\n因为是对全部的 ccc 存在 n0n_0n0​，因此 n0n_0n0​ 的数值可以依赖于 ccc。\n与 Big-Oh 不同，最后的符号是 &lt;&lt;&lt; 而不是 ≤\\le≤。\n\nlittle-oh 也可以被定义为：\nlim⁡n→∞f(n)g(n)=0\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\nn→∞lim​g(n)f(n)​=0\n 性质\nlittle-oh 作为一个二元关系 (binary relation)，拥有以下性质：\n\nlittle-oh 不具有 自反性 (Reflexive, e.g. xRxxRxxRx)。即 f(n)=nf(n) = nf(n)=n 不是 o(n)o(n)o(n)。\nlittle-oh 不具有 对称性 (Symmetric, e.g. xRy⇏yRxxRy \\not\\Rightarrow yRxxRy⇒yRx)。\nlittle-oh 具有传递性 (Transitive, e.g. xRyxRyxRy 且 yRz→xRzyRz \\rightarrow xRzyRz→xRz)。\n\nlittle-oh 更像是严格的 &lt;&lt;&lt;。\n 特点\n\n与 Big-Oh 类似，little-Oh 定义是函数的 严格无法达到的上限。\nlittle-oh 的意思是，当 nnn 足够大时，f(n)f(n)f(n) 的增长速率远小于 g(n)g(n)g(n)。\nlittle-oh 中 g(n)g(n)g(n) 阶级越小，越能说明 f(n)f(n)f(n) 的增长率。\n\n 推论 &amp; 方法\n\n\n推论 1： 如果 f(n)f(n)f(n) 是 o(g(n))o(g(n))o(g(n))，那么 f(n)f(n)f(n) 一定是 O(g(n))O(g(n))O(g(n))。正如 &lt;&lt;&lt; 与 ≤\\le≤ 一样，很明显 O(g(n))⊂o(g(n))O(g(n)) \\subset o(g(n))O(g(n))⊂o(g(n))。\n\n\n推论 2（乘法 1）： 如果 f1(n)f_1(n)f1​(n) 是 o(g1(n))o(g_1(n))o(g1​(n))，f2(n)f_2(n)f2​(n) 是 o(g2(n))o(g_2(n))o(g2​(n))，那么 f1(n)f2(n)f_1(n)f_2(n)f1​(n)f2​(n) 是 o(g1(n)g2(n))o(g_1(n)g_2(n))o(g1​(n)g2​(n))。\n\n\n推论 3（乘法 2）： 如果 f1(n)f_1(n)f1​(n) 是 o(g1(n))o(g_1(n))o(g1​(n))，f2(n)f_2(n)f2​(n) 是 O(g2(n))O(g_2(n))O(g2​(n))，那么 f1(n)f2(n)f_1(n)f_2(n)f1​(n)f2​(n) 是 o(g1(n)g2(n))o(g_1(n)g_2(n))o(g1​(n)g2​(n))。\n\n\n与 Big-Oh 类似，little-Oh 也可以使用删除规则。\n与 Big-Oh 不同的是，little-Oh 不能选择删除规则后的函数，而只能选择比该函数阶级更大的函数。\n 例子\n\n证明 f(n)=n2+nf(n) = n^2 + nf(n)=n2+n 是 o(n3)o(n^3)o(n3)\n要证明 f(n)=n2+nf(n) = n^2 + nf(n)=n2+n 是 O(n3)O(n^3)O(n3)，则需要证明 ∀c&gt;0,∃n0&gt;0,∀n≥n0:f(n)&lt;cg(n)\\forall c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : f(n) &lt; c g(n)∀c&gt;0,∃n0​&gt;0,∀n≥n0​:f(n)&lt;cg(n)。\n代入和整理可得 ∀c&gt;0,∃n0&gt;0,∀n≥n0:cn2−n−1&gt;0\\forall c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : c n^2 - n - 1 &gt; 0∀c&gt;0,∃n0​&gt;0,∀n≥n0​:cn2−n−1&gt;0。\n由公式可得，若 cn2−n−1=0c n^2 - n - 1 = 0cn2−n−1=0，且 nr&gt;0n_r &gt; 0nr​&gt;0，可解得 nr=1+4c+12c&gt;0n_r = \\frac{1 + \\sqrt{4c + 1}}{2c} &gt; 0nr​=2c1+4c+1​​&gt;0。\n且当 n&gt;nrn &gt; n_rn&gt;nr​ 时，cn2−n−1&gt;0c n^2 - n - 1 &gt; 0cn2−n−1&gt;0 恒成立，那么可以取 n0=nr+1=1+4c+12c+1n_0 = n_r + 1 = \\frac{1 + \\sqrt{4c + 1}}{2c} + 1n0​=nr​+1=2c1+4c+1​​+1，使得 ∀n≥n0:f(n)&lt;cg(n)\\forall n \\ge n_0 : f(n) &lt; c g(n)∀n≥n0​:f(n)&lt;cg(n) 恒成立。\n因此，f(n)=n2+nf(n) = n^2 + nf(n)=n2+n 是 O(n3)O(n^3)O(n3)，此时对于所有的 ccc 取 n0=1+4c+12c+1n_0 = \\frac{1 + \\sqrt{4c + 1}}{2c} + 1n0​=2c1+4c+1​​+1。\n\n\n 关于 Big-Oh 和 little-oh 的定义上的思考：\n如果将 Big-Oh 的定义改为：∃c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣&lt;cg(n)\\exists c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : |f(n)| \\textcolor{red}{&lt;} c g(n)∃c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣&lt;cg(n)，称为 O&lt;O_{&lt;}O&lt;​，而原定义称为 O≤O_{\\le}O≤​，\n那么实际上，对于 g(n)&gt;0g(n) &gt; 0g(n)&gt;0，f(n)f(n)f(n) 是 O&lt;(g(n))  ⟺  f(n)O_{&lt;}(g(n)) \\iff f(n)O&lt;​(g(n))⟺f(n) 是 O≤(g(n))O_{\\le}(g(n))O≤​(g(n))。\n唯一的区别是对于 f(n)=0,g(n)=0f(n) = 0, g(n) = 0f(n)=0,g(n)=0 来说 0 是 O≤(0)O_{\\le}(0)O≤​(0) 而不是 O&lt;(0)O_{&lt;}(0)O&lt;​(0)。\n而我们想要定义 Big-Oh 的渐进符号为 ≤\\textcolor{red}{\\le}≤，就得要求 0 是 O(0)O(0)O(0)，因此使用 ≤\\textcolor{red}{\\le}≤ 而不是 &lt;\\textcolor{red}{&lt;}&lt;。\n同理，对于 little-oh 如果定义改为：∀c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣≤cg(n)\\forall c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : |f(n)| \\textcolor{red}{\\le} c g(n)∀c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣≤cg(n)，称为 o≤o_{\\le}o≤​，而原定义称为 o&lt;o_{&lt;}o&lt;​。\n此时 o≤o_{\\le}o≤​ 与 o&lt;o_{&lt;}o&lt;​ 定义的唯一区别也是当 f(n)=0,g(n)=0f(n) = 0, g(n) = 0f(n)=0,g(n)=0 的时候，此时 0 是 o≤(0)o_{\\le}(0)o≤​(0) 而不是 o&lt;(0)o_{&lt;}(0)o&lt;​(0)。\n而我们想要定义 little-oh 的渐进符号是 &lt;\\textcolor{red}{&lt;}&lt;，就得要求 0 不是 o(0)o(0)o(0)，那么使用的是 &lt;\\textcolor{red}{&lt;}&lt; 而不是 ≤\\textcolor{red}{\\le}≤。\n实际上，对于 Big-Oh 和 little-oh 最主要的区别是 ∃c\\textcolor{red}{\\exists c}∃c 和 ∀c\\textcolor{red}{\\forall c}∀c。\n\n Big-Oh家族使用样例\n\n用于表示一个范围：算法 X 最坏的情况时间复杂度是 o(n4)o(n^4)o(n4) 和 Ω(n3)\\Omega(n^3)Ω(n3)，但是实际表现并不确定。\n用于确定一个增长率：算法 X 最佳的情况时间复杂度是 Θ(n2)\\Theta(n^2)Θ(n2)\n用来表示一个平均值：算法 X 平均情况时间复杂度是 O(n3)O(n^3)O(n3)\n\n 使用Big-Oh家族分析算法效率注意点\nBig-Oh 家族之所以实用是因为它隐藏了低阶项和常数。它们主要分析当 nnn 足够大时渐进的范围，也可以说是 nnn 的增长率。\n但是在 nnn 比较小时 这些被隐藏掉的项可能会成为非常重要的参考指标。\n也就是说，不能完全根据 Big-Oh 家族和阶数大小来完全判断一个算法的 实际工作时的效率。\n例如：\n\n100000n100000n100000n 是 O(n)O(n)O(n)，同时 2n2^n2n 是 O(2n)O(2^n)O(2n)，当时当 nnn 比较小时，例如 n=6n = 6n=6 时，前者要进行的计算数是 60000，而后者是 64，此时前者的效率是不如后者的。\nO(1.02n)O(1.02^n)O(1.02n) 尽管是指数级 (exponential)，但是它的效率并不逊色。\n\n不过 Big-Oh 家族在理论上对算法效率进行分析往往是有效的，并且在 nnn 比小时候程序所消耗的时间往往是会忽略不计的。\n 常用级数表示表\n根据阶级(order)从小到大排名。\n\n\n\n表示\n中文名\n英文名\n数量级\n\n\n\n\nO(nc), c&lt;0 or O ⁣(kn)O(n^c),\\ c &lt; 0 \\text{ or } O\\!\\left(\\frac{k}{n}\\right)O(nc), c&lt;0 or O(nk​)\n负数幂级\nnegative power\n∞\\infty∞, 不存在\n\n\nO(1)O(1)O(1)\n常数级\nconstant\n∞\\infty∞\n\n\nO(log⁡log⁡n)O(\\log \\log n)O(loglogn)\n双对数级\ndouble logarithmic\n22×1062^{2 \\times 10^6}22×106\n\n\nO(log⁡n)O(\\log n)O(logn)\n对数级\nlogarithmic\n1030103010^{301030}10301030\n\n\nO((log⁡n)c), c&gt;1O((\\log n)^c),\\ c &gt; 1O((logn)c), c&gt;1\n多重对数级\npolylogarithmic\n2106c2^{10^6 c}2106c\n\n\nO(nc), 0&lt;c&lt;1 or O(n)O(n^c),\\ 0 &lt; c &lt; 1 \\text{ or } O(\\sqrt{n})O(nc), 0&lt;c&lt;1 or O(n​)\n分数幂级\nfractional power\n106c10^{6c}106c\n\n\nO(n)O(n)O(n)\n线性级\nlinear\n10610^6106\n\n\nO(nlog⁡n)=O(log⁡n!)O(n \\log n) = O(\\log n!)O(nlogn)=O(logn!)\n对数线性/拟线性级\nloglinear, n-log-n\n10510^5105\n\n\nO(n2)O(n^2)O(n2)\n二次级\nquadratic\n10310^3103\n\n\nO(nc), c&gt;1O(n^c),\\ c &gt; 1O(nc), c&gt;1\n多项式/代数级\npolynomial, algebraic\n106c\\sqrt[c]{10^6}c106​\n\n\nO(cn)O(c^n)O(cn)\n指数级\nexponential\n6log⁡c106 \\log_c 106logc​10\n\n\nO(n!)O(n!)O(n!)\n阶乘级\nfactorial\n999\n\n\n\n Big-Oh 家族定义总结\n\n\n\n表示法\n名字\n描述\n渐进符号\n形式定义\n\n\n\n\no(g(n))o(g(n))o(g(n))\nlittle-Oh\n函数渐进地由 ggg 支配\n&lt;\\textcolor{red}{&lt;}&lt;\n∀c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣&lt;cg(n)\\forall c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : |f(n)| \\textcolor{red}{&lt;} c g(n)∀c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣&lt;cg(n)\n\n\nO(g(n))O(g(n))O(g(n))\nBig-Oh\n函数以 ggg 为渐进边界\n≤\\textcolor{red}{\\le}≤\n∃c&gt;0,∃n0&gt;0,∀n≥n0:∣f(n)∣≤cg(n)\\exists c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : |f(n)| \\textcolor{red}{\\le} c g(n)∃c&gt;0,∃n0​&gt;0,∀n≥n0​:∣f(n)∣≤cg(n)\n\n\nΘ(g(n))\\Theta(g(n))Θ(g(n))\nBig-Theta\n函数由 ggg 为渐进上边界和下边界\n≈\\textcolor{red}{\\approx}≈\n∃c′&gt;0,∃c′′&gt;0,∃n0&gt;0,∀n≥n0:c′g(n)≤f(n)≤c′′g(n)\\exists c&#x27; &gt; 0, \\exists c&#x27;&#x27; &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : c&#x27; g(n) \\textcolor{red}{\\le} f(n) \\textcolor{red}{\\le} c&#x27;&#x27; g(n)∃c′&gt;0,∃c′′&gt;0,∃n0​&gt;0,∀n≥n0​:c′g(n)≤f(n)≤c′′g(n)\n\n\nΩ(g(n))\\Omega(g(n))Ω(g(n))\nBig-Omega\n函数由 ggg 为渐进下边界\n&gt;\\textcolor{red}{&gt;}&gt;\n∃c&gt;0,∃n0&gt;0,∀n≥n0:f(n)≥cg(n)\\exists c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : f(n) \\textcolor{red}{\\ge} c g(n)∃c&gt;0,∃n0​&gt;0,∀n≥n0​:f(n)≥cg(n)\n\n\nω(g(n))\\omega(g(n))ω(g(n))\nlittle-omega\n函数渐进支配 ggg\n&gt;\\textcolor{red}{&gt;}&gt;\n∀c&gt;0,∃n0&gt;0,∀n≥n0:f(n)&gt;cg(n)\\forall c &gt; 0, \\exists n_0 &gt; 0, \\forall n \\ge n_0 : f(n) \\textcolor{red}{&gt;} c g(n)∀c&gt;0,∃n0​&gt;0,∀n≥n0​:f(n)&gt;cg(n)\n\n\n\n 三. Master Theory\n 前提\n 1. 分而治之 Divide and Conquer\n分而治之是一个设计算法的思想，它通常能够高速地处理问题。\n分而治之的组成成分如下：\n\n分解 (Divide)：将输入分为两个或多个不相交的输入子集。\n递归 (Recur)：使用递归解决这些子集的子问题。\n组合 (Conquer)：将所有子集的解组合起来形成输入的解。\n\n 2. 递归关系 Recurrence Relation\n\n\n\n\n\n\n\n\n\nA recurrence relation is a recursively-defined function.\n递归关系(Recurrence Relation)是使用递归定义的函数。\n假设一个程序的运行时间是 T(n)T(n)T(n)，那么递归关系会在一系列小于n的值中来表达 T(n)T(n)T(n)。\n\n例子：归并排序（merge-sort）的递归关系及其时间复杂度的证明\n假设归并排序的运行时间为 T(n)T(n)T(n)，那么：\nT(n)=2T(n2)+b+anT(n) = 2T\\left(\\frac{n}{2}\\right) + b + an\nT(n)=2T(2n​)+b+an\nT(1)=1T(1) = 1\nT(1)=1\n\n2T(n2)2T(\\frac{n}{2})2T(2n​) 表示递归分成两个子数组，每个子数组的大小为 n2\\frac{n}{2}2n​。\nananan 是 merge 的花费。\nbbb 是常数时间的交换。\n\n因此我们通过带入可以得到：\n\nT(1)=1T(1) = 1T(1)=1\nT(2)=2T(1)+b+2a=2+b+2aT(2) = 2T(1) + b + 2a = 2 + b + 2aT(2)=2T(1)+b+2a=2+b+2a\nT(4)=2T(2)+b+4a=2(2+b+2a)+b+4a=4+3b+8aT(4) = 2T(2) + b + 4a = 2(2 + b + 2a) + b + 4a = 4 + 3b + 8aT(4)=2T(2)+b+4a=2(2+b+2a)+b+4a=4+3b+8a\nT(8)=2T(4)+b+8a=2(4+3b+8a)+b+8a=8+7b+24aT(8) = 2T(4) + b + 8a = 2(4 + 3b + 8a) + b + 8a = 8 + 7b + 24aT(8)=2T(4)+b+8a=2(4+3b+8a)+b+8a=8+7b+24a\n\n由此我们猜测：\nT(2k)=2k+(2k−1)b+k2ka,k∈NT(2^k) = 2^k + (2^k - 1)b + k 2^k a,\\quad k \\in \\mathbb{N}\nT(2k)=2k+(2k−1)b+k2ka,k∈N\n我们可以使用数学归纳法（induction）来验证：\n\n\n\n\n\n\n\n\n\nClaim: T(2k)=2k+(2k−1)b+k2ka, k∈NT(2^k) = 2^k + (2^k - 1)b + k 2^k a,\\ k \\in \\mathbb{N}T(2k)=2k+(2k−1)b+k2ka, k∈N\nBase case: k=0, T(1)=1+0∗b+0∗1∗a=1k = 0,\\ T(1) = 1 + 0 * b + 0 * 1 * a = 1k=0, T(1)=1+0∗b+0∗1∗a=1 is meet the claim.\nStep case: Assume the claim is true at kkk, and we need to prove that T(k+1)T(k+1)T(k+1) is true.\nT(2k+1)=2T(2k)+b+2ka=2(2k+(2k−1)b+k2ka)+b+2ka=2k+1+(2k+1−2)b+b+k2k+1a+2ka=2k+1+(2k+1−1)b+(k+1)2k+1a\\begin{aligned}\nT(2^{k+1})\n&amp;= 2T(2^k) + b + 2^k a \\\\\n&amp;= 2\\big(2^k + (2^k - 1)b + k2^ka\\big) + b + 2^k a \\\\\n&amp;= 2^{k+1} + (2^{k+1} - 2)b + b + k2^{k+1}a + 2^k a \\\\\n&amp;= 2^{k+1} + (2^{k+1} - 1)b + (k+1)2^{k+1}a\n\\end{aligned}\nT(2k+1)​=2T(2k)+b+2ka=2(2k+(2k−1)b+k2ka)+b+2ka=2k+1+(2k+1−2)b+b+k2k+1a+2ka=2k+1+(2k+1−1)b+(k+1)2k+1a​\nQ.E.D.\n我们假设 T′(n)=n+(n−1)b+anlog⁡(n)T&#x27;(n) = n + (n-1)b + an\\log(n)T′(n)=n+(n−1)b+anlog(n)，for n=2k, k∈Nn = 2^k,\\ k \\in \\mathbb{N}n=2k, k∈N。\n我们可以证明出 T(n)T(n)T(n) 是 Θ(T′(n))\\Theta(T&#x27;(n))Θ(T′(n))。\n因此，T(n)T(n)T(n) 是 Θ(nlog⁡n)\\Theta(n\\log n)Θ(nlogn)。\n\n\n Master Theory\n考虑存在下面的递归关系：\nT(n)=aT(nb)+f(n)T(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\nT(n)=aT(bn​)+f(n)\nT(1)=1T(1) = 1\nT(1)=1\n这是一个由分而治之设计的算法：分解成 aaa 个子集，每个子集的大小是 nb\\frac{n}{b}bn​，此外每个递归/循环有一些额外的操作 f(n)f(n)f(n)。\nMaster 定理 (Master Theorem) 是一个根据 a,ba, ba,b 的数值以及对 f(n)f(n)f(n) 进行放缩来快速求出 T(n)T(n)T(n) 的 Big-Oh 家族的方法。\n下面就对 f(n),a,bf(n), a, bf(n),a,b 不同情况进行讨论。\n 1. f(n)=0f(n) = 0f(n)=0 时\n此时 T(n)=aT(nb)T(n) = aT\\left(\\frac{n}{b}\\right)T(n)=aT(bn​)，我们可以使用数学归纳法证明出 T(bk)=akT(b^k) = a^kT(bk)=ak。\n我们令 n=bkn = b^kn=bk，根据数学公式我们可以推导出 ak=(bk)log⁡baa^k = \\left(b^k\\right)^{\\log_b a}ak=(bk)logb​a，因此我们可以得到\nT(n)=nlog⁡baT(n) = n^{\\log_b a}\nT(n)=nlogb​a\n因此我们可以知道 T(n)T(n)T(n) 是 Θ(nlog⁡ba)\\Theta(n^{\\log_b a})Θ(nlogb​a)。\n 2. 当 f(n)≠0f(n) \\neq 0f(n)=0 时\n此时可以分为三种情况：\n\n\n\nf(n)f(n)f(n) 的形式\nccc 与 log⁡ba\\log_b alogb​a 的关系\nT(n)T(n)T(n) 的 Big-Theta\n描述\n\n\n\n\nf(n)f(n)f(n) 是 O(nc)O(n^c)O(nc)\nc&lt;log⁡bac &lt; \\log_b ac&lt;logb​a\nΘ(nlog⁡ba)\\Theta(n^{\\log_b a})Θ(nlogb​a)\nf(n)f(n)f(n) 的增长率非常小，此时忽略 f(n)f(n)f(n)\n\n\nf(n)f(n)f(n) 是 O(nclog⁡kn),k≥0O(n^c \\log^k n), k \\ge 0O(nclogkn),k≥0\nc=log⁡bac = \\log_b ac=logb​a\nΘ(nc(log⁡n)k+1)\\Theta(n^c(\\log n)^{k+1})Θ(nc(logn)k+1)\nf(n)f(n)f(n) 的增长率适中，此时混合使用 a,b,f(n)a, b, f(n)a,b,f(n)\n\n\nf(n)f(n)f(n) 是 Ω(nc)\\Omega(n^c)Ω(nc) 并满足正则条件\nc&gt;log⁡bac &gt; \\log_b ac&gt;logb​a\nΘ(f(n))\\Theta(f(n))Θ(f(n))\nf(n)f(n)f(n) 的增长率非常大，此时只考虑 f(n)f(n)f(n)\n\n\n\n情况三中需要满足正则条件 (Regularity Condition)：\n∃k&lt;1:af(nb)≤kf(n)\\exists k &lt; 1 : a f\\left(\\frac{n}{b}\\right) \\le k f(n)\n∃k&lt;1:af(bn​)≤kf(n)\n该条件保证这个条件确保 f(n)f(n)f(n) 不会增长过快导致 T(n)T(n)T(n) 完全被后递归部分主导。\n注意 f(n)f(n)f(n) 的形式以及 ccc 与 log⁡ba\\log_b alogb​a 的关系：\n\n情况一 中，f(n)f(n)f(n) 的渐进上边界不如 nlog⁡ban^{\\log_b a}nlogb​a，那么 f(n)f(n)f(n) 的增长率是可以被忽略的。\n情况二 中，f(n)f(n)f(n) 的增长率是与 nlog⁡ban^{\\log_b a}nlogb​a 持平的，因此应该混合使用 a,b,f(n)a, b, f(n)a,b,f(n)。\n情况三 中，f(n)f(n)f(n) 的渐进下边界超过了 nlog⁡ban^{\\log_b a}nlogb​a，因此只考虑 f(n)f(n)f(n)。\n\n 四. 其他理论\n 1. 平摊时间和平均时间\n平摊时间(amortized time)是从一组操作中每个操作平摊下来的时间。与平均时间(average time)不同，后者主要是针对一次操作的平均时间。\n\n\n\n概念\n平摊时间 Amortized Time\n平均时间 Average Time\n\n\n\n\n含义\n将某些昂贵操作的成本分摊到多次普通操作后的平均意义成本\n在随机输入或概率模型下对操作时间求数学期望\n\n\n基础假设\n最坏情况下也不会很差（分摊后）\n输入是随机的或特定概率分布\n\n\n保证\n提供最坏情况下的平均性能保证（强保证）\n只提供期望意义性能保证（弱保证/概率保证）\n\n\n关注点\n关注操作序列的整体表现\n关注输入概率，不保证最坏情况\n\n\n适用场景\n操作成本变化很大但存在规律，如扩容、重哈希、增量式操作\n输入随机或具体分布不明，如快速排序随机枢轴\n\n\n举例\n动态数组插入 O(1)O(1)O(1) 平摊\n哈希表查找 O(1)O(1)O(1) 平均，快速排序 O(nlogn)O(n log n)O(nlogn) 平均\n\n\n\n\n&lt;返回算法与数据结构导航\n","slug":"笔记/算法和数据结构/Big-Oh和Master","date":"2024-05-24T10:49:36.000Z","categories_index":"笔记-算法和数据结构","tags_index":"Algorithms,Data Structures,Big-Oh,Master Theory","author_index":"zExNocs"},{"id":"f2a68e25ffce8d6b8f475f30117ea7f4","title":"算法和数据结构导航","content":" 理论\n\n\n\n名称\n介绍\n\n\n\n\nBig-Oh 和 Master\n有关算法效率分析的笔记。\n\n\n数据结构基础理论和定义\n包括数据结构的基础定义，例如抽象ADT和具体CDT。\n\n\n算法基础理论和定义\n有关算法的通用基础理论的笔记。包括启发式算法的经典问题例子介绍。\n\n\n\n\n 算法\n\n\n\n名称\n介绍\n\n\n\n\n算法基础理论和定义\n有关算法的通用基础理论的笔记。包括启发式算法的经典问题例子介绍。\n\n\n排序\n有关排序算法的性质和简单的排序算法的实现和分析\n\n\n贪心算法\n有关贪心算法的介绍和其解决的问题样例，例如最小生成树的 Prim 算法。\n\n\n简单动态规划\n有关动态规划的介绍和性质，以及简单的动态规划问题和方法。\n\n\n最短路算法\n有关最短路问题描述和其解决的算法的笔记。\n\n\n\n\n 数据结构\n\n\n\n名称\n介绍\n\n\n\n\n数据结构基础理论和定义\n包括数据结构的基础定义，例如抽象ADT和具体CDT。\n\n\n基础树结构\n关于基础树结构的相关术语和ADT定义。\n\n\n链表\n有关链表数据结构的笔记。\n\n\n向量 Vector\n有关向量 vector 可变大小数组的数据结构笔记。\n\n\n优先队列和堆\n有关优先队列(ADT) 和其 CDT (堆) 的笔记\n\n\n映射 Map\n有关映射 map 的笔记。包括其 CDT 和部分 ADT 实现，例如基于哈希表的哈希映射。\n\n\n二叉搜索树 BST\n有关二叉搜索树的CDT和部分ADT实现，例如AVL树。\n\n\n\n\n1|[]()||","slug":"笔记/算法和数据结构/算法和数据结构导航","date":"2024-05-24T09:00:00.000Z","categories_index":"导航","tags_index":"Algorithms,Data Structures","author_index":"zExNocs"}]