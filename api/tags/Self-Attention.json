{"name":"Self-Attention","slug":"Self-Attention","count":1,"postlist":[{"title":"深度学习-Transformer","uid":"6579493455b403ba13c9ac59200fccaf","slug":"笔记/深度学习/Transformer","date":"2026-01-04T05:23:44.000Z","updated":"2026-01-04T08:38:24.768Z","comments":true,"path":"api/articles/笔记/深度学习/Transformer.json","keywords":null,"cover":"img/post/笔记/深度学习/Transformer/cover.png","text":"关于Transformer模型的笔记，以及其变体 BERT 和 GPT 系列的介绍，涵盖自注意力机制、位置编码、Encoder-Decoder结构、掩码机制和预训练方法等内容，以及 Prompt工程的基本概念和应用。...","permalink":"/post/笔记/深度学习/Transformer","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"笔记-深度学习","slug":"笔记-深度学习","count":10,"path":"api/categories/笔记-深度学习.json"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","count":11,"path":"api/tags/Machine-Learning.json"},{"name":"Deep Learning","slug":"Deep-Learning","count":11,"path":"api/tags/Deep-Learning.json"},{"name":"Transformer","slug":"Transformer","count":1,"path":"api/tags/Transformer.json"},{"name":"Self-Attention","slug":"Self-Attention","count":1,"path":"api/tags/Self-Attention.json"},{"name":"BERT","slug":"BERT","count":1,"path":"api/tags/BERT.json"},{"name":"GPT","slug":"GPT","count":1,"path":"api/tags/GPT.json"},{"name":"Prompt Engineering","slug":"Prompt-Engineering","count":1,"path":"api/tags/Prompt-Engineering.json"}],"author":{"name":"zExNocs","slug":"blog-author","avatar":"/img/avatar.jpg","link":"/","description":"<p>一个喜欢摸鱼的人。<br>去码头整点薯条。</p>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"github":{"icon":"/img/svg/github.svg","link":"https://github.com/zExNocs"},"bilibili":{"icon":"/img/svg/bilibili.svg","link":"https://space.bilibili.com/13423200"},"steam":{"icon":"/img/svg/steam.svg","link":"https://steamcommunity.com/id/zExNocs/"}}}},"feature":false}]}